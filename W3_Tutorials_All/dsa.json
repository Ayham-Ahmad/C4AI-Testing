{
  "course_name": "DSA",
  "description": "Data Structures and Algorithms (DSA) is a fundamental part of Computer Science that teaches you how to think and solve complex problems systematically. Using the right data structure and algorithm makes your program run faster, especially when working with lots of data. Knowing DSA can help you perform better in job interviews and land great jobs in tech companies. Tip: Sign in to track your progress - it's free. This tutorial is made to help you learn Data Structures and Algorithms (DSA) fast and easy. Animations, like the one below, are used to explain ideas along the way. Result: First, you will learn the fundamentals of DSA: understanding different data structures, basic algorithm concepts, and how they are used in programming. Then, you will learn more about complex data structures like trees and graphs, study advanced sorting and searching algorithms, explore concepts like time complexity, and more.",
  "course_summary": [
    {
      "title": "Introduction to Data Structures and Algorithms",
      "summary": "Data Structures is about how data can be stored in different structures.\nAlgorithms is about how to solve different problems, often by searching through and manipulating data structures.\nTheory about Data Structures and Algorithms (DSA) helps us to use large amounts of data to solve problems efficiently.\nWhat are Data Structures?\nA data structure is a way to store data.\nWe structure data in different ways depending on what data we have, and what we want to do with it.\nFirst, let's consider an example without computers in mind, just to get the idea.\nIf we want to store data about people we are related to, we use a family tree as the data structure. We choose a family tree as the data structure because we have information about people we are related to and how they are related, and we want an overview so that we can easily find a specific family member, several generations back.\nWith such a family tree data structure visually in front of you, it is easy to see, for example, who my mother's mother is—it is 'Emma,' right? But without the links from child to parents that this data structure provides, it would be difficult to determine how the individuals are related.\nData structures give us the possibility to manage large amounts of data efficiently for uses such as large databases and internet indexing services.\nData structures are essential ingredients in creating fast and powerful algorithms. They help in managing and organizing data, reduce complexity, and increase efficiency.\nIn Computer Science there are two different kinds of data structures.\nPrimitive Data Structures are basic data structures provided by programming languages to represent single values, such as integers, floating-point numbers, characters, and booleans.\nAbstract Data Structures are higher-level data structures that are built using primitive data types and provide more complex and specialized operations. Some common examples of abstract data structures include arrays, linked lists, stacks, queues, trees, and graphs.\nWhat are Algorithms?\nAn algorithm is a set of step-by-step instructions to solve a given problem or achieve a specific goal.\nA cooking recipe written on a piece of paper is an example of an algorithm, where the goal is to make a certain dinner. The steps needed to make a specific dinner are described exactly.\nWhen we talk about algorithms in Computer Science, the step-by-step instructions are written in a programming language, and instead of food ingredients, an algorithm uses data structures.\nAlgorithms are fundamental to computer programming as they provide step-by-step instructions for executing tasks. An efficient algorithm can help us to find the solution we are looking for, and to transform a slow program into a faster one.\nBy studying algorithms, developers can write better programs.\nAlgorithm examples:\nFinding the fastest route in a GPS navigation system\nNavigating an airplane or a car (cruise control)\nFinding what users search for (search engine)\nSorting, for example sorting movies by rating\nThe algorithms we will look at in this tutorial are designed to solve specific problems, and are often made to work on specific data structures. For example, the 'Bubble Sort' algorithm is designed to sort values, and is made to work on arrays.\nData Structures together with Algorithms\nData structures and algorithms (DSA) go hand in hand. A data structure is not worth much if you cannot search through it or manipulate it efficiently using algorithms, and the algorithms in this tutorial are not worth much without a data structure to work on.\nDSA is about finding efficient ways to store and retrieve data, to perform operations on data, and to solve specific problems.\nBy understanding DSA, you can:\nDecide which data structure or algorithm is best for a given situation.\nMake programs that run faster or use less memory.\nUnderstand how to approach complex problems and solve them in a systematic way.\nWhere is Data Structures and Algorithms Needed?\nData Structures and Algorithms (DSA) are used in virtually every software system, from operating systems to web applications:\nFor managing large amounts of data, such as in a social network or a search engine.\nFor scheduling tasks, to decide which task a computer should do first.\nFor planning routes, like in a GPS system to find the shortest path from A to B.\nFor optimizing processes, such as arranging tasks so they can be completed as quickly as possible.\nFor solving complex problems: From finding the best way to pack a truck to making a computer 'learn' from data.\nDSA is fundamental in nearly every part of the software world:\nOperating Systems\nDatabase Systems\nWeb Applications\nMachine Learning\nVideo Games\nCryptographic Systems\nData Analysis\nSearch Engines\nTheory and Terminology\nAs we go along in this tutorial, new theoretical concepts and terminology (new words) will be needed so that we can better understand the data structures and algorithms we will be working on.\nThese new words and concepts will be introduced and explained properly when they are needed, but here is a list of some key terms, just to get an overview of what is coming:\nWhere to Start?\nIn this tutorial, you will first learn about a data structure with matching algorithms, before moving on to the next data structure.\nFurther into the tutorial the concepts become more complex, and it is therefore a good idea to learn DSA by doing the tutorial step-by-step from the start.\nAnd as mentioned on the previous page, you should be comfortable in at least one of the most common programming languages, like for example JavaScript, C or Python, before doing this tutorial.\nOn the next page we will look at two different algorithms that prints out the first 100 Fibonacci numbers using only primitive data structures (two integer variables). One algorithm uses a loop, and one algorithm uses something called recursion.\nClick the 'Next' button to continue.",
      "examples": []
    },
    {
      "title": "A Simple Algorithm",
      "summary": "Fibonacci Numbers\nThe Fibonacci numbers are very useful for introducing algorithms, so before we continue, here is a short introduction to Fibonacci numbers.\nThe Fibonacci numbers are named after a 13th century Italian mathematician known as Fibonacci.\nThe two first Fibonacci numbers are 0 and 1, and the next Fibonacci number is always the sum of the two previous numbers, so we get 0, 1, 1, 2, 3, 5, 8, 13, 21, ...\nCreate fibonacci numbers.\nThis tutorial will use loops and recursion a lot. So before we continue, let's implement three different versions of the algorithm to create Fibonacci numbers, just to see the difference between programming with loops and programming with recursion in a simple way.\nThe Fibonacci Number Algorithm\nTo generate a Fibonacci number, all we need to do is to add the two previous Fibonacci numbers.\nThe Fibonacci numbers is a good way of demonstrating what an algorithm is. We know the principle of how to find the next number, so we can write an algorithm to create as many Fibonacci numbers as possible.\nBelow is the algorithm to create the 20 first Fibonacci numbers.\nHow it works:\nStart with the two first Fibonacci numbers 0 and 1.\nAdd the two previous numbers together to create a new Fibonacci number.\nUpdate the value of the two previous numbers.\nDo point a and b above 18 times.\nLoops vs Recursion\nTo show the difference between loops and recursion, we will implement solutions to find Fibonacci numbers in three different ways:\nAn implementation of the Fibonacci algorithm above using a for loop.\nAn implementation of the Fibonacci algorithm above using recursion.\nFinding the nnth Fibonacci number using recursion.\n1. Implementation Using a For Loop\nIt can be a good idea to list what the code must contain or do before programming it:\nTwo variables to hold the previous two Fibonacci numbers\nA for loop that runs 18 times\nCreate new Fibonacci numbers by adding the two previous ones\nPrint the new Fibonacci number\nUpdate the variables that hold the previous two fibonacci numbers\nUsing the list above, it is easier to write the program:\nExample\n2. Implementation Using Recursion\nRecursion is when a function calls itself.\nTo implement the Fibonacci algorithm we need most of the same things as in the code example above, but we need to replace the for loop with recursion.\nTo replace the for loop with recursion, we need to encapsulate much of the code in a function, and we need the function to call itself to create a new Fibonacci number as long as the produced number of Fibonacci numbers is below, or equal to, 19.\nOur code looks like this:\nExample\n3. Finding The nnth Fibonacci Number Using Recursion\nTo find the nnth Fibonacci number we can write code based on the mathematic formula for Fibonacci number nn:\nF(n)=F(n−1)+F(n−2)F(n) = F(n-1) + F(n-2)\nThis just means that for example the 10th Fibonacci number is the sum of the 9th and 8th Fibonacci numbers.\nNote: This formula uses a 0-based index. This means that to generate the 20th Fibonacci number, we must write F(19)F(19).\nWhen using this concept with recursion, we can let the function call itself as long as nn is less than, or equal to, 1. If n≤1n \\le 1 it means that the code execution has reached one of the first two Fibonacci numbers 1 or 0.\nThe code looks like this:\nExample\nNotice that this recursive method calls itself two times, not just one. This makes a huge difference in how the program will actually run on our computer. The number of calculations will explode when we increase the number of the Fibonacci number we want. To be more precise, the number of function calls will double every time we increase the Fibonacci number we want by one.\nJust take a look at the number of function calls for F(5)F(5):\nTo better understand the code, here is how the recursive function calls return values so that F(5)F(5) returns the correct value in the end:\nThere are two important things to notice here: The amount of function calls, and the amount of times the function is called with the same arguments.\nSo even though the code is fascinating and shows how recursion work, the actual code execution is too slow and ineffective to use for creating large Fibonacci numbers.\nSummary\nBefore we continue, let's look at what we have seen so far:\nAn algorithm can be implemented in different ways and in different programming languages.\nRecursion and loops are two different programming techniques that can be used to implement algorithms.\nIt is time to move on to the first data structure we will look at, the array.\nClick the \"Next\" button to continue.\nDSA Exercises\nTest Yourself With Exercises\nExercise:\nHow can we make this fibonacci() function recursive?\nStart the Exercise",
      "examples": [
        "prev2 = 0 prev1 = 1 print(prev2) print(prev1) for fibo in range(18): newFibo = prev1 + prev2 print(newFibo) prev2 = prev1 prev1 = newFibo",
        "print(0) print(1) count = 2 def fibonacci(prev1, prev2): global count if count <= 19: newFibo = prev1 + prev2 print(newFibo) prev2 = prev1 prev1 = newFibo count += 1 fibonacci(prev1, prev2) else: return fibonacci(1,0)",
        "def F(n): if n <= 1: return n else: return F(n - 1) + F(n - 2) print(F(19))",
        "print(0) print(1) count = 2 def fibonacci(prev1, prev2): global count if count <= 19: newFibo = prev1 + prev2 print(newFibo) prev2 = prev1 prev1 = newFibo count += 1 (prev1, prev2) else: return fibonacci(1,0)",
        "for"
      ]
    },
    {
      "title": "DSA Arrays",
      "summary": "Arrays\nAn array is a data structure used to store multiple elements.\nArrays are used by many algorithms.\nFor example, an algorithm can be used to look through an array to find the lowest value, like the animation below shows:\nSpeed:\nLowest value:\nIn Python, an array can be created like this:\nNote: The Python code above actually generates a Python 'list' data type, but for the scope of this tutorial the 'list' data type can be used in the same way as an array. Learn more about Python lists here.\nArrays are indexed, meaning that each element in the array has an index, a number that says where in the array the element is located. The programming languages in this tutorial (Python, Java, and C) use zero-based indexing for arrays, meaning that the first element in an array can be accessed at index 0.\nIn Python, this code use index 0 to write the first array element (value 7) to the console:\nExample\nPython:\nAlgorithm: Find The Lowest Value in an Array\nLet's create our first algorithm using the array data structure.\nBelow is the algorithm to find the lowest number in an array.\nHow it works:\nGo through the values in the array one by one.\nCheck if the current value is the lowest so far, and if it is, store it.\nAfter looking at all the values, the stored value will be the lowest of all values in the array.\nTry the simulation below to see how the algorithm for finding the lowest value works (the animation is the same as the one on the top of this page):\nSpeed:\nLowest value:\nThis next simulation also finds the lowest value in an array, just like the simulation above, but here we can see how the numbers inside the array are checked to find the lowest value:\nImplementation\nBefore implementing the algorithm using an actual programming language, it is usually smart to first write the algorithm as a step-by-step procedure.\nIf you can write down the algorithm in something between human language and programming language, the algorithm will be easier to implement later because we avoid drowning in all the details of the programming language syntax.\nCreate a variable 'minVal' and set it equal to the first value of the array.\nGo through every element in the array.\nIf the current element has a lower value than 'minVal', update 'minVal' to this value.\nAfter looking at all the elements in the array, the 'minVal' variable now contains the lowest value.\nYou can also write the algorithm in a way that looks more like a programming language if you want to, like this:\nVariable 'minVal' = array[0]\nFor each element in the array\nIf current element < minVal\nminVal = current element\nNote: The two step-by-step descriptions of the algorithm we have written above can be called 'pseudocode'. Pseudocode is a description of what a program does, using language that is something between human language and a programming language.\nAfter we have written down the algorithm, it is much easier to implement the algorithm in a specific programming language:\nExample\nPython:\nAlgorithm Time Complexity\nWhen exploring algorithms, we often look at how much time an algorithm takes to run relative to the size of the data set.\nIn the example above, the time the algorithm needs to run is proportional, or linear, to the size of the data set. This is because the algorithm must visit every array element one time to find the lowest value. The loop must run 5 times since there are 5 values in the array. And if the array had 1000 values, the loop would have to run 1000 times.\nTry the simulation below to see this relationship between the number of compare operations needed to find the lowest value, and the size of the array.\nSee this page for a more thorough explanation of what time complexity is.\nEach algorithm in this tutorial will be presented together with its time complexity.\nSet values: 300\nRandom\nDescending\nAscending\n10 Random\nOperations: 0\nDSA Exercises\nTest Yourself With Exercises\nExercise:\nHow can we print value \"7\" from the array below?\nStart the Exercise",
      "examples": [
        "my_array = [7, 12, 9, 4, 11]",
        "my_array = [7, 12, 9, 4, 11] print( my_array[0] )",
        "my_array = [7, 12, 9, 4, 11] minVal = my_array[0] # Step 1 for i in my_array: # Step 2 if i < minVal: # Step 3 minVal = i print('Lowest value: ',minVal) # Step 4",
        "print(my_array[])",
        "Variable 'minVal' = array[0]\nFor each element in the array\nIf current element < minVal\nminVal = current element"
      ]
    },
    {
      "title": "DSA Bubble Sort",
      "summary": "Bubble Sort\nBubble Sort is an algorithm that sorts an array from the lowest value to the highest value.\nSpeed:\nRun the simulation to see how it looks like when the Bubble Sort algorithm sorts an array of values. Each value in the array is represented by a column.\nThe word 'Bubble' comes from how this algorithm works, it makes the highest values 'bubble up'.\nHow it works:\nGo through the array, one value at a time.\nFor each value, compare the value with the next value.\nIf the value is higher than the next one, swap the values so that the highest value comes last.\nGo through the array as many times as there are values in the array.\nContinue reading to fully understand the Bubble Sort algorithm and how to implement it yourself.\nManual Run Through\nBefore we implement the Bubble Sort algorithm in a programming language, let's manually run through a short array only one time, just to get the idea.\nStep 1: We start with an unsorted array.\nStep 2: We look at the two first values. Does the lowest value come first? Yes, so we don't need to swap them.\nStep 3: Take one step forward and look at values 12 and 9. Does the lowest value come first? No.\nStep 4: So we need to swap them so that 9 comes first.\nStep 5: Taking one step forward, looking at 12 and 11.\nStep 6: We must swap so that 11 comes before 12.\nStep 7: Looking at 12 and 3, do we need to swap them? Yes.\nStep 8: Swapping 12 and 3 so that 3 comes first.\nRun the simulation below to see the 8 steps above animated:\nManual Run Through: What Happened?\nWe must understand what happened in this first run through to fully understand the algorithm, so that we can implement the algorithm in a programming language.\nCan you see what happened to the highest value 12? It has bubbled up to the end of the array, where it belongs. But the rest of the array remains unsorted.\nSo the Bubble Sort algorithm must run through the array again, and again, and again, each time the next highest value bubbles up to its correct position. The sorting continues until the lowest value 3 is left at the start of the array. This means that we need to run through the array 4 times, to sort the array of 5 values.\nAnd each time the algorithm runs through the array, the remaining unsorted part of the array becomes shorter.\nThis is how a full manual run through looks like:\nWe will now use what we have learned to implement the Bubble Sort algorithm in a programming language.\nBubble Sort Implementation\nTo implement the Bubble Sort algorithm in a programming language, we need:\nAn array with values to sort.\nAn inner loop that goes through the array and swaps values if the first value is higher than the next value. This loop must loop through one less value each time it runs.\nAn outer loop that controls how many times the inner loop must run. For an array with n values, this outer loop must run n-1 times.\nThe resulting code looks like this:\nExample\nBubble Sort Improvement\nThe Bubble Sort algorithm can be improved a little bit more.\nImagine that the array is almost sorted already, with the lowest numbers at the start, like this for example:\nIn this case, the array will be sorted after the first run, but the Bubble Sort algorithm will continue to run, without swapping elements, and that is not necessary.\nIf the algorithm goes through the array one time without swapping any values, the array must be finished sorted, and we can stop the algorithm, like this:\nExample\nBubble Sort Time Complexity\nFor a general explanation of what time complexity is, visit this page.\nFor a more thorough and detailed explanation of Bubble Sort time complexity, visit this page.\nThe Bubble Sort algorithm loops through every value in the array, comparing it to the value next to it. So for an array of nn values, there must be nn such comparisons in one loop.\nAnd after one loop, the array is looped through again and again nn times.\nThis means there are n⋅nn \\cdot n comparisons done in total, so the time complexity for Bubble Sort is:\nO(n2)__ \\underline{\\underline{O(n^2)}}\nThe graph describing the Bubble Sort time complexity looks like this:\nAs you can see, the run time increases really fast when the size of the array is increased.\nLuckily there are sorting algorithms that are faster than this, like Quicksort, that we will look at later.\nYou can simulate Bubble Sort below, where the red and dashed line is the theoretical time complexity O(n2)O(n^2). You can choose a number of values nn, and run an actual Bubble Sort implementation where the operations are counted and the count is marked as a blue cross in the plot below. How does theory compare with practice?\nSet values: 300\nRandom\nWorst Case\nBest Case\n10 Random\nOperations: 0\nDSA Exercises\nTest Yourself With Exercises\nExercise:\nUsing Bubble Sort on this array:\nTo sort the values from left to right in an increasing (ascending) order.\nHow does the array look like after the FIRST run through?\nStart the Exercise",
      "examples": [
        "my_array = [64, 34, 25, 12, 22, 11, 90, 5] n = len(my_array) for i in range(n-1): for j in range(n-i-1): if my_array[j] > my_array[j+1]: my_array[j], my_array[j+1] = my_array[j+1], my_array[j] print(\"Sorted array:\", my_array)",
        "my_array = [7, 3, 9, 12, 11]",
        "my_array = [7, 3, 9, 12, 11] n = len(my_array) for i in range(n-1): swapped = False for j in range(n-i-1): if my_array[j] > my_array[j+1]: my_array[j], my_array[j+1] = my_array[j+1], my_array[j] swapped = True if not swapped: break print(\"Sorted array:\", my_array)",
        "[7, 12, 9, 11, 3]",
        "[\n7, 12,\n9, 11, 3]",
        "[7,\n12, 9,\n11, 3]",
        "[7,\n9, 12,\n11, 3]",
        "[7, 9,\n12, 11,\n3]",
        "[7, 9,\n11, 12,\n3]",
        "[7, 9, 11,\n12, 3\n]",
        "[7, 9, 11,\n3, 12\n]",
        "[,,,,]",
        "[7,14,11,8,9]"
      ]
    },
    {
      "title": "DSA Selection Sort",
      "summary": "Selection Sort\nThe Selection Sort algorithm finds the lowest value in an array and moves it to the front of the array.\nSpeed:\nThe algorithm looks through the array again and again, moving the next lowest values to the front, until the array is sorted.\nHow it works:\nGo through the array to find the lowest value.\nMove the lowest value to the front of the unsorted part of the array.\nGo through the array again as many times as there are values in the array.\nContinue reading to fully understand the Selection Sort algorithm and how to implement it yourself.\nManual Run Through\nBefore we implement the Selection Sort algorithm in a programming language, let's manually run through a short array only one time, just to get the idea.\nStep 1: We start with an unsorted array.\nStep 2: Go through the array, one value at a time. Which value is the lowest? 3, right?\nStep 3: Move the lowest value 3 to the front of the array.\nStep 4: Look through the rest of the values, starting with 7. 7 is the lowest value, and already at the front of the array, so we don't need to move it.\nStep 5: Look through the rest of the array: 12, 9 and 11. 9 is the lowest value.\nStep 6: Move 9 to the front.\nStep 7: Looking at 12 and 11, 11 is the lowest.\nStep 8: Move it to the front.\nFinally, the array is sorted.\nRun the simulation below to see the steps above animated:\nManual Run Through: What Happened?\nWe must understand what happened above to fully understand the algorithm, so that we can implement the algorithm in a programming language.\nCan you see what happened to the lowest value 3? In step 3, it has been moved to the start of the array, where it belongs, but at that step the rest of the array remains unsorted.\nSo the Selection Sort algorithm must run through the array again and again, each time the next lowest value is moved in front of the unsorted part of the array, to its correct position. The sorting continues until the highest value 12 is left at the end of the array. This means that we need to run through the array 4 times, to sort the array of 5 values.\nAnd each time the algorithm runs through the array, the remaining unsorted part of the array becomes shorter.\nWe will now use what we have learned to implement the Selection Sort algorithm in a programming language.\nSelection Sort Implementation\nTo implement the Selection Sort algorithm in a programming language, we need:\nAn array with values to sort.\nAn inner loop that goes through the array, finds the lowest value, and moves it to the front of the array. This loop must loop through one less value each time it runs.\nAn outer loop that controls how many times the inner loop must run. For an array with nn values, this outer loop must run n−1n-1 times.\nThe resulting code looks like this:\nExample\nSelection Sort Shifting Problem\nThe Selection Sort algorithm can be improved a little bit more.\nIn the code above, the lowest value element is removed, and then inserted in front of the array.\nEach time the next lowest value array element is removed, all following elements must be shifted one place down to make up for the removal.\nThese shifting operation takes a lot of time, and we are not even done yet! After the lowest value (5) is found and removed, it is inserted at the start of the array, causing all following values to shift one position up to make space for the new value, like the image below shows.\nNote: You will not see these shifting operations happening in the code if you are using a high level programming language such as Python or Java, but the shifting operations are still happening in the background. Such shifting operations require extra time for the computer to do, which can be a problem.\nSolution: Swap Values!\nInstead of all the shifting, swap the lowest value (5) with the first value (64) like below.\nWe can swap values like the image above shows because the lowest value ends up in the correct position, and it does not matter where we put the other value we are swapping with, because it is not sorted yet.\nHere is a simulation that shows how this improved Selection Sort with swapping works:\nSpeed:\nHere is an implementation of the improved Selection Sort, using swapping:\nExample\nSelection Sort Time Complexity\nFor a general explanation of what time complexity is, visit this page.\nFor a more thorough and detailed explanation of Selection Sort time complexity, visit this page.\nSelection Sort sorts an array of nn values.\nOn average, about n2\\frac{n}{2} elements are compared to find the lowest value in each loop.\nAnd Selection Sort must run the loop to find the lowest value approximately nn times.\nWe get time complexity:\nO(n2⋅n)=O(n2)__ O( \\frac{n}{2} \\cdot n) = \\underline{\\underline{O(n^2)}}\nThe time complexity for the Selection Sort algorithm can be displayed in a graph like this:\nAs you can see, the run time is the same as for Bubble Sort: The run time increases really fast when the size of the array is increased.\nRun the simulation below for different sized arrays.\nThe red dashed line represents the theoretical time complexity O(n2)O(n^2).\nBlue crosses appear when you run the simulation. The blue crosses show how many operations are needed to sort an array of a certain size.\nSet values: 300\nRandom\nWorst Case\nBest Case\n10 Random\nOperations: 0\nThe most significant difference from Bubble sort that we can notice in this simulation is that best and worst case is actually almost the same for Selection Sort (O(n2)O(n^2)), but for Bubble Sort the best case runtime is only O(n)O(n).\nThe difference in best and worst case for Selection Sort is mainly the number of swaps. In the best case scenario Selection Sort does not have to swap any of the values because the array is already sorted. And in the worst case scenario, where the array already sorted, but in the wrong order, so Selection Sort must do as many swaps as there are values in array.\nDSA Exercises\nTest Yourself With Exercises\nExercise:\nUsing Selection Sort on this array:\nTo sort the values from left to right in an increasing (ascending) order.\nWhat is the value of the LAST element after the first run through?\nStart the Exercise",
      "examples": [
        "my_array = [64, 34, 25, 5, 22, 11, 90, 12] n = len(my_array) for i in range(n-1): min_index = i for j in range(i+1, n): if my_array[j] < my_array[min_index]: min_index = j min_value = my_array.pop(min_index) my_array.insert(i, min_value) print(\"Sorted array:\", my_array)",
        "my_array = [64, 34, 25, 12, 22, 11, 90, 5] n = len(my_array) for i in range(n): min_index = i for j in range(i+1, n): if my_array[j] < my_array[min_index]: min_index = j my_array[i], my_array[min_index] = my_array[min_index], my_array[i] print(\"Sorted array:\", my_array)",
        "[ 7, 12, 9, 11, 3]",
        "[ 7, 12, 9, 11,\n3\n]",
        "[\n3\n, 7, 12, 9, 11]",
        "[ 3,\n7\n, 12, 9, 11]",
        "[ 3, 7, 12,\n9\n, 11]",
        "[ 3, 7,\n9\n, 12, 11]",
        "[ 3, 7, 9, 12,\n11\n]",
        "[ 3, 7, 9,\n11\n, 12]",
        "[7,12,9,11,3]"
      ]
    },
    {
      "title": "DSA Insertion Sort",
      "summary": "Insertion Sort\nThe Insertion Sort algorithm uses one part of the array to hold the sorted values, and the other part of the array to hold values that are not sorted yet.\nSpeed:\nThe algorithm takes one value at a time from the unsorted part of the array and puts it into the right place in the sorted part of the array, until the array is sorted.\nHow it works:\nTake the first value from the unsorted part of the array.\nMove the value into the correct place in the sorted part of the array.\nGo through the unsorted part of the array again as many times as there are values.\nContinue reading to fully understand the Insertion Sort algorithm and how to implement it yourself.\nManual Run Through\nBefore we implement the Insertion Sort algorithm in a programming language, let's manually run through a short array, just to get the idea.\nStep 1: We start with an unsorted array.\nStep 2: We can consider the first value as the initial sorted part of the array. If it is just one value, it must be sorted, right?\nStep 3: The next value 12 should now be moved into the correct position in the sorted part of the array. But 12 is higher than 7, so it is already in the correct position.\nStep 4: Consider the next value 9.\nStep 5: The value 9 must now be moved into the correct position inside the sorted part of the array, so we move 9 in between 7 and 12.\nStep 6: The next value is 11.\nStep 7: We move it in between 9 and 12 in the sorted part of the array.\nStep 8: The last value to insert into the correct position is 3.\nStep 9: We insert 3 in front of all other values because it is the lowest value.\nFinally, the array is sorted.\nRun the simulation below to see the steps above animated:\nManual Run Through: What Happened?\nWe must understand what happened above to fully understand the algorithm, so that we can implement the algorithm in a programming language.\nThe first value is considered to be the initial sorted part of the array.\nEvery value after the first value must be compared to the values in the sorted part of the algorithm so that it can be inserted into the correct position.\nThe Insertion Sort Algorithm must run through the array 4 times, to sort the array of 5 values because we do not have to sort the first value.\nAnd each time the algorithm runs through the array, the remaining unsorted part of the array becomes shorter.\nWe will now use what we have learned to implement the Insertion Sort algorithm in a programming language.\nInsertion Sort Implementation\nTo implement the Insertion Sort algorithm in a programming language, we need:\nAn array with values to sort.\nAn outer loop that picks a value to be sorted. For an array with nn values, this outer loop skips the first value, and must run n−1n-1 times.\nAn inner loop that goes through the sorted part of the array, to find where to insert the value. If the value to be sorted is at index ii, the sorted part of the array starts at index 00 and ends at index i−1i-1.\nThe resulting code looks like this:\nExample\nInsertion Sort Improvement\nInsertion Sort can be improved a little bit more.\nThe way the code above first removes a value and then inserts it somewhere else is intuitive. It is how you would do Insertion Sort physically with a hand of cards for example. If low value cards are sorted to the left, you pick up a new unsorted card, and insert it in the correct place between the other already sorted cards.\nThe problem with this way of programming it is that when removing a value from the array, all elements above must be shifted one index place down:\nAnd when inserting the removed value into the array again, there are also many shift operations that must be done: all following elements must shift one position up to make place for the inserted value:\nThese shifting operations can take a lot of time, especially for an array with many elements.\nHidden memory shifts: You will not see these shifting operations happening in the code if you are using a high-level programming language such as Python or JavaScript, but the shifting operations are still happening in the background. Such shifting operations require extra time for the computer to do, which can be a problem.\nYou can read more about how arrays are stored in memory here.\nC and Java code examples above and below are the same: The issue of memory shifts happening behind the scenes is only relevant for high-level programming languages like Python or JavaScript, where arrays are dynamic, which means you can easily remove and insert elements. In lower-level programming languages like C and Java, where arrays have a fixed length, elements cannot be removed or inserted. As a result, there are no such memory shifts happening, and therefore the example codes above and below for C and Java remain the same.\nImproved Solution\nWe can avoid most of these shift operations by only shifting the values necessary:\nIn the image above, first value 7 is copied, then values 11 and 12 are shifted one place up in the array, and at last value 7 is put where value 11 was before.\nThe number of shifting operations is reduced from 12 to 2 in this case.\nThis improvement is implemented in the example below:\nExample\nWhat is also done in the code above is to break out of the inner loop. That is because there is no need to continue comparing values when we have already found the correct place for the current value.\nInsertion Sort Time Complexity\nFor a general explanation of what time complexity is, visit this page.\nFor a more thorough and detailed explanation of Insertion Sort time complexity, visit this page.\nInsertion Sort sorts an array of nn values.\nOn average, each value must be compared to about n2\\frac{n}{2} other values to find the correct place to insert it.\nInsertion Sort must run the loop to insert a value in its correct place approximately nn times.\nWe get time complexity for Insertion Sort:\nO(n2⋅n)=O(n2)__ O( \\frac{n}{2} \\cdot n) = \\underline{\\underline{O(n^2)}}\nThe time complexity for Insertion Sort can be displayed like this:\nUse the simulation below to see how the theoretical time complexity O(n2)O(n^2) (red line) compares with the number of operations of actual Insertion Sorts.\nSet values: 300\nRandom\nWorst Case\nBest Case\n10 Random\nOperations: 0\nFor Insertion Sort, there is a big difference between best, average and worst case scenarios. You can see that by running the different simulations above.\nNext up is Quicksort. Finally we will see a faster sorting algorithm!",
      "examples": [
        "my_array = [64, 34, 25, 12, 22, 11, 90, 5] n = len(my_array) for i in range(1,n): insert_index = i current_value = my_array.pop(i) for j in range(i-1, -1, -1): if my_array[j] > current_value: insert_index = j my_array.insert(insert_index, current_value) print(\"Sorted array:\", my_array)",
        "my_array = [64, 34, 25, 12, 22, 11, 90, 5] n = len(my_array) for i in range(1,n): insert_index = i current_value = my_array[i] for j in range(i-1, -1, -1): if my_array[j] > current_value: my_array[j+1] = my_array[j] insert_index = j else: break my_array[insert_index] = current_value print(\"Sorted array:\", my_array)",
        "[ 7, 12, 9, 11, 3]",
        "[\n7\n, 12, 9, 11, 3]",
        "[ 7,\n12\n, 9, 11, 3]",
        "[ 7, 12,\n9\n, 11, 3]",
        "[ 7,\n9\n, 12, 11, 3]",
        "[ 7, 9, 12, > 11, 3]",
        "[ 7, 9,\n11\n, 12, 3]",
        "[ 7, 9, 11, 12,\n3\n]",
        "[\n3\n,7, 9, 11, 12]"
      ]
    },
    {
      "title": "DSA Quicksort",
      "summary": "Quicksort\nAs the name suggests, Quicksort is one of the fastest sorting algorithms.\nThe Quicksort algorithm takes an array of values, chooses one of the values as the 'pivot' element, and moves the other values so that lower values are on the left of the pivot element, and higher values are on the right of it.\nSpeed:\nIn this tutorial the last element of the array is chosen to be the pivot element, but we could also have chosen the first element of the array, or any element in the array really.\nThen, the Quicksort algorithm does the same operation recursively on the sub-arrays to the left and right side of the pivot element. This continues until the array is sorted.\nRecursion is when a function calls itself.\nAfter the Quicksort algorithm has put the pivot element in between a sub-array with lower values on the left side, and a sub-array with higher values on the right side, the algorithm calls itself twice, so that Quicksort runs again for the sub-array on the left side, and for the sub-array on the right side. The Quicksort algorithm continues to call itself until the sub-arrays are too small to be sorted.\nThe algorithm can be described like this:\nHow it works:\nChoose a value in the array to be the pivot element.\nOrder the rest of the array so that lower values than the pivot element are on the left, and higher values are on the right.\nSwap the pivot element with the first element of the higher values so that the pivot element lands in between the lower and higher values.\nDo the same operations (recursively) for the sub-arrays on the left and right side of the pivot element.\nContinue reading to fully understand the Quicksort algorithm and how to implement it yourself.\nManual Run Through\nBefore we implement the Quicksort algorithm in a programming language, let's manually run through a short array, just to get the idea.\nStep 1: We start with an unsorted array.\nStep 2: We choose the last value 3 as the pivot element.\nStep 3: The rest of the values in the array are all greater than 3, and must be on the right side of 3. Swap 3 with 11.\nStep 4: Value 3 is now in the correct position. We need to sort the values to the right of 3. We choose the last value 11 as the new pivot element.\nStep 5: The value 7 must be to the left of pivot value 11, and 12 must be to the right of it. Move 7 and 12.\nStep 6: Swap 11 with 12 so that lower values 9 and 7 are on the left side of 11, and 12 is on the right side.\nStep 7: 11 and 12 are in the correct positions. We choose 7 as the pivot element in sub-array [ 9, 7], to the left of 11.\nStep 8: We must swap 9 with 7.\nAnd now, the array is sorted.\nRun the simulation below to see the steps above animated:\nManual Run Through: What Happened?\nBefore we implement the algorithm in a programming language we need to go through what happened above in more detail.\nWe have already seen that last value of the array is chosen as the pivot element, and the rest of the values are arranged so that the values lower than the pivot value are to the left, and the higher values are to the right.\nAfter that, the pivot element is swapped with the first of the higher values. This splits the original array in two, with the pivot element in between the lower and the higher values.\nNow we need to do the same as above with the sub-arrays on the left and right side of the old pivot element. And if a sub-array has length 0 or 1, we consider it finished sorted.\nTo sum up, the Quicksort algorithm makes the sub-arrays become shorter and shorter until array is sorted.\nQuicksort Implementation\nTo write a 'quickSort' method that splits the array into shorter and shorter sub-arrays we use recursion. This means that the 'quickSort' method must call itself with the new sub-arrays to the left and right of the pivot element. Read more about recursion here.\nTo implement the Quicksort algorithm in a programming language, we need:\nAn array with values to sort.\nA quickSort method that calls itself (recursion) if the sub-array has a size larger than 1.\nA partition method that receives a sub-array, moves values around, swaps the pivot element into the sub-array and returns the index where the next split in sub-arrays happens.\nThe resulting code looks like this:\nExample\nQuicksort Time Complexity\nFor a general explanation of what time complexity is, visit this page.\nFor a more thorough and detailed explanation of Quicksort time complexity, visit this page.\nThe worst case scenario for Quicksort is O(n2)O(n^2) . This is when the pivot element is either the highest or lowest value in every sub-array, which leads to a lot of recursive calls. With our implementation above, this happens when the array is already sorted.\nBut on average, the time complexity for Quicksort is actually just O(nlogn)O(n \\log n) , which is a lot better than for the previous sorting algorithms we have looked at. That is why Quicksort is so popular.\nBelow you can see the significant improvement in time complexity for Quicksort in an average scenario O(nlogn)O(n \\log n) , compared to the previous sorting algorithms Bubble, Selection and Insertion Sort with time complexity O(n2)O(n^2) :\nThe recursion part of the Quicksort algorithm is actually a reason why the average sorting scenario is so fast, because for good picks of the pivot element, the array will be split in half somewhat evenly each time the algorithm calls itself. So the number of recursive calls do not double, even if the number of values nn double.\nRun Quicksort on different kinds of arrays with different number of values in the simulation below:\nSet values: 300\nRandom\nDescending\nAscending\n10 Random\nOperations: 0\nDSA Exercises\nTest Yourself With Exercises\nExercise:\nComplete the code for the Quicksort algorithm.\nStart the Exercise",
      "examples": [
        "def partition(array, low, high): pivot = array[high] i = low - 1 for j in range(low, high): if array[j] <= pivot: i += 1 array[i], array[j] = array[j], array[i] array[i+1], array[high] = array[high], array[i+1] return i+1 def quicksort(array, low=0, high=None): if high is None: high = len(array) - 1 if low < high: pivot_index = partition(array, low, high) quicksort(array, low, pivot_index-1) quicksort(array, pivot_index+1, high) my_array = [64, 34, 25, 12, 22, 11, 90, 5] quicksort(my_array) print(\"Sorted array:\", my_array)",
        "[ 11, 9, 12, 7, 3]",
        "[ 11, 9, 12, 7,\n3\n]",
        "[\n3\n, 9, 12, 7,\n11\n]",
        "[ 3, 9, 12, 7,\n11\n]",
        "[ 3, 9,\n7, 12\n, 11]",
        "[ 3, 9, 7,\n11, 12\n]",
        "[ 3, 9,\n7\n, 11, 12]",
        "[ 3,\n7, 9\n, 11, 12]",
        "def partition(array, low, high): pivot = array[high] i = low - 1 for j in range(low, high): if array[j] <= pivot: i += 1 array[i], array[j] = array[j], array[i] array[i+1], array[high] = array[high], array[i+1] return i+1 def quicksort(array, low=0, high=None): if high is None: high = len(array) - 1 if low < high: pivot_index = partition(array, low, high) (array, low, pivot_index-1) (array, pivot_index+1, high) my_array = [64, 34, 25, 12, 22, 11, 90, 5] quicksort(my_array) print(\"Sorted array:\", my_array)"
      ]
    },
    {
      "title": "DSA Counting Sort",
      "summary": "Counting Sort\nThe Counting Sort algorithm sorts an array by counting the number of times each value occurs.\nSpeed:\nRun the simulation to see how 17 integer values from 1 till 5 are sorted using Counting Sort.\nCounting Sort does not compare values like the previous sorting algorithms we have looked at, and only works on non negative integers.\nFurthermore, Counting Sort is fast when the range of possible values kk is smaller than the number of values nn.\nHow it works:\nCreate a new array for counting how many there are of the different values.\nGo through the array that needs to be sorted.\nFor each value, count it by increasing the counting array at the corresponding index.\nAfter counting the values, go through the counting array to create the sorted array.\nFor each count in the counting array, create the correct number of elements, with values that correspond to the counting array index.\nConditions for Counting Sort\nThese are the reasons why Counting Sort is said to only work for a limited range of non-negative integer values:\nInteger values: Counting Sort relies on counting occurrences of distinct values, so they must be integers. With integers, each value fits with an index (for non negative values), and there is a limited number of different values, so that the number of possible different values kk is not too big compared to the number of values nn.\nNon negative values: Counting Sort is usually implemented by creating an array for counting. When the algorithm goes through the values to be sorted, value x is counted by increasing the counting array value at index x. If we tried sorting negative values, we would get in trouble with sorting value -3, because index -3 would be outside the counting array.\nLimited range of values: If the number of possible different values to be sorted kk is larger than the number of values to be sorted nn, the counting array we need for sorting will be larger than the original array we have that needs sorting, and the algorithm becomes ineffective.\nManual Run Through\nBefore we implement the Counting Sort algorithm in a programming language, let's manually run through a short array, just to get the idea.\nStep 1: We start with an unsorted array.\nStep 2: We create another array for counting how many there are of each value. The array has 4 elements, to hold values 0 through 3.\nStep 3: Now let's start counting. The first element is 2, so we must increment the counting array element at index 2.\nStep 4: After counting a value, we can remove it, and count the next value, which is 3.\nStep 5: The next value we count is 0, so we increment index 0 in the counting array.\nStep 6: We continue like this until all values are counted.\nStep 7: Now we will recreate the elements from the initial array, and we will do it so that the elements are ordered lowest to highest.\nThe first element in the counting array tells us that we have 1 element with value 0. So we push 1 element with value 0 into the array, and we decrease the element at index 0 in the counting array with 1.\nStep 8: From the counting array we see that we do not need to create any elements with value 1.\nStep 9: We push 3 elements with value 2 into the end of the array. And as we create these elements we also decrease the counting array at index 2.\nStep 10: At last we must add 2 elements with value 3 at the end of the array.\nFinally! The array is sorted.\nRun the simulation below to see the steps above animated:\nManual Run Through: What Happened?\nBefore we implement the algorithm in a programming language we need to go through what happened above in more detail.\nWe have seen that the Counting Sort algorithm works in two steps:\nEach value gets counted by incrementing at the correct index in the counting array. After a value is counted, it is removed.\nThe values are recreated in the right order by using the count, and the index of the count, from the counting array.\nWith this in mind, we can start implementing the algorithm using Python.\nCounting Sort Implementation\nTo implement the Counting Sort algorithm in a programming language, we need:\nAn array with values to sort.\nA 'countingSort' method that receives an array of integers.\nAn array inside the method to keep count of the values.\nA loop inside the method that counts and removes values, by incrementing elements in the counting array.\nA loop inside the method that recreates the array by using the counting array, so that the elements appear in the right order.\nOne more thing: We need to find out what the highest value in the array is, so that the counting array can be created with the correct size. For example, if the highest value is 5, the counting array must be 6 elements in total, to be able count all possible non negative integers 0, 1, 2, 3, 4 and 5.\nThe resulting code looks like this:\nExample\nCounting Sort Time Complexity\nFor a general explanation of what time complexity is, visit this page.\nFor a more thorough and detailed explanation of Counting Sort time complexity, visit this page.\nHow fast the Counting Sort algorithm runs depends on both the range of possible values kk and the number of values nn.\nIn general, time complexity for Counting Sort is O(n+k)O(n+k).\nIn a best case scenario, the range of possible different values kk is very small compared to the number of values nn and Counting Sort has time complexity O(n)O(n).\nBut in a worst case scenario, the range of possible different values kk is very big compared to the number of values nn and Counting Sort can have time complexity O(n2)O(n^2) or even worse.\nThe plot below shows how much the time complexity for Counting Sort can vary.\nAs you can see, it is important to consider the range of values compared to the number of values to be sorted before choosing Counting Sort as your algorithm. Also, as mentioned at the top of the page, keep in mind that Counting Sort only works for non negative integer values.\nRun different simulations of Counting Sort to see how the number of operations falls between the worst case scenario O(n2)O(n^2) (red line) and best case scenario O(n)O(n) (green line).\nSet values (n): 300\nRange (k), from 0 to: 1000\nRandom\nDescending\nAscending\n10 Random\nOperations: 0\nAs mentioned previously: if the numbers to be sorted varies a lot in value (large kk), and there are few numbers to sort (small nn), the Counting Sort algorithm is not effective.\nIf we hold nn and kk fixed, the \"Random\", \"Descending\" and \"Ascending\" alternatives in the simulation above results in the same number of operations. This is because the same thing happens in all three cases: A counting array is set up, the numbers are counted, and the new sorted array is created.\nDSA Exercises\nTest Yourself With Exercises\nExercise:\nUsing Counting Sort on this array:\nHow does the counting array look like?\nStart the Exercise",
      "examples": [
        "def countingSort(arr): max_val = max(arr) count = [0] * (max_val + 1) while len(arr) > 0: num = arr.pop(0) count[num] += 1 for i in range(len(count)): while count[i] > 0: arr.append(i) count[i] -= 1 return arr unsortedArr = [4, 2, 2, 6, 3, 3, 1, 6, 5, 2, 3] sortedArr = countingSort(unsortedArr) print(\"Sorted array:\", sortedArr)",
        "myArray = [ 2, 3, 0, 2, 3, 2]",
        "myArray = [ 2, 3, 0, 2, 3, 2]\ncountArray = [ 0, 0, 0, 0]",
        "myArray = [\n2\n, 3, 0, 2, 3, 2]\ncountArray = [ 0, 0,\n1\n, 0]",
        "myArray = [\n3\n, 0, 2, 3, 2]\ncountArray = [ 0, 0, 1,\n1\n]",
        "myArray = [\n0\n, 2, 3, 2]\ncountArray = [\n1\n, 0, 1, 1]",
        "myArray = [ ]\ncountArray = [\n1, 0, 3, 2\n]",
        "myArray = [\n0\n]\ncountArray = [\n0\n, 0, 3, 2]",
        "myArray = [ 0]\ncountArray = [ 0,\n0\n, 3, 2]",
        "myArray = [ 0,\n2, 2, 2\n]\ncountArray = [ 0, 0,\n0\n, 2]",
        "myArray = [0, 2, 2, 2,\n3, 3\n]\ncountArray = [ 0, 0, 0,\n0\n]",
        "[,,,4,2,1]",
        "countArray = [ 0, 0, 0, 0]",
        "myArray = [\n2\n, 3, 0, 2, 3, 2]",
        "countArray = [ 0, 0,\n1\n, 0]",
        "myArray = [\n3\n, 0, 2, 3, 2]",
        "countArray = [ 0, 0, 1,\n1\n]",
        "myArray = [\n0\n, 2, 3, 2]",
        "countArray = [\n1\n, 0, 1, 1]",
        "myArray = [ ]",
        "countArray = [\n1, 0, 3, 2\n]",
        "myArray = [\n0\n]",
        "countArray = [\n0\n, 0, 3, 2]",
        "myArray = [ 0]",
        "countArray = [ 0,\n0\n, 3, 2]",
        "myArray = [ 0,\n2, 2, 2\n]",
        "countArray = [ 0, 0,\n0\n, 2]",
        "myArray = [0, 2, 2, 2,\n3, 3\n]",
        "countArray = [ 0, 0, 0,\n0\n]",
        "[1,0,5,3,3,1,3,3,4,4]"
      ]
    },
    {
      "title": "DSA Radix Sort",
      "summary": "Radix Sort\nThe Radix Sort algorithm sorts an array by individual digits, starting with the least significant digit (the rightmost one).\nClick the button to do Radix Sort, one step (digit) at a time.\nThe radix (or base) is the number of unique digits in a number system. In the decimal system we normally use, there are 10 different digits from 0 till 9.\nRadix Sort uses the radix so that decimal values are put into 10 different buckets (or containers) corresponding to the digit that is in focus, then put back into the array before moving on to the next digit.\nRadix Sort is a non comparative algorithm that only works with non negative integers.\nThe Radix Sort algorithm can be described like this:\nHow it works:\nStart with the least significant digit (rightmost digit).\nSort the values based on the digit in focus by first putting the values in the correct bucket based on the digit in focus, and then put them back into array in the correct order.\nMove to the next digit, and sort again, like in the step above, until there are no digits left.\nStable Sorting\nRadix Sort must sort the elements in a stable way for the result to be sorted correctly.\nA stable sorting algorithm is an algorithm that keeps the order of elements with the same value before and after the sorting. Let's say we have two elements \"K\" and \"L\", where \"K\" comes before \"L\", and they both have value \"3\". A sorting algorithm is considered stable if element \"K\" still comes before \"L\" after the array is sorted.\nIt makes little sense to talk about stable sorting algorithms for the previous algorithms we have looked at individually, because the result would be same if they are stable or not. But it is important for Radix Sort that the the sorting is done in a stable way because the elements are sorted by just one digit at a time.\nSo after sorting the elements on the least significant digit and moving to the next digit, it is important to not destroy the sorting work that has already been done on the previous digit position, and that is why we need to take care that Radix Sort does the sorting on each digit position in a stable way.\nIn the simulation below it is revealed how the underlying sorting into buckets is done. And to get a better understanding of how stable sorting works, you can also choose to sort in an unstable way, that will lead to an incorrect result. The sorting is made unstable by simply putting elements into buckets from the end of the array instead of from the start of the array.\nSpeed:\nStable sort? Yes\nManual Run Through\nLet's try to do the sorting manually, just to get an even better understanding of how Radix Sort works before actually implementing it in a programming language.\nStep 1: We start with an unsorted array, and an empty array to fit values with corresponding radices 0 till 9.\nStep 2: We start sorting by focusing on the least significant digit.\nStep 3: Now we move the elements into the correct positions in the radix array according to the digit in focus. Elements are taken from the start of myArray and pushed into the correct position in the radixArray.\nStep 4: We move the elements back into the initial array, and the sorting is now done for the least significant digit. Elements are taken from the end radixArray, and put into the start of myArray.\nStep 5: We move focus to the next digit. Notice that values 45 and 25 are still in the same order relative to each other as they were to start with, because we sort in a stable way.\nStep 6: We move elements into the radix array according to the focused digit.\nStep 7: We move elements back into the start of myArray, from the back of radixArray.\nThe sorting is finished!\nRun the simulation below to see the steps above animated:\nManual Run Through: What Happened?\nWe see that values are moved from the array and placed in the radix array according to the current radix in focus. And then the values are moved back into the array we want to sort.\nThis moving of values from the array we want to sort and back again must be done as many times as the maximum number of digits in a value. So for example if 437 is the highest number in the array that needs to be sorted, we know we must sort three times, once for each digit.\nWe also see that the radix array needs to be two-dimensional so that more than one value on a specific radix, or index.\nAnd, as mentioned earlier, we must move values between the two arrays in a way that keeps the order of values with the same radix in focus, so the the sorting is stable.\nRadix Sort Implementation\nTo implement the Radix Sort algorithm we need:\nAn array with non negative integers that needs to be sorted.\nA two dimensional array with index 0 to 9 to hold values with the current radix in focus.\nA loop that takes values from the unsorted array and places them in the correct position in the two dimensional radix array.\nA loop that puts values back into the initial array from the radix array.\nAn outer loop that runs as many times as there are digits in the highest value.\nThe resulting code looks like this:\nExample\nOn line 7, we use floor division (\"//\") to divide the maximum value 802 by 1 the first time the while loop runs, the next time it is divided by 10, and the last time it is divided by 100. When using floor division \"//\", any number beyond the decimal point are disregarded, and an integer is returned.\nOn line 11, it is decided where to put a value in the radixArray based on its radix, or digit in focus. For example, the second time the outer while loop runs exp will be 10. Value 170 divided by 10 will be 17. The \"%10\" operation divides by 10 and returns what is left. In this case 17 is divided by 10 one time, and 7 is left. So value 170 is placed in index 7 in the radixArray.\nRadix Sort Using Other Sorting Algorithms\nRadix Sort can actually be implemented together with any other sorting algorithm as long as it is stable. This means that when it comes down to sorting on a specific digit, any stable sorting algorithm will work, such as counting sort or bubble sort.\nThis is an implementation of Radix Sort that uses Bubble Sort to sort on the individual digits:\nExample\nRadix Sort Time Complexity\nFor a general explanation of what time complexity is, visit this page.\nFor a more thorough and detailed explanation of Radix Sort time complexity, visit this page.\nThe time complexity for Radix Sort is:\nO(n⋅k)__ \\underline{\\underline{O(n \\cdot k)}}\nThis means that Radix Sort depends both on the values that need to be sorted nn, and the number of digits in the highest value kk.\nA best case scenario for Radix Sort is if there are lots of values to sort, but the values have few digits. For example if there are more than a million values to sort, and the highest value is 999, with just three digits. In such a case the time complexity O(n⋅k)O(n \\cdot k) can be simplified to just O(n)O(n).\nA worst case scenario for Radix Sort would be if there are as many digits in the highest value as there are values to sort. This is perhaps not a common scenario, but the time complexity would be O(n2)O(n^2)in this case.\nThe most average or common case is perhaps if the number of digits kk is something like k(n)=lognk(n)= \\log n. If so, Radix Sort gets time complexity O(n⋅logn)O(n \\cdot \\log n ). An example of such a case would be if there are 1000000 values to sort, and the values have 6 digits.\nSee different possible time complexities for Radix Sort in the image below.\nRun different simulations of Radix Sort to see how the number of operations falls between the worst case scenario O(n2)O(n^2) (red line) and best case scenario O(n)O(n) (green line).\nSet values (n): 300\nDigits (k): 4\nRandom\nDescending\nAscending\n10 Random\nOperations: 0\nThe bars representing the different values are scaled to fit the window, so that it looks ok. This means that values with 7 digits look like they are just 5 times bigger than values with 2 digits, but in reality, values with 7 digits are actually 5000 times bigger than values with 2 digits!\nIf we hold nn and kk fixed, the \"Random\", \"Descending\" and \"Ascending\" alternatives in the simulation above results in the same number of operations. This is because the same thing happens in all three cases.\nDSA Exercises\nTest Yourself With Exercises\nExercise:\nTo sort an array with Radix Sort, what property must the sorting have for the sorting to be done properly?\nStart the Exercise",
      "examples": [
        "myArray = [170, 45, 75, 90, 802, 24, 2, 66] print(\"Original array:\", myArray) radixArray = [[], [], [], [], [], [], [], [], [], []] maxVal = max(myArray) exp = 1 while maxVal // exp > 0: while len(myArray) > 0: val = myArray.pop() radixIndex = (val // exp) % 10 radixArray[radixIndex].append(val) for bucket in radixArray: while len(bucket) > 0: val = bucket.pop() myArray.append(val) exp *= 10 print(\"Sorted array:\", myArray)",
        "def bubbleSort(arr): n = len(arr) for i in range(n): for j in range(0, n - i - 1): if arr[j] > arr[j + 1]: arr[j], arr[j + 1] = arr[j + 1], arr[j] def radixSortWithBubbleSort(arr): max_val = max(arr) exp = 1 while max_val // exp > 0: radixArray = [[],[],[],[],[],[],[],[],[],[]] for num in arr: radixIndex = (num // exp) % 10 radixArray[radixIndex].append(num) for bucket in radixArray: bubbleSort(bucket) i = 0 for bucket in radixArray: for num in bucket: arr[i] = num i += 1 exp *= 10 myArray = [170, 45, 75, 90, 802, 24, 2, 66] print(\"Original array:\", myArray) radixSortWithBubbleSort(myArray) print(\"Sorted array:\", myArray)",
        "myArray = [ 33, 45, 40, 25, 17, 24]\nradixArray = [ [], [], [], [], [], [], [], [], [], [] ]",
        "myArray = [ 3\n3\n, 4\n5\n, 4\n0\n, 2\n5\n, 1\n7\n, 2\n4\n]\nradixArray = [ [], [], [], [], [], [], [], [], [], [] ]",
        "myArray = [ ]\nradixArray = [ [4\n0\n], [], [], [3\n3\n], [2\n4\n], [4\n5\n, 2\n5\n], [], [1\n7\n], [], [] ]",
        "myArray = [ 4\n0\n, 3\n3\n, 2\n4\n, 4\n5\n, 2\n5\n, 1\n7\n]\nradixArray = [ [], [], [], [], [], [], [], [], [], [] ]",
        "myArray = [\n4\n0,\n3\n3,\n2\n4,\n4\n5,\n2\n5,\n1\n7 ]\nradixArray = [ [], [], [], [], [], [], [], [], [], [] ]",
        "myArray = [ ]\nradixArray = [ [], [\n1\n7], [\n2\n4,\n2\n5], [\n3\n3], [\n4\n0,\n4\n5], [], [], [], [], [] ]",
        "myArray = [\n1\n7,\n2\n4,\n2\n5,\n3\n3,\n4\n0,\n4\n5 ]\nradixArray = [ [], [], [], [], [], [], [], [], [], [] ]",
        "Radix Sort must use a sorting algorithm.",
        "myArray = [ 33, 45, 40, 25, 17, 24]",
        "radixArray = [ [], [], [], [], [], [], [], [], [], [] ]",
        "myArray = [ 3\n3\n, 4\n5\n, 4\n0\n, 2\n5\n, 1\n7\n, 2\n4\n]",
        "myArray = [ ]",
        "radixArray = [ [4\n0\n], [], [], [3\n3\n], [2\n4\n], [4\n5\n, 2\n5\n], [], [1\n7\n], [], [] ]",
        "myArray = [ 4\n0\n, 3\n3\n, 2\n4\n, 4\n5\n, 2\n5\n, 1\n7\n]",
        "myArray = [\n4\n0,\n3\n3,\n2\n4,\n4\n5,\n2\n5,\n1\n7 ]",
        "radixArray = [ [], [\n1\n7], [\n2\n4,\n2\n5], [\n3\n3], [\n4\n0,\n4\n5], [], [], [], [], [] ]",
        "myArray = [\n1\n7,\n2\n4,\n2\n5,\n3\n3,\n4\n0,\n4\n5 ]"
      ]
    },
    {
      "title": "DSA Merge Sort",
      "summary": "Merge Sort\nThe Merge Sort algorithm is a divide-and-conquer algorithm that sorts an array by first breaking it down into smaller arrays, and then building the array back together the correct way so that it is sorted.\nSpeed:\nDivide: The algorithm starts with breaking up the array into smaller and smaller pieces until one such sub-array only consists of one element.\nConquer: The algorithm merges the small pieces of the array back together by putting the lowest values first, resulting in a sorted array.\nThe breaking down and building up of the array to sort the array is done recursively.\nIn the animation above, each time the bars are pushed down represents a recursive call, splitting the array into smaller pieces. When the bars are lifted up, it means that two sub-arrays have been merged together.\nThe Merge Sort algorithm can be described like this:\nHow it works:\nDivide the unsorted array into two sub-arrays, half the size of the original.\nContinue to divide the sub-arrays as long as the current piece of the array has more than one element.\nMerge two sub-arrays together by always putting the lowest value first.\nKeep merging until there are no sub-arrays left.\nTake a look at the drawing below to see how Merge Sort works from a different perspective. As you can see, the array is split into smaller and smaller pieces until it is merged back together. And as the merging happens, values from each sub-array are compared so that the lowest value comes first.\nManual Run Through\nLet's try to do the sorting manually, just to get an even better understanding of how Merge Sort works before actually implementing it in a programming language.\nStep 1: We start with an unsorted array, and we know that it splits in half until the sub-arrays only consist of one element. The Merge Sort function calls itself two times, once for each half of the array. That means that the first sub-array will split into the smallest pieces first.\nStep 2: The splitting of the first sub-array is finished, and now it is time to merge. 8 and 9 are the first two elements to be merged. 8 is the lowest value, so that comes before 9 in the first merged sub-array.\nStep 3: The next sub-arrays to be merged is [ 12] and [ 8, 9]. Values in both arrays are compared from the start. 8 is lower than 12, so 8 comes first, and 9 is also lower than 12.\nStep 4: Now the second big sub-array is split recursively.\nStep 5: 3 and 11 are merged back together in the same order as they are shown because 3 is lower than 11.\nStep 6: Sub-array with values 5 and 4 is split, then merged so that 4 comes before 5.\nStep 7: The two sub-arrays on the right are merged. Comparisons are done to create elements in the new merged array:\n3 is lower than 4\n4 is lower than 11\n5 is lower than 11\n11 is the last remaining value\nStep 8: The two last remaining sub-arrays are merged. Let's look at how the comparisons are done in more detail to create the new merged and finished sorted array:\n3 is lower than 8:\nStep 9: 4 is lower than 8:\nStep 10: 5 is lower than 8:\nStep 11: 8 and 9 are lower than 11:\nStep 12: 11 is lower than 12:\nThe sorting is finished!\nRun the simulation below to see the steps above animated:\nManual Run Through: What Happened?\nWe see that the algorithm has two stages: first splitting, then merging.\nAlthough it is possible to implement the Merge Sort algorithm without recursion, we will use recursion because that is the most common approach.\nWe cannot see it in the steps above, but to split an array in two, the length of the array is divided by two, and then rounded down to get a value we call \"mid\". This \"mid\" value is used as an index for where to split the array.\nAfter the array is split, the sorting function calls itself with each half, so that the array can be split again recursively. The splitting stops when a sub-array only consists of one element.\nAt the end of the Merge Sort function the sub-arrays are merged so that the sub-arrays are always sorted as the array is built back up. To merge two sub-arrays so that the result is sorted, the values of each sub-array are compared, and the lowest value is put into the merged array. After that the next value in each of the two sub-arrays are compared, putting the lowest one into the merged array.\nMerge Sort Implementation\nTo implement the Merge Sort algorithm we need:\nAn array with values that needs to be sorted.\nA function that takes an array, splits it in two, and calls itself with each half of that array so that the arrays are split again and again recursively, until a sub-array only consist of one value.\nAnother function that merges the sub-arrays back together in a sorted way.\nThe resulting code looks like this:\nExample\nOn line 6, arr[:mid] takes all values from the array up until, but not including, the value on index \"mid\".\nOn line 7, arr[mid:] takes all values from the array, starting at the value on index \"mid\" and all the next values.\nOn lines 26-27, the first part of the merging is done. At this this point the values of the two sub-arrays are compared, and either the left sub-array or the right sub-array is empty, so the result array can just be filled with the remaining values from either the left or the right sub-array. These lines can be swapped, and the result will be the same.\nMerge Sort without Recursion\nSince Merge Sort is a divide and conquer algorithm, recursion is the most intuitive code to use for implementation. The recursive implementation of Merge Sort is also perhaps easier to understand, and uses less code lines in general.\nBut Merge Sort can also be implemented without the use of recursion, so that there is no function calling itself.\nTake a look at the Merge Sort implementation below, that does not use recursion:\nExample\nYou might notice that the merge functions are exactly the same in the two Merge Sort implementations above, but in the implementation right above here the while loop inside the mergeSort function is used to replace the recursion. The while loop does the splitting and merging of the array in place, and that makes the code a bit harder to understand.\nTo put it simply, the while loop inside the mergeSort function uses short step lengths to sort tiny pieces (sub-arrays) of the initial array using the merge function. Then the step length is increased to merge and sort larger pieces of the array until the whole array is sorted.\nMerge Sort Time Complexity\nFor a general explanation of what time complexity is, visit this page.\nFor a more thorough and detailed explanation of Merge Sort time complexity, visit this page.\nThe time complexity for Merge Sort is\nO(n⋅logn) O( n \\cdot \\log n )\nAnd the time complexity is pretty much the same for different kinds of arrays. The algorithm needs to split the array and merge it back together whether it is already sorted or completely shuffled.\nThe image below shows the time complexity for Merge Sort.\nRun the simulation below for different number of values in an array, and see how the number of operations Merge Sort needs on an array of nn elements is O(nlogn)O(n \\log n):\nSet values: 300\nRandom\nDescending\nAscending\n10 Random\nOperations: 0\nIf we hold the number of values nn fixed, the number of operations needed for the \"Random\", \"Descending\" and \"Ascending\" is almost the same.\nMerge Sort performs almost the same every time because the array is split, and merged using comparison, both if the array is already sorted or not.",
      "examples": [
        "def mergeSort(arr): if len(arr) <= 1: return arr mid = len(arr) // 2 leftHalf = arr[:mid] rightHalf = arr[mid:] sortedLeft = mergeSort(leftHalf) sortedRight = mergeSort(rightHalf) return merge(sortedLeft, sortedRight) def merge(left, right): result = [] i = j = 0 while i < len(left) and j < len(right): if left[i] < right[j]: result.append(left[i]) i += 1 else: result.append(right[j]) j += 1 result.extend(left[i:]) result.extend(right[j:]) return result unsortedArr = [3, 7, 6, -10, 15, 23.5, 55, -13] sortedArr = mergeSort(unsortedArr) print(\"Sorted array:\", sortedArr)",
        "def merge(left, right): result = [] i = j = 0 while i < len(left) and j < len(right): if left[i] < right[j]: result.append(left[i]) i += 1 else: result.append(right[j]) j += 1 result.extend(left[i:]) result.extend(right[j:]) return result def mergeSort(arr): step = 1 # Starting with sub-arrays of length 1 length = len(arr) while step < length: for i in range(0, length, 2 * step): left = arr[i:i + step] right = arr[i + step:i + 2 * step] merged = merge(left, right) # Place the merged array back into the original array for j, val in enumerate(merged): arr[i + j] = val step *= 2 # Double the sub-array length for the next iteration return arr unsortedArr = [3, 7, 6, -10, 15, 23.5, 55, -13] sortedArr = mergeSort(unsortedArr) print(\"Sorted array:\", sortedArr)",
        "[ 12, 8, 9, 3, 11, 5, 4]\n[ 12, 8, 9] [ 3, 11, 5, 4]\n[ 12] [ 8, 9] [ 3, 11, 5, 4]\n[ 12] [ 8] [ 9] [ 3, 11, 5, 4]",
        "[ 12] [\n8\n,\n9\n] [ 3, 11, 5, 4]",
        "[\n8\n,\n9\n,\n12\n] [ 3, 11, 5, 4]",
        "[ 8, 9, 12] [ 3, 11, 5, 4]\n[ 8, 9, 12] [ 3, 11] [ 5, 4]\n[ 8, 9, 12] [ 3] [ 11] [ 5, 4]",
        "[ 8, 9, 12] [\n3\n,\n11\n] [ 5, 4]",
        "[ 8, 9, 12] [ 3, 11] [\n5\n] [\n4\n]\n[ 8, 9, 12] [ 3, 11] [\n4\n,\n5\n]",
        "[ 8, 9, 12] [\n3\n,\n4\n,\n5\n,\n11\n]",
        "Before [\n8\n, 9, 12] [\n3\n, 4, 5, 11]\nAfter: [\n3\n,\n8\n, 9, 12] [ 4, 5, 11]",
        "Before [ 3,\n8\n, 9, 12] [\n4\n, 5, 11]\nAfter: [ 3,\n4\n,\n8\n, 9, 12] [ 5, 11]",
        "Before [ 3, 4,\n8\n, 9, 12] [\n5\n, 11]\nAfter: [ 3, 4,\n5\n,\n8\n, 9, 12] [ 11]",
        "Before [ 3, 4, 5,\n8\n,\n9\n, 12] [\n11\n]\nAfter: [ 3, 4, 5,\n8\n,\n9\n, 12] [\n11\n]",
        "Before [ 3, 4, 5, 8, 9,\n12\n] [\n11\n]\nAfter: [ 3, 4, 5, 8, 9,\n11\n,\n12\n]",
        "[",
        "12",
        ",",
        "8",
        "9",
        "3",
        "11",
        "5",
        "4",
        "]",
        "[ 12, 8, 9, 3, 11, 5, 4]",
        "[ 12, 8, 9] [ 3, 11, 5, 4]",
        "[ 12] [ 8, 9] [ 3, 11, 5, 4]",
        "[ 12] [ 8] [ 9] [ 3, 11, 5, 4]",
        "[ 8, 9, 12] [ 3, 11, 5, 4]",
        "[ 8, 9, 12] [ 3, 11] [ 5, 4]",
        "[ 8, 9, 12] [ 3] [ 11] [ 5, 4]",
        "[ 8, 9, 12] [ 3, 11] [\n5\n] [\n4\n]",
        "[ 8, 9, 12] [ 3, 11] [\n4\n,\n5\n]",
        "Before [\n8\n, 9, 12] [\n3\n, 4, 5, 11]",
        "After: [\n3\n,\n8\n, 9, 12] [ 4, 5, 11]",
        "Before [ 3,\n8\n, 9, 12] [\n4\n, 5, 11]",
        "After: [ 3,\n4\n,\n8\n, 9, 12] [ 5, 11]",
        "Before [ 3, 4,\n8\n, 9, 12] [\n5\n, 11]",
        "After: [ 3, 4,\n5\n,\n8\n, 9, 12] [ 11]",
        "Before [ 3, 4, 5,\n8\n,\n9\n, 12] [\n11\n]",
        "After: [ 3, 4, 5,\n8\n,\n9\n, 12] [\n11\n]",
        "Before [ 3, 4, 5, 8, 9,\n12\n] [\n11\n]",
        "After: [ 3, 4, 5, 8, 9,\n11\n,\n12\n]"
      ]
    },
    {
      "title": "DSA Linear Search",
      "summary": "Linear Search\nThe Linear Search algorithm searches through an array and returns the index of the value it searches for.\nSpeed:\nFind value:\nCurrent value:\nRun the simulation above to see how the Linear Search algorithm works.\nToo see what happens when a value is not found, try to find value 5.\nThis algorithm is very simple and easy to understand and implement.\nIf the array is already sorted, it is better to use the much faster Binary Search algorithm that we will explore on the next page.\nA big difference between sorting algorithms and searching algorithms is that sorting algorithms modify the array, but searching algorithms leave the array unchanged.\nHow it works:\nGo through the array value by value from the start.\nCompare each value to check if it is equal to the value we are looking for.\nIf the value is found, return the index of that value.\nIf the end of the array is reached and the value is not found, return -1 to indicate that the value was not found.\nManual Run Through\nLet's try to do the searching manually, just to get an even better understanding of how Linear Search works before actually implementing it in a programming language. We will search for value 11.\nStep 1: We start with an array of random values.\nStep 2: We look at the first value in the array, is it equal to 11?\nStep 3: We move on to the next value at index 1, and compare it to 11 to see if it is equal.\nStep 4: We check the next value at index 2.\nStep 5: We move on to the next value at index 3. Is it equal to 11?\nWe have found it!\nValue 11 is found at index 3.\nReturning index position 3.\nLinear Search is finished.\nRun the simulation below to see the steps above animated:\nManual Run Through: What Happened?\nThis algorithm is really straight forward.\nEvery value is checked from the start of the array to see if the value is equal to 11, the value we are trying to find.\nWhen the value is found, the searching is stopped, and the index where the value is found is returned.\nIf the array is searched through without finding the value, -1 is returned.\nLinear Search Implementation\nTo implement the Linear Search algorithm we need:\nAn array with values to search through.\nA target value to search for.\nA loop that goes through the array from start to end.\nAn if-statement that compares the current value with the target value, and returns the current index if the target value is found.\nAfter the loop, return -1, because at this point we know the target value has not been found.\nThe resulting code for Linear Search looks like this:\nExample\nLinear Search Time Complexity\nFor a general explanation of what time complexity is, visit this page.\nFor a more thorough and detailed explanation of Insertion Sort time complexity, visit this page.\nIf Linear Search runs and finds the target value as the first array value in an array with nn values, only one compare is needed.\nBut if Linear Search runs through the whole array of nn values, without finding the target value, nn compares are needed.\nThis means that time complexity for Linear Search is\nO(n) O(n)\nIf we draw how much time Linear Search needs to find a value in an array of nn values, we get this graph:\nRun the simulation below for different number of values in an array, and see how many compares are needed for Linear Search to find a value in an array of nn values:\nSet values: 300\nRandom\nDescending\nAscending\n10 Random\nOperations: 0\nNot found!\nChoosing \"Random\", \"Descending\" or \"Ascending\" in the simulation above has no effect on how fast Linear Search is.\nDSA Exercises\nTest Yourself With Exercises\nExercise:\nComplete the code.\nStart the Exercise",
      "examples": [
        "def linearSearch(arr, targetVal): for i in range(len(arr)): if arr[i] == targetVal: return i return -1 arr = [3, 7, 2, 9, 5] targetVal = 9 result = linearSearch(arr, targetVal) if result != -1: print(\"Value\",targetVal,\"found at index\",result) else: print(\"Value\",targetVal,\"not found\")",
        "[ 12, 8, 9, 11, 5, 11]",
        "[\n12\n, 8, 9, 11, 5, 11]",
        "[ 12,\n8\n, 9, 11, 5, 11]",
        "[ 12, 8,\n9\n, 11, 5, 11]",
        "[ 12, 8, 9,\n11\n, 5, 11]",
        "def linearSearch(arr, targetVal): for i in range(len(arr)): if arr[i] == targetVal: return -1"
      ]
    },
    {
      "title": "DSA Binary Search",
      "summary": "Binary Search\nThe Binary Search algorithm searches through an array and returns the index of the value it searches for.\nSpeed:\nFind value:\nCurrent value:\nRun the simulation to see how the Binary Search algorithm works.\nToo see what happens when a value is not found, try to find value 5.\nBinary Search is much faster than Linear Search, but requires a sorted array to work.\nThe Binary Search algorithm works by checking the value in the center of the array. If the target value is lower, the next value to check is in the center of the left half of the array. This way of searching means that the search area is always half of the previous search area, and this is why the Binary Search algorithm is so fast.\nThis process of halving the search area happens until the target value is found, or until the search area of the array is empty.\nHow it works:\nCheck the value in the center of the array.\nIf the target value is lower, search the left half of the array. If the target value is higher, search the right half.\nContinue step 1 and 2 for the new reduced part of the array until the target value is found or until the search area is empty.\nIf the value is found, return the target value index. If the target value is not found, return -1.\nManual Run Through\nLet's try to do the searching manually, just to get an even better understanding of how Binary Search works before actually implementing it in a programming language. We will search for value 11.\nStep 1: We start with an array.\nStep 2: The value in the middle of the array at index 3, is it equal to 11?\nStep 3: 7 is less than 11, so we must search for 11 to the right of index 3. The values to the right of index 3 are [ 11, 15, 25]. The next value to check is the middle value 15, at index 5.\nStep 4: 15 is higher than 11, so we must search to the left of index 5. We have already checked index 0-3, so index 4 is only value left to check.\nWe have found it!\nValue 11 is found at index 4.\nReturning index position 4.\nBinary Search is finished.\nRun the simulation below to see the steps above animated:\nManual Run Through: What Happened?\nTo start with, the algorithm has two variables \"left\" and \"right\".\n\"left\" is 0 and represents the index of the first value in the array, and \"right\" is 6 and represents the index of the last value in the array.\n(left+right)/2=(0+6)/2=3(left+right)/2=(0+6)/2=3 is the first index used to check if the middle value (7) is equal to the target value (11).\n7 is lower than the target value 11, so in the next loop the search area must be limited to the right side of the middle value: [ 11, 15, 25], on index 4-6.\nTo limit the search area and find a new middle value, \"left\" is updated to index 4, \"right\" is still 6. 4 and 6 are the indexes for the first and last values in the new search area, the right side of the previous middle value. The new middle value index is (left+right)/2=(4+6)/2=10/2=5(left+right)/2=(4+6)/2=10/2=5.\nThe new middle value on index 5 is checked: 15 is higher than 11, so if the target value 11 exists in the array it must be on the left side of index 5. The new search area is created by updating \"right\" from 6 to 4. Now both \"left\" and \"right\" is 4, (left+right)/2=(4+4)/2=4(left+right)/2=(4+4)/2=4, so there is only index 4 left to check. The target value 11 is found at index 4, so index 4 is returned.\nIn general, this is the way the Binary Search algorithm continues to halve the array search area until the target value is found.\nWhen the target value is found, the index of the target value is returned. If the target value is not found, -1 is returned.\nBinary Search Implementation\nTo implement the Binary Search algorithm we need:\nAn array with values to search through.\nA target value to search for.\nA loop that runs as long as left index is less than, or equal to, the right index.\nAn if-statement that compares the middle value with the target value, and returns the index if the target value is found.\nAn if-statement that checks if the target value is less than, or larger than, the middle value, and updates the \"left\" or \"right\" variables to narrow down the search area.\nAfter the loop, return -1, because at this point we know the target value has not been found.\nThe resulting code for Binary Search looks like this:\nExample\nBinary Search Time Complexity\nFor a general explanation of what time complexity is, visit this page.\nFor a more thorough and detailed explanation of Insertion Sort time complexity, visit this page.\nEach time Binary Search checks a new value to see if it is the target value, the search area is halved.\nThis means that even in the worst case scenario where Binary Search cannot find the target value, it still only needs log2n \\log_{2}n comparisons to look through a sorted array of nn values.\nTime complexity for Binary Search is\nO(log2n) O( \\log_{2} n )\nNote: When writing time complexity using Big O notation we could also just have written O(logn) O( \\log n ) , but O(log2n) O( \\log_{2} n ) reminds us that the array search area is halved for every new comparison, which is the basic concept of Binary Search, so we will just keep the base 2 indication in this case.\nIf we draw how much time Binary Search needs to find a value in an array of nn values, compared to Linear Search, we get this graph:\nRun the Binary Search simulation below for different number of values nn in an array, and see how many compares are needed for Binary Search to find the target value:\nSet values: 300\nAscending\n10 Ascending\nRandom\nDescending\nOperations: 0\nNot found!\nAs you can see when running simulations of Binary Search, the search requires very few compares, even if the the array is big and the value we are looking for is not found.\nDSA Exercises\nTest Yourself With Exercises\nExercise:\nWhat kind of array?\nStart the Exercise",
      "examples": [
        "def binarySearch(arr, targetVal): left = 0 right = len(arr) - 1 while left <= right: mid = (left + right) // 2 if arr[mid] == targetVal: return mid if arr[mid] < targetVal: left = mid + 1 else: right = mid - 1 return -1 myArray = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19] myTarget = 15 result = binarySearch(myArray, myTarget) if result != -1: print(\"Value\",myTarget,\"found at index\", result) else: print(\"Target not found in array.\")",
        "[ 2, 3, 7, 7, 11, 15, 25]",
        "[ 2, 3, 7,\n7\n, 11, 15, 25]",
        "[ 2, 3, 7, 7, 11,\n15\n, 25]",
        "[ 2, 3, 7, 7,\n11\n, 15, 25]",
        "For the Binary Search algorithm to work, the array must already be ."
      ]
    },
    {
      "title": "DSA Linked Lists",
      "summary": "A Linked List is, as the word implies, a list where the nodes are linked together. Each node contains data and a pointer. The way they are linked together is that each node points to where in the memory the next node is placed.\nLinked Lists\nA linked list consists of nodes with some sort of data, and a pointer, or link, to the next node.\nA big benefit with using linked lists is that nodes are stored wherever there is free space in memory, the nodes do not have to be stored contiguously right after each other like elements are stored in arrays. Another nice thing with linked lists is that when adding or removing nodes, the rest of the nodes in the list do not have to be shifted.\nLinked Lists vs Arrays\nThe easiest way to understand linked lists is perhaps by comparing linked lists with arrays.\nLinked lists consist of nodes, and is a linear data structure we make ourselves, unlike arrays which is an existing data structure in the programming language that we can use.\nNodes in a linked list store links to other nodes, but array elements do not need to store links to other elements.\nNote: How linked lists and arrays are stored in memory will be explained in more detail on the next page.\nThe table below compares linked lists with arrays to give a better understanding of what linked lists are.\nTo explain these differences in more detail, the next page will focus on how linked lists and arrays are stored in memory.\nDSA Exercises\nTest Yourself With Exercises\nExercise:\nWhat is a node in a Linked List?\nStart the Exercise",
      "examples": [
        "Each node in a Linked List contains , and a to where the next node is placed in memory."
      ]
    },
    {
      "title": "DSA Linked Lists in Memory",
      "summary": "Computer Memory\nTo explain what linked lists are, and how linked lists are different from arrays, we need to understand some basics about how computer memory works.\nComputer memory is the storage your program uses when it is running. This is where your variables, arrays and linked lists are stored.\nVariables in Memory\nLet's imagine that we want to store the integer \"17\" in a variable myNumber. For simplicity, let's assume the integer is stored as two bytes (16 bits), and the address in memory to myNumber is 0x7F25.\n0x7F25 is actually the address to the first of the two bytes of memory where the myNumber integer value is stored. When the computer goes to 0x7F25 to read an integer value, it knows that it must read both the first and the second byte, since integers are two bytes on this specific computer.\nThe image below shows how the variable myNumber = 17 is stored in memory.\nThe example above shows how an integer value is stored on the simple, but popular, Arduino Uno microcontroller. This microcontroller has an 8 bit architecture with 16 bit address bus and uses two bytes for integers and two bytes for memory addresses. For comparison, personal computers and smart phones use 32 or 64 bits for integers and addresses, but the memory works basically in the same way.\nArrays in Memory\nTo understand linked lists, it is useful to first know how arrays are stored in memory.\nElements in an array are stored contiguously in memory. That means that each element is stored right after the previous element.\nThe image below shows how an array of integers myArray = [3,5,13,2] is stored in memory. We use a simple kind of memory here with two bytes for each integer, like in the previous example, just to get the idea.\nThe computer has only got the address of the first byte of myArray, so to access the 3rd element with code myArray[2] the computer starts at 0x7F23 and jumps over the two first integers. The computer knows that an integer is stored in two bytes, so it jumps 2x2 bytes forward from 0x7F23 and reads value 13 starting at address 0x7F27.\nWhen removing or inserting elements in an array, every element that comes after must be either shifted up to make place for the new element, or shifted down to take the removed element's place. Such shifting operations are time consuming and can cause problems in real-time systems for example.\nThe image below shows how elements are shifted when an array element is removed.\nManipulating arrays is also something you must think about if you are programming in C, where you have to explicitly move other elements when inserting or removing an element. In C this does not happen in the background.\nIn C you also need to make sure that you have allocated enough space for the array to start with, so that you can add more elements later.\nYou can read more about arrays on this previous DSA tutorial page.\nLinked Lists in Memory\nInstead of storing a collection of data as an array, we can create a linked list.\nLinked lists are used in many scenarios, like dynamic data storage, stack and queue implementation or graph representation, to mention some of them.\nA linked list consists of nodes with some sort of data, and at least one pointer, or link, to other nodes.\nA big benefit with using linked lists is that nodes are stored wherever there is free space in memory, the nodes do not have to be stored contiguously right after each other like elements are stored in arrays. Another nice thing with linked lists is that when adding or removing nodes, the rest of the nodes in the list do not have to be shifted.\nThe image below shows how a linked list can be stored in memory. The linked list has four nodes with values 3, 5, 13 and 2, and each node has a pointer to the next node in the list.\nEach node takes up four bytes. Two bytes are used to store an integer value, and two bytes are used to store the address to the next node in the list. As mentioned before, how many bytes that are needed to store integers and addresses depend on the architecture of the computer. This example, like the previous array example, fits with a simple 8-bit microcontroller architecture.\nTo make it easier to see how the nodes relate to each other, we will display nodes in a linked list in a simpler way, less related to their memory location, like in the image below:\nIf we put the same four nodes from the previous example together using this new visualization, it looks like this:\nAs you can see, the first node in a linked list is called the \"Head\", and the last node is called the \"Tail\".\nUnlike with arrays, the nodes in a linked list are not placed right after each other in memory. This means that when inserting or removing a node, shifting of other nodes is not necessary, so that is a good thing.\nSomething not so good with linked lists is that we cannot access a node directly like we can with an array by just writing myArray[5] for example. To get to node number 5 in a linked list, we must start with the first node called \"head\", use that node's pointer to get to the next node, and do so while keeping track of the number of nodes we have visited until we reach node number 5.\nLearning about linked lists helps us to better understand concepts like memory allocation and pointers.\nLinked lists are also important to understand before learning about more complex data structures such as trees and graphs, that can be implemented using linked lists.\nMemory in Modern Computers\nSo far on this page we have used the memory in an 8 bit microcontroller as an example to keep it simple and easier to understand.\nMemory in modern computers work in the same way in principle as memory in an 8 bit microcontroller, but more memory is used to store integers, and more memory is used to store memory addresses.\nThe code below gives us the size of an integer and the size of a memory address on the server we are running these examples on.\nExample\nCode written in C:\nThe code example above only runs in C because Java and Python runs on an abstraction level above specific/direct memory allocation.\nLinked List Implementation in C\nLet's implement this linked list from earlier:\nLet's implement this linked list in C to see a concrete example of how linked lists are stored in memory.\nIn the code below, after including the libraries, we create a node struct which is like a class that represents what a node is: the node contains data and a pointer to the next node.\nThe createNode() function allocates memory for a new node, fills in the data part of the node with an integer given as an argument to the function, and returns the pointer (memory address) to the new node.\nThe printList() function is just for going through the linked list and printing each node's value.\nInside the main() function, four nodes are created, linked together, printed, and then the memory is freed. It is good practice to free memory after we are done using it to avoid memory leaks. Memory leak is when memory is not freed after use, gradually taking up more and more memory.\nExample\nA basic linked list in C:\nTo print the linked list in the code above, the printList() function goes from one node to the next using the \"next\" pointers, and that is called \"traversing\" or \"traversal\" of the linked list. You will learn more about linked list traversal and other linked list operations on the Linked Lists Oprations page.\nLinked List Implementation in Python and Java\nWe will now implement this same linked list using Python and Java instead.\nIn the Python code below, the Node class represents what a node is: the node contains data and a link to the next node.\nThe Node class is used to to create four nodes, the nodes are then linked together, and printed at the end.\nAs you can see, the Python code is a lot shorter than the C code, and perhaps better if you just want to understand the concept of linked lists, and not how linked lists are stored in memory.\nThe Java code is very similar to the Python code. Click the \"Run Example\" button below and choose the \"Java\" tab to see the Java code.\nExample\nA basic linked list in Python:\nDSA Exercises\nTest Yourself With Exercises\nExercise:\nWhat is a benefit of using Linked Lists?\nStart the Exercise",
      "examples": [
        "#include <stdio.h> int main() { int myVal = 13; printf(\"Value of integer 'myVal': %d\\n\", myVal); printf(\"Size of integer 'myVal': %lu bytes\\n\", sizeof(myVal)); // 4 bytes printf(\"Address to 'myVal': %p\\n\", &myVal); printf(\"Size of the address to 'myVal': %lu bytes\\n\", sizeof(&myVal)); // 8 bytes return 0; }",
        "#include <stdio.h> #include <stdlib.h> typedef struct Node { int data; struct Node* next; } Node; Node* createNode(int data) { Node* newNode = (Node*)malloc(sizeof(Node)); if (!newNode) { printf(\"Memory allocation failed!\\n\"); exit(1); } newNode->data = data; newNode->next = NULL; return newNode; } void printList(Node* node) { while (node) { printf(\"%d -> \", node->data); node = node->next; } printf(\"null\\n\"); } int main() { Node* node1 = createNode(3); Node* node2 = createNode(5); Node* node3 = createNode(13); Node* node4 = createNode(2); node1->next = node2; node2->next = node3; node3->next = node4; printList(node1); // Free the memory free(node1); free(node2); free(node3); free(node4); return 0; }",
        "class Node: def __init__(self, data): self.data = data self.next = None node1 = Node(3) node2 = Node(5) node3 = Node(13) node4 = Node(2) node1.next = node2 node2.next = node3 node3.next = node4 currentNode = node1 while currentNode: print(currentNode.data, end=\" -> \") currentNode = currentNode.next print(\"null\")",
        "A good thing about Linked Lists is that when inserting or removing a node, other elements do not have to be in memory.",
        "createNode()",
        "printList()",
        "main()",
        "Node"
      ]
    },
    {
      "title": "DSA Linked Lists Types",
      "summary": "Types of Linked Lists\nThere are three basic forms of linked lists:\nSingly linked lists\nDoubly linked lists\nCircular linked lists\nA singly linked list is the simplest kind of linked lists. It takes up less space in memory because each node has only one address to the next node, like in the image below.\nA doubly linked list has nodes with addresses to both the previous and the next node, like in the image below, and therefore takes up more memory. But doubly linked lists are good if you want to be able to move both up and down in the list.\nA circular linked list is like a singly or doubly linked list with the first node, the \"head\", and the last node, the \"tail\", connected.\nIn singly or doubly linked lists, we can find the start and end of a list by just checking if the links are null. But for circular linked lists, more complex code is needed to explicitly check for start and end nodes in certain applications.\nCircular linked lists are good for lists you need to cycle through continuously.\nThe image below is an example of a singly circular linked list:\nThe image below is an example of a doubly circular linked list:\nNote: What kind of linked list you need depends on the problem you are trying to solve.\nLinked List Implementations\nBelow are basic implementations of:\nSingly linked list\nDoubly linked list\nCircular singly linked list\nCircular doubly linked list\nThe next page will cover different operations that can be done on linked lists.\n1. Singly Linked List Implementation\nBelow is an implementation of this singly linked list:\nExample\nA basic singly linked list in Python:\n(This is the same example as on the bottom of the previous page.)\n2. Doubly Linked List Implementation\nBelow is an implementation of this doubly linked list:\nExample\nA basic doubly linked list in Python:\n3. Circular Singly Linked List Implementation\nBelow is an implementation of this circular singly linked list:\nExample\nA basic circular singly linked list in Python:\nLine 14: This makes the singly list circular.\nLine 17: This is how the program knows when to stop so that it only goes through the list one time.\n4. Circular Doubly Linked List Implementation\nBelow is an implementation of this circular doubly linked list:\nExample\nA basic circular doubly linked list in Python:\nLines 13 and 22: These links makes the doubly linked list circular.\nLines 26: This is how the program knows when to stop so that it only goes through the list one time.\nDSA Exercises\nTest Yourself With Exercises\nExercise:\nTake a look at this singly Linked List:\nHow can we make this Linked List circular?\nStart the Exercise",
      "examples": [
        "class Node: def __init__(self, data): self.data = data self.next = None node1 = Node(3) node2 = Node(5) node3 = Node(13) node4 = Node(2) node1.next = node2 node2.next = node3 node3.next = node4 currentNode = node1 while currentNode: print(currentNode.data, end=\" -> \") currentNode = currentNode.next print(\"null\")",
        "class Node: def __init__(self, data): self.data = data self.next = None self.prev = None node1 = Node(3) node2 = Node(5) node3 = Node(13) node4 = Node(2) node1.next = node2 node2.prev = node1 node2.next = node3 node3.prev = node2 node3.next = node4 node4.prev = node3 print(\"\\nTraversing forward:\") currentNode = node1 while currentNode: print(currentNode.data, end=\" -> \") currentNode = currentNode.next print(\"null\") print(\"\\nTraversing backward:\") currentNode = node4 while currentNode: print(currentNode.data, end=\" -> \") currentNode = currentNode.prev print(\"null\")",
        "class Node: def __init__(self, data): self.data = data self.next = None node1 = Node(3) node2 = Node(5) node3 = Node(13) node4 = Node(2) node1.next = node2 node2.next = node3 node3.next = node4 node4.next = node1 currentNode = node1 startNode = node1 print(currentNode.data, end=\" -> \") currentNode = currentNode.next while currentNode != startNode: print(currentNode.data, end=\" -> \") currentNode = currentNode.next print(\"...\")",
        "class Node: def __init__(self, data): self.data = data self.next = None self.prev = None node1 = Node(3) node2 = Node(5) node3 = Node(13) node4 = Node(2) node1.next = node2 node1.prev = node4 node2.prev = node1 node2.next = node3 node3.prev = node2 node3.next = node4 node4.prev = node3 node4.next = node1 print(\"\\nTraversing forward:\") currentNode = node1 startNode = node1 print(currentNode.data, end=\" -> \") currentNode = currentNode.next while currentNode != startNode: print(currentNode.data, end=\" -> \") currentNode = currentNode.next print(\"...\") print(\"\\nTraversing backward:\") currentNode = node4 startNode = node4 print(currentNode.data, end=\" -> \") currentNode = currentNode.prev while currentNode != startNode: print(currentNode.data, end=\" -> \") currentNode = currentNode.prev print(\"...\")",
        "The list can be made circular by connecting the next pointer in the last node, to the node."
      ]
    },
    {
      "title": "DSA Linked Lists Operations",
      "summary": "Linked List Operations\nBasic things we can do with linked lists are:\nTraversal\nRemove a node\nInsert a node\nSort\nFor simplicity, singly linked lists will be used to explain these operations below.\nTraversal of a Linked List\nTraversing a linked list means to go through the linked list by following the links from one node to the next.\nTraversal of linked lists is typically done to search for a specific node, and read or modify the node's content, remove the node, or insert a node right before or after that node.\nTo traverse a singly linked list, we start with the first node in the list, the head node, and follow that node's next link, and the next node's next link and so on, until the next address is null, like in the animation below:\nThe code below prints out the node values as it traverses along the linked list, in the same way as the animation above.\nExample\nTraversal of a singly linked list in Python:\nFind The Lowest Value in a Linked List\nLet's find the lowest value in a singly linked list by traversing it and checking each value.\nFinding the lowest value in a linked list is very similar to how we found the lowest value in an array, except that we need to follow the next link to get to the next node.\nThis is how finding the lowest value in a linked list works in principle:\nLowest value:\nTo find the lowest value we need to traverse the list like in the previous code. But in addition to traversing the list, we must also update the current lowest value when we find a node with a lower value.\nIn the code below, the algorithm to find the lowest value is moved into a function called findLowestValue.\nExample\nFinding the lowest value in a singly linked list in Python:\nThe marked lines above is the core of the algorithm. The initial lowest value is set to be the value of the first node. Then, if a lower value is found, the lowest value variable is udated.\nDelete a Node in a Linked List\nIn this case we have the link (or pointer or address) to a node that we want to delete.\nIt is important to connect the nodes on each side of the node before deleting it, so that the linked list is not broken.\nSo before deleting the node, we need to get the next pointer from the previous node, and connect the previous node to the new next node before deleting the node in between.\nIn a singly linked list, like we have here, to get the next pointer from the previous node we actually need traverse the list from the start, because there is no way to go backwards from the node we want to delete.\nThe simulation below shows the node we want to delete, and how the list must be traversed first to connect the list properly before deleting the node without breaking the linked list.\nAlso, it is a good idea to first connect next pointer to the node after the node we want to delete, before we delete it. This is to avoid a 'dangling' pointer, a pointer that points to nothing, even if it is just for a brief moment.\nIn the code below, the algorithm to delete a node is moved into a function called deleteSpecificNode.\nExample\nDeleting a specific node in a singly linked list in Python:\nIn the deleteSpecificNode function above, the return value is the new head of the linked list. So for example, if the node to be deleted is the first node, the new head returned will be the next node.\nInsert a Node in a Linked List\nInserting a node into a linked list is very similar to deleting a node, because in both cases we need to take care of the next pointers to make sure we do not break the linked list.\nTo insert a node in a linked list we first need to create the node, and then at the position where we insert it, we need to adjust the pointers so that the previous node points to the new node, and the new node points to the correct next node.\nThe simulation below shows how the links are adjusted when inserting a new node.\nNew node is created\nNode 1 is linked to new node\nNew node is linked to next node\nExample\nInserting a node in a singly linked list in Python:\nIn the insertNodeAtPosition function above, the return value is the new head of the linked list. So for example, if the node is inserted at the start of the linked list, the new head returned will be the new node.\nOther Linked Lists Operations\nWe have only covered three basic linked list operations above: traversal (or search), node deletion, and node insertion.\nThere are a lot of other operations that could be done with linked lists, like sorting for example.\nPreviously in the tutorial we have covered many sorting algorithms, and we could do many of these sorting algorithms on linked lists as well. Let's take selection sort for example. In selection sort we find the lowest value, remove it, and insert it at the beginning. We could do the same with a linked list as well, right? We have just seen how to search through a linked list, how to remove a node, and how to insert a node.\nNote: We cannot sort linked lists with sorting algorithms like Counting Sort, Radix Sort or Quicksort because they use indexes to modify array elements directly based on their position.\nLinked Lists vs Arrays\nThese are some key linked list properties, compared to arrays:\nLinked lists are not allocated to a fixed size in memory like arrays are, so linked lists do not require to move the whole list into a larger memory space when the fixed memory space fills up, like arrays must.\nLinked list nodes are not laid out one right after the other in memory (contiguously), so linked list nodes do not have to be shifted up or down in memory when nodes are inserted or deleted.\nLinked list nodes require more memory to store one or more links to other nodes. Array elements do not require that much memory, because array elements do not contain links to other elements.\nLinked list operations are usually harder to program and require more lines than similar array operations, because programming languages have better built in support for arrays.\nWe must traverse a linked list to find a node at a specific position, but with arrays we can access an element directly by writing myArray[5].\nNote: When using arrays in programming languages like Java or Python, even though we do not need to write code to handle when an array fills up its memory space, and we do not have to shift elements up or down in memory when an element is removed or inserted, these things still happen in the background and can cause problems in time critical applications.\nTime Complexity of Linked Lists Operations\nHere we discuss time complexity of linked list operations, and compare these with the time complexity of the array algorithms that we have discussed previously in this tutorial.\nRemember that time complexity just says something about the approximate number of operations needed by the algorithm based on a large set of data nn, and does not tell us the exact time a specific implementation of an algorithm takes.\nThis means that even though linear search is said to have the same time complexity for arrays as for linked list: O(n)O(n), it does not mean they take the same amount of time. The exact time it takes for an algorithm to run depends on programming language, computer hardware, differences in time needed for operations on arrays vs linked lists, and many other things as well.\nLinear search for linked lists works the same as for arrays. A list of unsorted values are traversed from the head node until the node with the specific value is found. Time complexity is O(n)O(n).\nBinary search is not possible for linked lists because the algorithm is based on jumping directly to different array elements, and that is not possible with linked lists.\nSorting algorithms have the same time complexities as for arrays, and these are explained earlier in this tutorial. But remember, sorting algorithms that are based on directly accessing an array element based on an index, do not work on linked lists.\nDSA Exercises\nTest Yourself With Exercises\nExercise:\nComplete the code for the Linked List traversal function.\nStart the Exercise",
      "examples": [
        "class Node: def __init__(self, data): self.data = data self.next = None def traverseAndPrint(head): currentNode = head while currentNode: print(currentNode.data, end=\" -> \") currentNode = currentNode.next print(\"null\") node1 = Node(7) node2 = Node(11) node3 = Node(3) node4 = Node(2) node5 = Node(9) node1.next = node2 node2.next = node3 node3.next = node4 node4.next = node5 traverseAndPrint(node1)",
        "class Node: def __init__(self, data): self.data = data self.next = None def findLowestValue(head): minValue = head.data currentNode = head.next while currentNode: if currentNode.data < minValue: minValue = currentNode.data currentNode = currentNode.next return minValue node1 = Node(7) node2 = Node(11) node3 = Node(3) node4 = Node(2) node5 = Node(9) node1.next = node2 node2.next = node3 node3.next = node4 node4.next = node5 print(\"The lowest value in the linked list is:\", findLowestValue(node1))",
        "class Node: def __init__(self, data): self.data = data self.next = None def traverseAndPrint(head): currentNode = head while currentNode: print(currentNode.data, end=\" -> \") currentNode = currentNode.next print(\"null\") def deleteSpecificNode(head, nodeToDelete): if head == nodeToDelete: return head.next currentNode = head while currentNode.next and currentNode.next != nodeToDelete: currentNode = currentNode.next if currentNode.next is None: return head currentNode.next = currentNode.next.next return head node1 = Node(7) node2 = Node(11) node3 = Node(3) node4 = Node(2) node5 = Node(9) node1.next = node2 node2.next = node3 node3.next = node4 node4.next = node5 print(\"Before deletion:\") traverseAndPrint(node1) # Delete node4 node1 = deleteSpecificNode(node1, node4) print(\"\\nAfter deletion:\") traverseAndPrint(node1)",
        "class Node: def __init__(self, data): self.data = data self.next = None def traverseAndPrint(head): currentNode = head while currentNode: print(currentNode.data, end=\" -> \") currentNode = currentNode.next print(\"null\") def insertNodeAtPosition(head, newNode, position): if position == 1: newNode.next = head return newNode currentNode = head for _ in range(position - 2): if currentNode is None: break currentNode = currentNode.next newNode.next = currentNode.next currentNode.next = newNode return head node1 = Node(7) node2 = Node(3) node3 = Node(2) node4 = Node(9) node1.next = node2 node2.next = node3 node3.next = node4 print(\"Original list:\") traverseAndPrint(node1) # Insert a new node with value 97 at position 2 newNode = Node(97) node1 = insertNodeAtPosition(node1, newNode, 2) print(\"\\nAfter insertion:\") traverseAndPrint(node1)",
        "def traverseAndPrint(head): currentNode = while currentNode: print(currentNode.data, end=\" -> \") currentNode = currentNode. print(\"null\")"
      ]
    },
    {
      "title": "DSA Stacks",
      "summary": "Stacks\nA stack is a data structure that can hold many elements.\nResult:\nThink of a stack like a pile of pancakes.\nIn a pile of pancakes, the pancakes are both added and removed from the top. So when removing a pancake, it will always be the last pancake you added. This way of organizing elements is called LIFO: Last In First Out.\nBasic operations we can do on a stack are:\nPush: Adds a new element on the stack.\nPop: Removes and returns the top element from the stack.\nPeek: Returns the top element on the stack.\nisEmpty: Checks if the stack is empty.\nSize: Finds the number of elements in the stack.\nExperiment with these basic operations in the stack animation above.\nStacks can be implemented by using arrays or linked lists.\nStacks can be used to implement undo mechanisms, to revert to previous states, to create algorithms for depth-first search in graphs, or for backtracking.\nStacks are often mentioned together with Queues, which is a similar data structure described on the next page.\nStack Implementation using Arrays\nTo better understand the benefits with using arrays or linked lists to implement stacks, you should check out this page that explains how arrays and linked lists are stored in memory.\nThis is how it looks like when we use an array as a stack:\nResult:\nReasons to implement stacks using arrays:\nMemory Efficient: Array elements do not hold the next elements address like linked list nodes do.\nEasier to implement and understand: Using arrays to implement stacks require less code than using linked lists, and for this reason it is typically easier to understand as well.\nA reason for not using arrays to implement stacks:\nFixed size: An array occupies a fixed part of the memory. This means that it could take up more memory than needed, or if the array fills up, it cannot hold more elements.\nNote: When using arrays in Python for this tutorial, we are really using the Python 'list' data type, but for the scope of this tutorial the 'list' data type can be used in the same way as an array. Learn more about Python lists here.\nSince Python lists has good support for functionality needed to implement stacks, we start with creating a stack and do stack operations with just a few lines like this:\nExample\nPython:\nBut to explicitly create a data structure for stacks, with basic operations, we should create a stack class instead. This way of creating stacks in Python is also more similar to how stacks can be created in other programming languages like C and Java.\nExample\nPython:\nStack Implementation using Linked Lists\nA reason for using linked lists to implement stacks:\nDynamic size: The stack can grow and shrink dynamically, unlike with arrays.\nReasons for not using linked lists to implement stacks:\nExtra memory: Each stack element must contain the address to the next element (the next linked list node).\nReadability: The code might be harder to read and write for some because it is longer and more complex.\nThis is how a stack can be implemented using a linked list.\nExample\nPython:\nDSA Exercises\nTest Yourself With Exercises\nExercise:\nThe image below represents a \"Stack\" data structure.\nRunning the peek() method on the Stack above, what is returned?\nStart the Exercise",
      "examples": [
        "stack = [] # Push stack.append('A') stack.append('B') stack.append('C') print(\"Stack: \", stack) # Pop element = stack.pop() print(\"Pop: \", element) # Peek topElement = stack[-1] print(\"Peek: \", topElement) # isEmpty isEmpty = not bool(stack) print(\"isEmpty: \", isEmpty) # Size print(\"Size: \",len(stack))",
        "class Stack: def __init__(self): self.stack = [] def push(self, element): self.stack.append(element) def pop(self): if self.isEmpty(): return \"Stack is empty\" return self.stack.pop() def peek(self): if self.isEmpty(): return \"Stack is empty\" return self.stack[-1] def isEmpty(self): return len(self.stack) == 0 def size(self): return len(self.stack) # Create a stack myStack = Stack() myStack.push('A') myStack.push('B') myStack.push('C') print(\"Stack: \", myStack.stack) print(\"Pop: \", myStack.pop()) print(\"Peek: \", myStack.peek()) print(\"isEmpty: \", myStack.isEmpty()) print(\"Size: \", myStack.size())",
        "class Node: def __init__(self, value): self.value = value self.next = None class Stack: def __init__(self): self.head = None self.size = 0 def push(self, value): new_node = Node(value) if self.head: new_node.next = self.head self.head = new_node self.size += 1 def pop(self): if self.isEmpty(): return \"Stack is empty\" popped_node = self.head self.head = self.head.next self.size -= 1 return popped_node.value def peek(self): if self.isEmpty(): return \"Stack is empty\" return self.head.value def isEmpty(self): return self.size == 0 def stackSize(self): return self.size myStack = Stack() myStack.push('A') myStack.push('B') myStack.push('C') print(\"Pop: \", myStack.pop()) print(\"Peek: \", myStack.peek()) print(\"isEmpty: \", myStack.isEmpty()) print(\"Size: \", myStack.stackSize())"
      ]
    },
    {
      "title": "DSA Queues",
      "summary": "Queues\nA queue is a data structure that can hold many elements.\nResult:\nThink of a queue as people standing in line in a supermarket.\nThe first person to stand in line is also the first who can pay and leave the supermarket. This way of organizing elements is called FIFO: First In First Out.\nBasic operations we can do on a queue are:\nEnqueue: Adds a new element to the queue.\nDequeue: Removes and returns the first (front) element from the queue.\nPeek: Returns the first element in the queue.\nisEmpty: Checks if the queue is empty.\nSize: Finds the number of elements in the queue.\nExperiment with these basic operations in the queue animation above.\nQueues can be implemented by using arrays or linked lists.\nQueues can be used to implement job scheduling for an office printer, order processing for e-tickets, or to create algorithms for breadth-first search in graphs.\nQueues are often mentioned together with Stacks, which is a similar data structure described on the previous page.\nQueue Implementation using Arrays\nTo better understand the benefits with using arrays or linked lists to implement queues, you should check out this page that explains how arrays and linked lists are stored in memory.\nThis is how it looks like when we use an array as a queue:\nResult:\nReasons to implement queues using arrays:\nMemory Efficient: Array elements do not hold the next elements address like linked list nodes do.\nEasier to implement and understand: Using arrays to implement queues require less code than using linked lists, and for this reason it is typically easier to understand as well.\nReasons for not using arrays to implement queues:\nFixed size: An array occupies a fixed part of the memory. This means that it could take up more memory than needed, or if the array fills up, it cannot hold more elements. And resizing an array can be costly.\nShifting cost: Dequeue causes the first element in a queue to be removed, and the other elements must be shifted to take the removed elements' place. This is inefficient and can cause problems, especially if the queue is long.\nAlternatives: Some programming languages have built-in data structures optimized for queue operations that are better than using arrays.\nNote: When using arrays in Python for this tutorial, we are really using the Python 'list' data type, but for the scope of this tutorial the 'list' data type can be used in the same way as an array. Learn more about Python lists here.\nSince Python lists has good support for functionality needed to implement queues, we start with creating a queue and do queue operations with just a few lines:\nExample\nPython:\nBut to explicitly create a data structure for queues, with basic operations, we should create a queue class instead. This way of creating queues in Python is also more similar to how queues can be created in other programming languages like C and Java.\nExample\nPython:\nQueue Implementation using Linked Lists\nReasons for using linked lists to implement queues:\nDynamic size: The queue can grow and shrink dynamically, unlike with arrays.\nNo shifting: The front element of the queue can be removed (enqueue) without having to shift other elements in the memory.\nReasons for not using linked lists to implement queues:\nExtra memory: Each queue element must contain the address to the next element (the next linked list node).\nReadability: The code might be harder to read and write for some because it is longer and more complex.\nThis is how a queue can be implemented using a linked list.\nExample\nPython:\nDSA Exercises\nTest Yourself With Exercises\nExercise:\nThe Array below is used as a Queue data structure:\nWhich indexes and values are affected by the endueue and dedueue operations?\nStart the Exercise",
      "examples": [
        "queue = [] # Enqueue queue.append('A') queue.append('B') queue.append('C') print(\"Queue: \", queue) # Dequeue element = queue.pop(0) print(\"Dequeue: \", element) # Peek frontElement = queue[0] print(\"Peek: \", frontElement) # isEmpty isEmpty = not bool(queue) print(\"isEmpty: \", isEmpty) # Size print(\"Size: \", len(queue))",
        "class Queue: def __init__(self): self.queue = [] def enqueue(self, element): self.queue.append(element) def dequeue(self): if self.isEmpty(): return \"Queue is empty\" return self.queue.pop(0) def peek(self): if self.isEmpty(): return \"Queue is empty\" return self.queue[0] def isEmpty(self): return len(self.queue) == 0 def size(self): return len(self.queue) # Create a queue myQueue = Queue() myQueue.enqueue('A') myQueue.enqueue('B') myQueue.enqueue('C') print(\"Queue: \", myQueue.queue) print(\"Dequeue: \", myQueue.dequeue()) print(\"Peek: \", myQueue.peek()) print(\"isEmpty: \", myQueue.isEmpty()) print(\"Size: \", myQueue.size())",
        "class Node: def __init__(self, data): self.data = data self.next = None class Queue: def __init__(self): self.front = None self.rear = None self.length = 0 def enqueue(self, element): new_node = Node(element) if self.rear is None: self.front = self.rear = new_node self.length += 1 return self.rear.next = new_node self.rear = new_node self.length += 1 def dequeue(self): if self.isEmpty(): return \"Queue is empty\" temp = self.front self.front = temp.next self.length -= 1 if self.front is None: self.rear = None return temp.data def peek(self): if self.isEmpty(): return \"Queue is empty\" return self.front.data def isEmpty(self): return self.length == 0 def size(self): return self.length def printQueue(self): temp = self.front while temp: print(temp.data, end=\" \") temp = temp.next print() # Create a queue myQueue = Queue() myQueue.enqueue('A') myQueue.enqueue('B') myQueue.enqueue('C') print(\"Queue: \", end=\"\") myQueue.printQueue() print(\"Dequeue: \", myQueue.dequeue()) print(\"Peek: \", myQueue.peek()) print(\"isEmpty: \", myQueue.isEmpty()) print(\"Size: \", myQueue.size())",
        "enqueue(7): value 7 is placed on index in the array. dequeue(): value is taken out of the queue.",
        "[5,11,8,3]"
      ]
    },
    {
      "title": "DSA Hash Tables",
      "summary": "Hash Table\nA Hash Table is a data structure designed to be fast to work with.\nThe reason Hash Tables are sometimes preferred instead of arrays or linked lists is because searching for, adding, and deleting data can be done really quickly, even for large amounts of data.\nIn a Linked List, finding a person \"Bob\" takes time because we would have to go from one node to the next, checking each node, until the node with \"Bob\" is found.\nAnd finding \"Bob\" in an Array could be fast if we knew the index, but when we only know the name \"Bob\", we need to compare each element (like with Linked Lists), and that takes time.\nWith a Hash Table however, finding \"Bob\" is done really fast because there is a way to go directly to where \"Bob\" is stored, using something called a hash function.\nBuilding A Hash Table from Scratch\nTo get the idea of what a Hash Table is, let's try to build one from scratch, to store unique first names inside it.\nWe will build the Hash Set in 5 steps:\nStarting with an array.\nStoring names using a hash function.\nLooking up an element using a hash function.\nHandling collisions.\nThe basic Hash Set code example and simulation.\nStep 1: Starting with an array\nUsing an array, we could store names like this:\nTo find \"Bob\" in this array, we need to compare each name, element by element, until we find \"Bob\".\nIf the array was sorted alphabetically, we could use Binary Search to find a name quickly, but inserting or deleting names in the array would mean a big operation of shifting elements in memory.\nTo make interacting with the list of names really fast, let's use a Hash Table for this instead, or a Hash Set, which is a simplified version of a Hash Table.\nTo keep it simple, let's assume there is at most 10 names in the list, so the array must be a fixed size of 10 elements. When talking about Hash Tables, each of these elements is called a bucket.\nStep 2: Storing names using a hash function\nNow comes the special way we interact with the Hash Set we are making.\nWe want to store a name directly into its right place in the array, and this is where the hash function comes in.\nA hash function can be made in many ways, it is up to the creator of the Hash Table. A common way is to find a way to convert the value into a number that equals one of the Hash Set's index numbers, in this case a number from 0 to 9. In our example we will use the Unicode number of each character, summarize them and do a modulo 10 operation to get index numbers 0-9.\nExample\nThe character \"B\" has Unicode code point 66, \"o\" has 111, and \"b\" has 98. Adding those together we get 275. Modulo 10 of 275 is 5, so \"Bob\" should be stored as an array element at index 5.\nThe number returned by the hash function is called the hash code.\nUnicode number: Everything in our computers are stored as numbers, and the Unicode code point is a unique number that exist for every character. For example, the character A has Unicode number (also called Unicode code point) 65. Just try it in the simulation below. See this page for more information about how characters are represented as numbers.\nModulo: A mathematical operation, written as % in most programming languages (or modmod in mathematics). A modulo operation divides a number with another number, and gives us the resulting remainder. So for example, 7 % 3 will give us the remainder 1. (Dividing 7 apples between 3 people, means that each person gets 2 apples, with 1 apple to spare.)\nAfter storing \"Bob\" where the hash code tells us (index 5), our array now looks like this:\nWe can use the hash function to find out where to store the other names \"Pete\", \"Jones\", \"Lisa\", and \"Siri\" as well.\nAfter using the hash function to store those names in the correct position, our array looks like this:\nStep 3: Looking up a name using a hash function\nWe have now established a super basic Hash Set, because we do not have to check the array element by element anymore to find out if \"Pete\" is in there, we can just use the hash function to go straight to the right element!\nTo find out if \"Pete\" is stored in the array, we give the name \"Pete\" to our hash function, we get back hash code 8, we go directly to the element at index 8, and there he is. We found \"Pete\" without checking any other elements.\nExample\nWhen deleting a name from our Hash Set, we can also use the hash function to go straight to where the name is, and set that element value to None.\nStep 4: Handling collisions\nLet's also add \"Stuart\" to our Hash Set.\nWe give \"Stuart\" to our hash function, and we get the hash code 3, meaning \"Stuart\" should be stored at index 3.\nTrying to store \"Stuart\" creates what is called a collision, because \"Lisa\" is already stored at index 3.\nTo fix the collision, we can make room for more elements in the same bucket, and solving the collision problem in this way is called chaining. We can give room for more elements in the same bucket by implementing each bucket as a linked list, or as an array.\nAfter implementing each bucket as an array, to give room for potentially more than one name in each bucket, \"Stuart\" can also be stored at index 3, and our Hash Set now looks like this:\nSearching for \"Stuart\" in our Hash Set now means that using the hash function we end up directly in bucket 3, but then be must first check \"Lisa\" in that bucket, before we find \"Stuart\" as the second element in bucket 3.\nStep 5: Hash Set code example and simulation\nTo complete our very basic Hash Set code, let's have functions for adding and searching for names in the Hash Set, which is now a two dimensional array.\nRun the code example below, and try it with different values to get a better understanding of how a Hash Set works.\nExample\nThe next two pages show better and more detailed implementations of Hast Sets and Hash Tables.\nTry the Hash Set simulation below to get a better ide of how a Hash Set works in principle.\nHash Set\nHash Code\n0 % 10 = 0\nTry interacting with the Hash Set0\nUses of Hash Tables\nHash Tables are great for:\nChecking if something is in a collection (like finding a book in a library).\nStoring unique items and quickly finding them (like storing phone numbers).\nConnecting values to keys (like linking names to phone numbers).\nThe most important reason why Hash Tables are great for these things is that Hash Tables are very fast compared Arrays and Linked Lists, especially for large sets. Arrays and Linked Lists have time complexity O(n)O(n) for search and delete, while Hash Tables have just O(1)O(1) on average! Read more about time complexity here.\nHash Set vs. Hash Map\nA Hash Table can be a Hash Set or a Hash Map. The next two pages describe these data structures in more detail.\nHere's how Hash Sets and Hash Maps are different and similar:\nHash Tables Summarized\nHash Table elements are stored in storage containers called buckets.\nEvery Hash Table element has a part that is unique that is called the key.\nA hash function takes the key of an element to generate a hash code.\nThe hash code says what bucket the element belongs to, so now we can go directly to that Hash Table element: to modify it, or to delete it, or just to check if it exists. Specific hash functions are explained in detail on the next two pages.\nA collision happens when two Hash Table elements have the same hash code, because that means they belong to the same bucket. A collision can be solved in two ways.\nChaining is the way collisions are solved in this tutorial, by using arrays or linked lists to allow more than one element in the same bucket.\nOpen Addressing is another way to solve collisions. With open addressing, if we want to store an element but there is already an element in that bucket, the element is stored in the next available bucket. This can be done in many different ways, but we will not explain open addressing any further here.\nConclusion\nHash Tables are powerful tools in programming, helping you to manage and access data efficiently.\nWhether you use a Hash Set or a Hash Map depends on what you need: just to know if something is there, or to find detailed information about it.",
      "examples": [
        "my_array = ['Pete', 'Jones', 'Lisa', 'Bob', 'Siri']",
        "my_hash_set = [None,None,None,None,None,None,None,None,None,None]",
        "def hash_function(value): sum_of_chars = 0 for char in value: sum_of_chars += ord(char) return sum_of_chars % 10 print(\"'Bob' has hash code:\",hash_function('Bob'))",
        "my_hash_set = [None,None,None,None,None,'Bob',None,None,None,None]",
        "my_hash_set = [None,'Jones',None,'Lisa',None,'Bob',None,'Siri','Pete',None]",
        "my_hash_set = [None,'Jones',None,'Lisa',None,'Bob',None,'Siri','Pete',None] def hash_function(value): sum_of_chars = 0 for char in value: sum_of_chars += ord(char) return sum_of_chars % 10 def contains(name): index = hash_function(name) return my_hash_set[index] == name print(\"'Pete' is in the Hash Set:\",contains('Pete'))",
        "my_hash_set = [ [None], ['Jones'], [None], ['Lisa', 'Stuart'], [None], ['Bob'], [None], ['Siri'], ['Pete'], [None] ]",
        "my_hash_set = [ [None], ['Jones'], [None], ['Lisa'], [None], ['Bob'], [None], ['Siri'], ['Pete'], [None] ] def hash_function(value): return sum(ord(char) for char in value) % 10 def add(value): index = hash_function(value) bucket = my_hash_set[index] if value not in bucket: bucket.append(value) def contains(value): index = hash_function(value) bucket = my_hash_set[index] return value in bucket add('Stuart') print(my_hash_set) print('Contains Stuart:',contains('Stuart'))",
        "A",
        "65",
        "%",
        "7 % 3",
        "1",
        "None"
      ]
    },
    {
      "title": "DSA Hash Sets",
      "summary": "Hash Sets\nA Hash Set is a form of Hash Table data structure that usually holds a large number of elements.\nUsing a Hash Set we can search, add, and remove elements really fast.\nHash Sets are used for lookup, to check if an element is part of a set.\nHash Set\nHash Code\n0 % 10 = 0\nTry interacting with the Hash Set0\nA Hash Set stores unique elements in buckets according to the element's hash code.\nHash code: A number generated from an element's unique value (key), to determine what bucket that Hash Set element belongs to.\nUnique elements: A Hash Set cannot have more than one element with the same value.\nBucket: A Hash Set consists of many such buckets, or containers, to store elements. If two elements have the same hash code, they belong to the same bucket. The buckets are therefore often implemented as arrays or linked lists, because a bucket needs to be able to hold more than one element.\nFinding The Hash Code\nA hash code is generated by a hash function.\nThe hash function in the animation above takes the name written in the input, and sums up the Unicode code points for every character in that name.\nAfter that, the hash function does a modulo 10 operation (% 10) on the sum of characters to get the hash code as a number from 0 to 9.\nThis means that a name is put into one of ten possible buckets in the Hash Set, according to the hash code of that name. The same hash code is generated and used when we want to search for or remove a name from the Hash Set.\nThe Hash Code gives us instant access as long as there is just one name in the corresponding bucket.\nUnicode code point: Everything in our computers are stored as numbers, and the Unicode code point is a unique number that exist for every character. For example, the character A has Unicode code point 65. Just try it in the simulation above. See this page for more information about how characters are represented as numbers.\nModulo: A mathematical operation, written as % in most programming languages (or modmod in mathematics). A modulo operation divides a number with another number, and gives us the resulting remainder. So for example, 7 % 3 will give us the remainder 1. (Dividing 7 apples between 3 people, means that each person gets 2 apples, with 1 apple to spare.)\nDirect Access in Hash Sets\nSearching for Peter in the Hash Set above, means that the hash code 2 is generated (512 % 10), and that directs us right to the bucket Peter is in. If that is the only name in that bucket, we will find Peter right away.\nIn cases like this we say that the Hash Set has constant time O(1)O(1) for searching, adding, and removing elements, which is really fast.\nBut, if we search for Jens, we need to search through the other names in that bucket before we find Jens. In a worst case scenario, all names end up in the same bucket, and the name we are searching for is the last one. In such a worst case scenario the Hash Set has time complexity O(n)O(n), which is the same time complexity as arrays and linked lists.\nTo keep Hash Sets fast, it is therefore important to have a hash function that will distribute the elements evenly between the buckets, and to have around as many buckets as Hash Set elements.\nHaving a lot more buckets than Hash Set elements is a waste of memory, and having a lot less buckets than Hash Set elements is a waste of time.\nHash Set Implementation\nHash Sets in Python are typically done by using Python's own set data type, but to get a better understanding of how Hash Sets work we will not use that here.\nTo implement a Hash Set in Python we create a class SimpleHashSet.\nInside the SimpleHashSet class we have a method __init__ to initialize the Hash Set, a method hash_function for the hash function, and methods for the basic Hash Set operations: add, contains, and remove.\nWe also create a method print_set to better see how the Hash Set looks like.\nExample\nUsing the SimpleHashSet class we can create the same Hash Set as in the top of this page:\nExample",
      "examples": [
        "class SimpleHashSet: def __init__(self, size=100): self.size = size self.buckets = [[] for _ in range(size)] # A list of buckets, each is a list (to handle collisions) def hash_function(self, value): # Simple hash function: sum of character codes modulo the number of buckets return sum(ord(char) for char in value) % self.size def add(self, value): # Add a value if it's not already present index = self.hash_function(value) bucket = self.buckets[index] if value not in bucket: bucket.append(value) def contains(self, value): # Check if a value exists in the set index = self.hash_function(value) bucket = self.buckets[index] return value in bucket def remove(self, value): # Remove a value index = self.hash_function(value) bucket = self.buckets[index] if value in bucket: bucket.remove(value) def print_set(self): # Print all elements in the hash set print(\"Hash Set Contents:\") for index, bucket in enumerate(self.buckets): print(f\"Bucket {index}: {bucket}\")",
        "class SimpleHashSet: def __init__(self, size=100): self.size = size self.buckets = [[] for _ in range(size)] # A list of buckets, each is a list (to handle collisions) def hash_function(self, value): # Simple hash function: sum of character codes modulo the number of buckets return sum(ord(char) for char in value) % self.size def add(self, value): # Add a value if it's not already present index = self.hash_function(value) bucket = self.buckets[index] if value not in bucket: bucket.append(value) def contains(self, value): # Check if a value exists in the set index = self.hash_function(value) bucket = self.buckets[index] return value in bucket def remove(self, value): # Remove a value index = self.hash_function(value) bucket = self.buckets[index] if value in bucket: bucket.remove(value) def print_set(self): # Print all elements in the hash set print(\"Hash Set Contents:\") for index, bucket in enumerate(self.buckets): print(f\"Bucket {index}: {bucket}\") # Creating the Hash Set from the simulation hash_set = SimpleHashSet(size=10) hash_set.add(\"Charlotte\") hash_set.add(\"Thomas\") hash_set.add(\"Jens\") hash_set.add(\"Peter\") hash_set.add(\"Lisa\") hash_set.add(\"Adele\") hash_set.add(\"Michaela\") hash_set.add(\"Bob\") hash_set.print_set() print(\"\\n'Peter' is in the set:\",hash_set.contains('Peter')) print(\"Removing 'Peter'\") hash_set.remove('Peter') print(\"'Peter' is in the set:\",hash_set.contains('Peter')) print(\"'Adele' has hash code:\",hash_set.hash_function('Adele'))",
        "% 10",
        "A",
        "65",
        "%",
        "7 % 3",
        "1",
        "Peter",
        "2",
        "512 % 10",
        "Jens",
        "set",
        "SimpleHashSet",
        "__init__",
        "hash_function",
        "add",
        "contains",
        "remove",
        "print_set"
      ]
    },
    {
      "title": "DSA Hash Maps",
      "summary": "Hash Maps\nA Hash Map is a form of Hash Table data structure that usually holds a large number of entries.\nUsing a Hash Map we can search, add, modify, and remove entries really fast.\nHash Maps are used to find detailed information about something.\nIn the simulation below, people are stored in a Hash Map. A person can be looked up using a person's unique social security number (the Hash Map key), and then we can see that person's name (the Hash Map value).\nHash Map\nHash Code\n0 % 10 = 0\nTry interacting with the Hash Map0\nNote: The Hash Map would be more useful if more information about each person was attached to the corresponding social security number, like last name, birth date, and address, and maybe other things as well. But the Hash Map simulation above is made to be as simple as possible.\nIt is easier to understand how Hash Maps work if you first have a look at the two previous pages about Hash Tables and Hash Sets. It is also important to understand the meaning of the words below.\nEntry: Consists of a key and a value, forming a key-value pair.\nKey: Unique for each entry in the Hash Map. Used to generate a hash code determining the entry's bucket in the Hash Map. This ensures that every entry can be efficiently located.\nHash Code: A number generated from an entry's key, to determine what bucket that Hash Map entry belongs to.\nBucket: A Hash Map consists of many such buckets, or containers, to store entries.\nValue: Can be nearly any kind of information, like name, birth date, and address of a person. The value can be many different kinds of information combined.\nFinding The Hash Code\nA hash code is generated by a hash function.\nThe hash function in the simulation above takes the numbers in the social security number (not the dash), add them together, and does a modulo 10 operation (% 10) on the sum of characters to get the hash code as a number from 0 to 9.\nThis means that a person is stored in one of ten possible buckets in the Hash Map, according to the hash code of that person's social security number. The same hash code is generated and used when we want to search for or remove a person from the Hash Map.\nThe Hash Code gives us instant access as long as there is just one person in the corresponding bucket.\nIn the simulation above, Charlotte has social security number 123-4567. Adding the numbers together gives us a sum 28, and modulo 10 of that is 8. That is why she belongs to bucket 8.\nModulo: A mathematical operation, written as % in most programming languages (or modmod in mathematics). A modulo operation divides a number with another number, and gives us the resulting remainder. So for example, 7 % 3 will give us the remainder 1. (Dividing 7 apples between 3 people, means that each person gets 2 apples, with 1 apple to spare.)\nDirect Access in Hash Maps\nSearching for Charlotte in the Hash Map, we must use the social security number 123-4567 (the Hash Map key), which generates the hash code 8, as explained above.\nThis means we can go straight to bucket 8 to get her name (the Hash Map value), without searching through other entries in the Hash Map.\nIn cases like this we say that the Hash Map has constant time O(1)O(1) for searching, adding, and removing entries, which is really fast compared to using an array or a linked list.\nBut, in a worst case scenario, all the people are stored in the same bucket, and if the person we are trying to find is last person in this bucket, we need to compare with all the other social security numbers in that bucket before we find the person we are looking for.\nIn such a worst case scenario the Hash Map has time complexity O(n)O(n), which is the same time complexity as arrays and linked lists.\nTo keep Hash Maps fast, it is therefore important to have a hash function that will distribute the entries evenly between the buckets, and to have around as many buckets as Hash Map entries.\nHaving a lot more buckets than Hash Map entries is a waste of memory, and having a lot less buckets than Hash Map entries is a waste of time.\nNote: A social security number can be really long, like 11 digits, which means it is possible to store 100 billion people with unique social security numbers. This is a lot more than in any country's population, and even a lot more than there are people on Earth.\nUsing an array where each person's social security number is the index in the array where this person is stored is therefore a huge waste of space (mostly empty buckets).\nUsing a Hash Map (or a database with similar properties) makes more sense as the number of buckets can be adjusted to the number of people.\nHash Map Implementation\nHash Maps in Python are typically done by using Python's own dictionary data type, but to get a better understanding of how Hash Maps work we will not use that here.\nTo implement a Hash Map in Python we create a class SimpleHashMap.\nInside the SimpleHashMap class we have a method __init__ to initialize the Hash Map, a method hash_function for the hash function, and methods for the basic Hash Map operations: put, get, and remove.\nWe also create a method print_map to better see how the Hash Map looks like.\nExample\nUsing the SimpleHashMap class we can create the same Hash Map as in the top of this page:\nExample",
      "examples": [
        "class SimpleHashMap: def __init__(self, size=100): self.size = size self.buckets = [[] for _ in range(size)] # A list of buckets, each is a list (to handle collisions) def hash_function(self, key): # Sum only the numerical values of the key, ignoring non-numeric characters numeric_sum = sum(int(char) for char in key if char.isdigit()) return numeric_sum % 10 # Perform modulo 10 on the sum def put(self, key, value): # Add or update a key-value pair index = self.hash_function(key) bucket = self.buckets[index] for i, (k, v) in enumerate(bucket): if k == key: bucket[i] = (key, value) # Update existing key return bucket.append((key, value)) # Add new key-value pair if not found def get(self, key): # Retrieve a value by key index = self.hash_function(key) bucket = self.buckets[index] for k, v in bucket: if k == key: return v return None # Key not found def remove(self, key): # Remove a key-value pair index = self.hash_function(key) bucket = self.buckets[index] for i, (k, v) in enumerate(bucket): if k == key: del bucket[i] # Remove the key-value pair return def print_map(self): # Print all key-value pairs in the hash map print(\"Hash Map Contents:\") for index, bucket in enumerate(self.buckets): print(f\"Bucket {index}: {bucket}\")",
        "class SimpleHashMap: def __init__(self, size=100): self.size = size self.buckets = [[] for _ in range(size)] # A list of buckets, each is a list (to handle collisions) def hash_function(self, key): # Sum only the numerical values of the key, ignoring non-numeric characters numeric_sum = sum(int(char) for char in key if char.isdigit()) return numeric_sum % 10 # Perform modulo 10 on the sum def put(self, key, value): # Add or update a key-value pair index = self.hash_function(key) bucket = self.buckets[index] for i, (k, v) in enumerate(bucket): if k == key: bucket[i] = (key, value) # Update existing key return bucket.append((key, value)) # Add new key-value pair if not found def get(self, key): # Retrieve a value by key index = self.hash_function(key) bucket = self.buckets[index] for k, v in bucket: if k == key: return v return None # Key not found def remove(self, key): # Remove a key-value pair index = self.hash_function(key) bucket = self.buckets[index] for i, (k, v) in enumerate(bucket): if k == key: del bucket[i] # Remove the key-value pair return def print_map(self): # Print all key-value pairs in the hash map print(\"Hash Map Contents:\") for index, bucket in enumerate(self.buckets): print(f\"Bucket {index}: {bucket}\") # Creating the Hash Map from the simulation hash_map = SimpleHashMap(size=10) # Adding some entries hash_map.put(\"123-4567\", \"Charlotte\") hash_map.put(\"123-4568\", \"Thomas\") hash_map.put(\"123-4569\", \"Jens\") hash_map.put(\"123-4570\", \"Peter\") hash_map.put(\"123-4571\", \"Lisa\") hash_map.put(\"123-4672\", \"Adele\") hash_map.put(\"123-4573\", \"Michaela\") hash_map.put(\"123-6574\", \"Bob\") hash_map.print_map() # Demonstrating retrieval print(\"\\nName associated with '123-4570':\", hash_map.get(\"123-4570\")) print(\"Updating the name for '123-4570' to 'James'\") hash_map.put(\"123-4570\",\"James\") # Checking if Peter is still there print(\"Name associated with '123-4570':\", hash_map.get(\"123-4570\"))",
        "% 10",
        "Charlotte",
        "123-4567",
        "28",
        "8",
        "%",
        "7 % 3",
        "1",
        "dictionary",
        "SimpleHashMap",
        "__init__",
        "hash_function",
        "put",
        "get",
        "remove",
        "print_map"
      ]
    },
    {
      "title": "DSA Trees",
      "summary": "Trees\nThe Tree data structure is similar to Linked Lists in that each node contains data and can be linked to other nodes.\nWe have previously covered data structures like Arrays, Linked Lists, Stacks, and Queues. These are all linear structures, which means that each element follows directly after another in a sequence. Trees however, are different. In a Tree, a single element can have multiple 'next' elements, allowing the data structure to branch out in various directions.\nThe data structure is called a \"tree\" because it looks like a tree, only upside down, just like in the image below.\nThe Tree data structure can be useful in many cases:\nHierarchical Data: File systems, organizational models, etc.\nDatabases: Used for quick data retrieval.\nRouting Tables: Used for routing data in network algorithms.\nSorting/Searching: Used for sorting data and searching for data.\nPriority Queues: Priority queue data structures are commonly implemented using trees, such as binary heaps.\nTree Terminology and Rules\nLearn words used to describe the tree data structure by using the interactive tree visualization below.\nThe whole tree\nRoot node\nEdges\nNodes\nLeaf nodes\nChild nodes\nParent nodes\nTree height (h=2)\nTree size (n=10)\nThe first node in a tree is called the root node.\nA link connecting one node to another is called an edge.\nA parent node has links to its child nodes. Another word for a parent node is internal node.\nA node can have zero, one, or many child nodes.\nA node can only have one parent node.\nNodes without links to other child nodes are called leaves, or leaf nodes.\nThe tree height is the maximum number of edges from the root node to a leaf node. The height of the tree above is 2.\nThe height of a node is the maximum number of edges between the node and a leaf node.\nThe tree size is the number of nodes in the tree.\nTypes of Trees\nTrees are a fundamental data structure in computer science, used to represent hierarchical relationships. This tutorial covers several key types of trees.\nBinary Trees: Each node has up to two children, the left child node and the right child node. This structure is the foundation for more complex tree types like Binay Search Trees and AVL Trees.\nBinary Search Trees (BSTs): A type of Binary Tree where for each node, the left child node has a lower value, and the right child node has a higher value.\nAVL Trees: A type of Binary Search Tree that self-balances so that for every node, the difference in height between the left and right subtrees is at most one. This balance is maintained through rotations when nodes are inserted or deleted.\nEach of these data structures are described in detail on the next pages, including animations and how to implement them.\nDSA Exercises\nTest Yourself With Exercises\nExercise:\nIn a Tree data structure, like the one below:\nWhat are nodes C, D, E, and G called?\nStart the Exercise",
      "examples": [
        "Nodes C, D, E, and G are called nodes."
      ]
    },
    {
      "title": "DSA Binary Trees",
      "summary": "Binary Trees\nA Binary Tree is a type of tree data structure where each node can have a maximum of two child nodes, a left child node and a right child node.\nThis restriction, that a node can have a maximum of two child nodes, gives us many benefits:\nAlgorithms like traversing, searching, insertion and deletion become easier to understand, to implement, and run faster.\nKeeping data sorted in a Binary Search Tree (BST) makes searching very efficient.\nBalancing trees is easier to do with a limited number of child nodes, using an AVL Binary Tree for example.\nBinary Trees can be represented as arrays, making the tree more memory efficient.\nUse the animation below to see how a Binary Tree looks, and what words we use to describe it.\nThe Binary Tree\nRoot node\nA's left child\nA's right child\nB's subtree\nTree size (n=8)\nTree height (h=3)\nChild nodes\nParent/internal nodes\nA parent node, or internal node, in a Binary Tree is a node with one or two child nodes.\nThe left child node is the child node to the left.\nThe right child node is the child node to the right.\nThe tree height is the maximum number of edges from the root node to a leaf node.\nBinary Trees vs Arrays and Linked Lists\nBenefits of Binary Trees over Arrays and Linked Lists:\nArrays are fast when you want to access an element directly, like element number 700 in an array of 1000 elements for example. But inserting and deleting elements require other elements to shift in memory to make place for the new element, or to take the deleted elements place, and that is time consuming.\nLinked Lists are fast when inserting or deleting nodes, no memory shifting needed, but to access an element inside the list, the list must be traversed, and that takes time.\nBinary Trees, such as Binary Search Trees and AVL Trees, are great compared to Arrays and Linked Lists because they are BOTH fast at accessing a node, AND fast when it comes to deleting or inserting a node, with no shifts in memory needed.\nWe will take a closer look at how Binary Search Trees (BSTs) and AVL Trees work on the next two pages, but first let's look at how a Binary Tree can be implemented, and how it can be traversed.\nTypes of Binary Trees\nThere are different variants, or types, of Binary Trees worth discussing to get a better understanding of how Binary Trees can be structured.\nThe different kinds of Binary Trees are also worth mentioning now as these words and concepts will be used later in the tutorial.\nBelow are short explanations of different types of Binary Tree structures, and below the explanations are drawings of these kinds of structures to make it as easy to understand as possible.\nA balanced Binary Tree has at most 1 in difference between its left and right subtree heights, for each node in the tree.\nA complete Binary Tree has all levels full of nodes, except the last level, which is can also be full, or filled from left to right. The properties of a complete Binary Tree means it is also balanced.\nA full Binary Tree is a kind of tree where each node has either 0 or 2 child nodes.\nA perfect Binary Tree has all leaf nodes on the same level, which means that all levels are full of nodes, and all internal nodes have two child nodes.The properties of a perfect Binary Tree means it is also full, balanced, and complete.\nBinary Tree Implementation\nLet's implement this Binary Tree:\nThe Binary Tree above can be implemented much like we implemented a Singly Linked List, except that instead of linking each node to one next node, we create a structure where each node can be linked to both its left and right child nodes.\nThis is how a Binary Tree can be implemented:\nExample\nPython:\nBinary Tree Traversal\nGoing through a Tree by visiting every node, one node at a time, is called traversal.\nSince Arrays and Linked Lists are linear data structures, there is only one obvious way to traverse these: start at the first element, or node, and continue to visit the next until you have visited them all.\nBut since a Tree can branch out in different directions (non-linear), there are different ways of traversing Trees.\nThere are two main categories of Tree traversal methods:\nBreadth First Search (BFS) is when the nodes on the same level are visited before going to the next level in the tree. This means that the tree is explored in a more sideways direction.\nDepth First Search (DFS) is when the traversal moves down the tree all the way to the leaf nodes, exploring the tree branch by branch in a downwards direction.\nThere are three different types of DFS traversals:\npre-order\nin-order\npost-order\nThese three Depth First Search traversals are described in detail on the next pages.\nDSA Exercises\nTest Yourself With Exercises\nExercise:\nIn a Binary Tree data structure, like the one below:\nWhat is the relationship between node B and nodes E and F?\nStart the Exercise",
      "examples": [
        "class TreeNode: def __init__(self, data): self.data = data self.left = None self.right = None root = TreeNode('R') nodeA = TreeNode('A') nodeB = TreeNode('B') nodeC = TreeNode('C') nodeD = TreeNode('D') nodeE = TreeNode('E') nodeF = TreeNode('F') nodeG = TreeNode('G') root.left = nodeA root.right = nodeB nodeA.left = nodeC nodeA.right = nodeD nodeB.left = nodeE nodeB.right = nodeF nodeF.left = nodeG # Test print(\"root.right.left.data:\", root.right.left.data)",
        "Node E is B's child node, and node F is B's child node."
      ]
    },
    {
      "title": "DSA Pre-order Traversal",
      "summary": "Pre-order Traversal of Binary Trees\nPre-order Traversal is a type of Depth First Search, where each node is visited in a certain order. Read more about Binary Tree traversals in general here.\nPre-order traversal of a Binary Tree looks like this:\nResult:\nPre-order Traversal is done by visiting the root node first, then recursively do a pre-order traversal of the left subtree, followed by a recursive pre-order traversal of the right subtree. It's used for creating a copy of the tree, prefix notation of an expression tree, etc.\nThis traversal is \"pre\" order because the node is visited \"before\" the recursive pre-order traversal of the left and right subtrees.\nThis is how the code for pre-order traversal looks like:\nExample\nPython:\nThe first node to be printed is node R, as the Pre-order Traversal works by first visiting, or printing, the current node (line 4), before calling the left and right child nodes recursively (line 5 and 6).\nThe preOrderTraversal() function keeps traversing the left subtree recursively (line 5), before going on to traversing the right subtree (line 6). So the next nodes that are printed are 'A' and then 'C'.\nThe first time the argument node is None is when the left child of node C is given as an argument (C has no left child).\nAfter None is returned the first time when calling C's left child, C's right child also returns None, and then the recursive calls continue to propagate back so that A's right child D is the next to be printed.\nThe code continues to propagate back so that the rest of the nodes in R's right subtree gets printed.",
      "examples": [
        "def preOrderTraversal(node): if node is None: return print(node.data, end=\", \") preOrderTraversal(node.left) preOrderTraversal(node.right)",
        "preOrderTraversal()",
        "node",
        "None"
      ]
    },
    {
      "title": "DSA In-order Traversal",
      "summary": "In-order Traversal of Binary Trees\nIn-order Traversal is a type of Depth First Search, where each node is visited in a certain order. Read more about Binary Tree traversals in general here.\nRun the animation below to see how an In-order Traversal of a Binary Tree is done.\nResult:\nIn-order Traversal does a recursive In-order Traversal of the left subtree, visits the root node, and finally, does a recursive In-order Traversal of the right subtree. This traversal is mainly used for Binary Search Trees where it returns values in ascending order.\nWhat makes this traversal \"in\" order, is that the node is visited in between the recursive function calls. The node is visited after the In-order Traversal of the left subtree, and before the In-order Traversal of the right subtree.\nThis is how the code for In-order Traversal looks like:\nExample\nPython:\nThe inOrderTraversal() function keeps calling itself with the current left child node as an argument (line 4) until that argument is None and the function returns (line 2-3).\nThe first time the argument node is None is when the left child of node C is given as an argument (C has no left child).\nAfter that, the data part of node C is printed (line 5), which means that 'C' is the first thing that gets printed.\nThen, node C's right child is given as an argument (line 6), which is None, so the function call returns without doing anything else.\nAfter 'C' is printed, the previous inOrderTraversal() function calls continue to run, so that 'A' gets printed, then 'D', then 'R', and so on.",
      "examples": [
        "def inOrderTraversal(node): if node is None: return inOrderTraversal(node.left) print(node.data, end=\", \") inOrderTraversal(node.right)",
        "inOrderTraversal()",
        "None",
        "node",
        "data"
      ]
    },
    {
      "title": "DSA Post-order Traversal",
      "summary": "Post-order Traversal of Binary Trees\nPost-order Traversal is a type of Depth First Search, where each node is visited in a certain order. Read more about Binary Tree traversals in general here.\nDoing a Post-order Traversal on a Binary Tree can be visualized like this:\nResult:\nPost-order Traversal works by recursively doing a Post-order Traversal of the left subtree and the right subtree, followed by a visit to the root node. It is used for deleting a tree, post-fix notation of an expression tree, etc.\nWhat makes this traversal \"post\" is that visiting a node is done \"after\" the left and right child nodes are called recursively.\nThis is how the code for Post-order Traversal looks like:\nExample\nPython:\nThe postOrderTraversal() function keeps traversing the left subtree recursively (line 4), until None is returned when C's left child node is called as the node argument.\nAfter C's left child node returns None, line 5 runs and C's right child node returns None, and then the letter 'C' is printed (line 6).\nThis means that C is visited, or printed, \"after\" its left and right child nodes are traversed, that is why it is called \"post\" order traversal.\nThe postOrderTraversal() function continues to propagate back to previous recursive function calls, so the next node to be printed is 'D', then 'A'.\nThe function continues to propagate back and printing nodes until all nodes are printed, or visited.",
      "examples": [
        "def postOrderTraversal(node): if node is None: return postOrderTraversal(node.left) postOrderTraversal(node.right) print(node.data, end=\", \")",
        "postOrderTraversal()",
        "None",
        "node"
      ]
    },
    {
      "title": "DSA Array Implementation",
      "summary": "Array Implementation of Binary Trees\nTo avoid the cost of all the shifts in memory that we get from using Arrays, it is useful to implement Binary Trees with pointers from one element to the next, just like Binary Trees are implemented before this point, especially when the Binary Tree is modified often.\nBut in case we read from the Binary Tree a lot more than we modify it, an Array implementation of a Binary Tree can make sense as it needs less memory, it can be easier to implement, and it can be faster for certain operations due to cache locality.\nCache Locality is when the fast cache memory in the computer stores parts of memory that was recently accessed, or when the cache stores parts of memory that is close to the address that is currently accessed. This happens because it is likely that the CPU needs something in the next cycle that is close to what it used in the previous cycle, either close in time or close in space.\nSince Array elements are stored contiguously in memory, one element right after the other, computers are sometimes faster when reading from Arrays because the next element is already cached, available for fast access in case the CPU needs it in the next cycle.\nHow arrays are stored in memory is explained more in detail here.\nConsider this Binary Tree:\nThis Binary Tree can be stored in an Array starting with the root node R on index 0. The rest of the tree can be built by taking a node stored on index ii, and storing its left child node on index 2⋅i+12\\cdot i+1, and its right child node on index 2⋅i+22\\cdot i+2.\nBelow is an Array implementation of the Binary Tree.\nExample\nPython:\nIn this Array implementation, since the Binary Tree nodes are placed in an array, much of the code is about accessing nodes using indexes, and about how to find the correct indexes.\nLet's say we want to find the left and right child nodes of node B. Because B is on index 2, B's left child is on index 2⋅2+1=52\\cdot 2+1=5, which is node E, right? And B's right child is on index 2⋅2+2=62\\cdot 2+2=6, which is node F, and that also fits with the drawing above, right?\nAs you can see on line 1, this implementation requires empty array elements where nodes have no child nodes. So to avoid wasting space on empty Array elements, Binary Trees stored using Array implementation should be a \"perfect\" Binary Tree, or a nearly perfect one.\nA perfect Binary Tree is when every internal node have exactly two child nodes, and all leaf nodes are on the same level.\nIf we remove the G node in the Binary Tree above, it looks like this:\nAnd the first line in the code above can be written without wasting space on empty Array elements:\nThis is how the three different DFS traversals can be done on an Array implementation of a Binary Tree.\nExample\nPython:\nBy comparing how these traversals are done in an array implementation to how the pointer implementation was traversed, you can see that the pre-order, in-order, and post-order traversals works in the same recursive way.",
      "examples": [
        "binary_tree_array = ['R', 'A', 'B', 'C', 'D', 'E', 'F', None, None, None, None, None, None, 'G'] def left_child_index(index): return 2 * index + 1 def right_child_index(index): return 2 * index + 2 def get_data(index): if 0 <= index < len(binary_tree_array): return binary_tree_array[index] return None right_child = right_child_index(0) left_child_of_right_child = left_child_index(right_child) data = get_data(left_child_of_right_child) print(\"root.right.left.data:\", data)",
        "binary_tree_array = ['R', 'A', 'B', 'C', 'D', 'E', 'F']",
        "binary_tree_array = ['R', 'A', 'B', 'C', 'D', 'E', 'F', None, None, None, None, None, None, 'G'] def left_child_index(index): return 2 * index + 1 def right_child_index(index): return 2 * index + 2 def pre_order(index): if index >= len(binary_tree_array) or binary_tree_array[index] is None: return [] return [binary_tree_array[index]] + pre_order(left_child_index(index)) + pre_order(right_child_index(index)) def in_order(index): if index >= len(binary_tree_array) or binary_tree_array[index] is None: return [] return in_order(left_child_index(index)) + [binary_tree_array[index]] + in_order(right_child_index(index)) def post_order(index): if index >= len(binary_tree_array) or binary_tree_array[index] is None: return [] return post_order(left_child_index(index)) + post_order(right_child_index(index)) + [binary_tree_array[index]] print(\"Pre-order Traversal:\", pre_order(0)) print(\"In-order Traversal:\", in_order(0)) print(\"Post-order Traversal:\", post_order(0))"
      ]
    },
    {
      "title": "DSA Binary Search Trees",
      "summary": "A Binary Search Tree is a Binary Tree where every node's left child has a lower value, and every node's right child has a higher value.\nA clear advantage with Binary Search Trees is that operations like search, delete, and insert are fast and done without having to shift values in memory.\nBinary Search Trees\nA Binary Search Tree (BST) is a type of Binary Tree data structure, where the following properties must be true for any node \"X\" in the tree:\nThe X node's left child and all of its descendants (children, children's children, and so on) have lower values than X's value.\nThe right child, and all its descendants have higher values than X's value.\nLeft and right subtrees must also be Binary Search Trees.\nThese properties makes it faster to search, add and delete values than a regular binary tree.\nTo make this as easy to understand and implement as possible, let's also assume that all values in a Binary Search Tree are unique.\nUse the Binary Search Tree below to better understand these concepts and relevant terminology.\nBinary Search Tree (BST)\nTree size (n=8)\nRoot node\n7's left child\n7's right child\nTree height (h=3)\n15's height (h=2)\n13's right subtree\n13's in-order successor\nChild nodes\nParent/Internal nodes\nLeaf nodes\nThe size of a tree is the number of nodes in it (nn).\nA subtree starts with one of the nodes in the tree as a local root, and consists of that node and all its descendants.\nThe descendants of a node are all the child nodes of that node, and all their child nodes, and so on. Just start with a node, and the descendants will be all nodes that are connected below that node.\nThe node's height is the maximum number of edges between that node and a leaf node.\nA node's in-order successor is the node that comes after it if we were to do in-order traversal. In-order traversal of the BST above would result in node 13 coming before node 14, and so the successor of node 13 is node 14.\nTraversal of a Binary Search Tree\nJust to confirm that we actually have a Binary Search Tree data structure in front of us, we can check if the properties at the top of this page are true. So for every node in the figure above, check if all the values to the left of the node are lower, and that all values to the right are higher.\nAnother way to check if a Binary Tree is BST, is to do an in-order traversal (like we did on the previous page) and check if the resulting list of values are in an increasing order.\nThe code below is an implementation of the Binary Search Tree in the figure above, with traversal.\nExample\nPython:\nAs we can see by running the code example above, the in-order traversal produces a list of numbers in an increasing (ascending) order, which means that this Binary Tree is a Binary Search Tree.\nSearch for a Value in a BST\nSearching for a value in a BST is very similar to how we found a value using Binary Search on an array.\nFor Binary Search to work, the array must be sorted already, and searching for a value in an array can then be done really fast.\nSimilarly, searching for a value in a BST can also be done really fast because of how the nodes are placed.\nHow it works:\nStart at the root node.\nIf this is the value we are looking for, return.\nIf the value we are looking for is higher, continue searching in the right subtree.\nIf the value we are looking for is lower, continue searching in the left subtree.\nIf the subtree we want to search does not exist, depending on the programming language, return None, or NULL, or something similar, to indicate that the value is not inside the BST.\nUse the animation below to see how we search for a value in a Binary Search Tree.\nClick Search.\nThe algorithm above can be implemented like this:\nExample\nPython:\nThe time complexity for searching a BST for a value is O(h)O(h), where hh is the height of the tree.\nFor a BST with most nodes on the right side for example, the height of the tree becomes larger than it needs to be, and the worst case search will take longer. Such trees are called unbalanced.\nBoth Binary Search Trees above have the same nodes, and in-order traversal of both trees gives us the same result but the height is very different. It takes longer time to search the unbalanced tree above because it is higher.\nWe will use the next page to describe a type of Binary Tree called AVL Trees. AVL trees are self-balancing, which means that the height of the tree is kept to a minimum so that operations like search, insertion and deletion take less time.\nInsert a Node in a BST\nInserting a node in a BST is similar to searching for a value.\nHow it works:\nStart at the root node.\nCompare each node:\nIs the value lower? Go left.\nIs the value higher? Go right.\nIs the value lower? Go left.\nIs the value higher? Go right.\nContinue to compare nodes with the new value until there is no right or left to compare with. That is where the new node is inserted.\nInserting nodes as described above means that an inserted node will always become a new leaf node.\nUse the simulation below to see how new nodes are inserted.\nClick Insert.\nAll nodes in the BST are unique, so in case we find the same value as the one we want to insert, we do nothing.\nThis is how node insertion in BST can be implemented:\nExample\nPython:\nFind The Lowest Value in a BST Subtree\nThe next section will explain how we can delete a node in a BST, but to do that we need a function that finds the lowest value in a node's subtree.\nHow it works:\nStart at the root node of the subtree.\nGo left as far as possible.\nThe node you end up in is the node with the lowest value in that BST subtree.\nIn the figure below, if we start at node 13 and keep going left, we end up in node 3, which is the lowest value, right?\nAnd if we start at node 15 and keep going left, we end up in node 14, which is the lowest value in node 15's subtree.\nThis is how the function for finding the lowest value in the subtree of a BST node looks like:\nExample\nPython:\nWe will use this minValueNode() function in the section below, to find a node's in-order successor, and use that to delete a node.\nDelete a Node in a BST\nTo delete a node, our function must first search the BST to find it.\nAfter the node is found there are three different cases where deleting a node must be done differently.\nHow it works:\nIf the node is a leaf node, remove it by removing the link to it.\nIf the node only has one child node, connect the parent node of the node you want to remove to that child node.\nIf the node has both right and left child nodes: Find the node's in-order successor, change values with that node, then delete it.\nIn step 3 above, the successor we find will always be a leaf node, and because it is the node that comes right after the node we want to delete, we can swap values with it and delete it.\nUse the animation below to see how different nodes are deleted.\nNode 8 is a leaf node (case 1), so after we find it, we can just delete it.\nNode 19 has only one child node (case 2). To delete node 19, the parent node 15 is connected directly to node 18, and then node 19 can be removed.\nNode 13 has two child nodes (case 3). We find the successor, the node that comes right after during in-order traversal, by finding the lowest node in node 13's right subtree, which is node 14. Value 14 is put into node 13, and then we can delete node 14.\nThis is how a BST can be implemented with functionality for deleting a node:\nExample\nPython:\nLine 1: The node argument here makes it possible for the function to call itself recursively on smaller and smaller subtrees in the search for the node with the data we want to delete.\nLine 2-8: This is searching for the node with correct data that we want to delete.\nLine 9-22: The node we want to delete has been found. There are three such cases:\nCase 1: Node with no child nodes (leaf node). None is returned, and that becomes the parent node's new left or right value by recursion (line 6 or 8).\nCase 2: Node with either left or right child node. That left or right child node becomes the parent's new left or right child through recursion (line 7 or 9).\nCase 3: Node has both left and right child nodes. The in-order successor is found using the minValueNode() function. We keep the successor's value by setting it as the value of the node we want to delete, and then we can delete the successor node.\nLine 24: node is returned to maintain the recursive functionality.\nBST Compared to Other Data Structures\nBinary Search Trees take the best from two other data structures: Arrays and Linked Lists.\nSearching a BST is just as fast as Binary Search on an array, with the same time complexity O(\\log n)O(\\log n).\nAnd deleting and inserting new values can be done without shifting elements in memory, just like with Linked Lists.\nBST Balance and Time Complexity\nOn a Binary Search Tree, operations like inserting a new node, deleting a node, or searching for a node are actually O(h)O(h). That means that the higher the tree is (hh), the longer the operation will take.\nThe reason why we wrote that searching for a value is O(\\log n)O(\\log n) in the table above is because that is true if the tree is \"balanced\", like in the image below.\nWe call this tree balanced because there are approximately the same number of nodes on the left and right side of the tree.\nThe exact way to tell that a Binary Tree is balanced is that the height of the left and right subtrees of any node only differs by one. In the image above, the left subtree of the root node has height h=2h=2, and the right subtree has height h=3h=3.\nFor a balanced BST, with a large number of nodes (big nn), we get height h \\approx \\log_2 nh \\approx \\log_2 n, and therefore the time complexity for searching, deleting, or inserting a node can be written as O(h) = O(\\log n)O(h) = O(\\log n).\nBut, in case the BST is completely unbalanced, like in the image below, the height of the tree is approximately the same as the number of nodes, h \\approx nh \\approx n, and we get time complexity O(h) = O(n)O(h) = O(n) for searching, deleting, or inserting a node.\nSo, to optimize operations on a BST, the height must be minimized, and to do that the tree must be balanced.\nAnd keeping a Binary Search Tree balanced is exactly what AVL Trees do, which is the data structure explained on the next page.\nDSA Exercises\nTest Yourself With Exercises\nExercise:\nInserting a node with value 6 in this Binary Search Tree:\nWhere is the new node inserted?\nStart the Exercise",
      "examples": [
        "class TreeNode: def __init__(self, data): self.data = data self.left = None self.right = None def inOrderTraversal(node): if node is None: return inOrderTraversal(node.left) print(node.data, end=\", \") inOrderTraversal(node.right) root = TreeNode(13) node7 = TreeNode(7) node15 = TreeNode(15) node3 = TreeNode(3) node8 = TreeNode(8) node14 = TreeNode(14) node19 = TreeNode(19) node18 = TreeNode(18) root.left = node7 root.right = node15 node7.left = node3 node7.right = node8 node15.left = node14 node15.right = node19 node19.left = node18 # Traverse inOrderTraversal(root)",
        "def search(node, target): if node is None: return None elif node.data == target: return node elif target < node.data: return search(node.left, target) else: return search(node.right, target)",
        "def insert(node, data): if node is None: return TreeNode(data) else: if data < node.data: node.left = insert(node.left, data) elif data > node.data: node.right = insert(node.right, data) return node",
        "def minValueNode(node): current = node while current.left is not None: current = current.left return current",
        "def delete(node, data): if not node: return None if data < node.data: node.left = delete(node.left, data) elif data > node.data: node.right = delete(node.right, data) else: # Node with only one child or no child if not node.left: temp = node.right node = None return temp elif not node.right: temp = node.left node = None return temp # Node with two children, get the in-order successor node.data = minValueNode(node.right).data node.right = delete(node.right, node.data) return node",
        "The node with value 6 becomes the right child node of the node with value .",
        "None",
        "NULL",
        "minValueNode()",
        "node",
        "data"
      ]
    },
    {
      "title": "DSA AVL Trees",
      "summary": "The AVL Tree is a type of Binary Search Tree named after two Soviet inventors Georgy Adelson-Velsky and Evgenii Landis who invented the AVL Tree in 1962.\nAVL trees are self-balancing, which means that the tree height is kept to a minimum so that a very fast runtime is guaranteed for searching, inserting and deleting nodes, with time complexity O(logn)O( \\log n).\nAVL Trees\nThe only difference between a regular Binary Search Tree and an AVL Tree is that AVL Trees do rotation operations in addition, to keep the tree balance.\nA Binary Search Tree is in balance when the difference in height between left and right subtrees is less than 2.\nBy keeping balance, the AVL Tree ensures a minimum tree height, which means that search, insert, and delete operations can be done really fast.\nThe two trees above are both Binary Search Trees, they have the same nodes, and the same in-order traversal (alphabetical), but the height is very different because the AVL Tree has balanced itself.\nStep through the building of an AVL Tree in the animation below to see how the balance factors are updated, and how rotation operations are done when required to restore the balance.\nContinue reading to learn more about how the balance factor is calculated, how rotation operations are done, and how AVL Trees can be implemented.\nLeft and Right Rotations\nTo restore balance in an AVL Tree, left or right rotations are done, or a combination of left and right rotations.\nThe previous animation shows one specific left rotation, and one specific right rotation.\nBut in general, left and right rotations are done like in the animation below.\nNotice how the subtree changes its parent. Subtrees change parent in this way during rotation to maintain the correct in-order traversal, and to maintain the BST property that the left child is less than the right child, for all nodes in the tree.\nAlso keep in mind that it is not always the root node that become unbalanced and need rotation.\nThe Balance Factor\nA node's balance factor is the difference in subtree heights.\nThe subtree heights are stored at each node for all nodes in an AVL Tree, and the balance factor is calculated based on its subtree heights to check if the tree has become out of balance.\nThe height of a subtree is the number of edges between the root node of the subtree and the leaf node farthest down in that subtree.\nThe Balance Factor (BFBF) for a node (XX) is the difference in height between its right and left subtrees.\nBF(X)=height(rightSubtree(X))−height(leftSubtree(X)) BF(X) = height(rightSubtree(X)) - height(leftSubtree(X))\nBalance factor values\n0: The node is in balance.\nmore than 0: The node is \"right heavy\".\nless than 0: The node is \"left heavy\".\nIf the balance factor is less than -1, or more than 1, for one or more nodes in the tree, the tree is considered not in balance, and a rotation operation is needed to restore balance.\nLet's take a closer look at the different rotation operations that an AVL Tree can do to regain balance.\nThe Four \"out-of-balance\" Cases\nWhen the balance factor of just one node is less than -1, or more than 1, the tree is regarded as out of balance, and a rotation is needed to restore balance.\nThere are four different ways an AVL Tree can be out of balance, and each of these cases require a different rotation operation.\nSee animations and explanations of these cases below.\nThe Left-Left (LL) Case\nThe node where the unbalance is discovered is left heavy, and the node's left child node is also left heavy.\nWhen this LL case happens, a single right rotation on the unbalanced node is enough to restore balance.\nStep through the animation below to see the LL case, and how the balance is restored by a single right rotation.\nAs you step through the animation above, two LL cases happen:\nWhen D is added, the balance factor of Q becomes -2, which means the tree is unbalanced. This is an LL case because both the unbalance node Q and its left child node P are left heavy (negative balance factors). A single right rotation at node Q restores the tree balance.\nAfter nodes L, C, and B are added, P's balance factor is -2, which means the tree is out of balance. This is also an LL case because both the unbalanced node P and its left child node D are left heavy. A single right rotation restores the balance.\nNote: The second time the LL case happens in the animation above, a right rotation is done, and L goes from being the right child of D to being the left child of P. Rotations are done like that to keep the correct in-order traversal ('B, C, D, L, P, Q' in the animation above). Another reason for changing parent when a rotation is done is to keep the BST property, that the left child is always lower than the node, and that the right child always higher.\nThe Right-Right (RR) Case\nA Right-Right case happens when a node is unbalanced and right heavy, and the right child node is also right heavy.\nA single left rotation at the unbalanced node is enough to restore balance in the RR case.\nThe RR case happens two times in the animation above:\nWhen node D is inserted, A becomes unbalanced, and bot A and B are right heavy. A left rotation at node A restores the tree balance.\nAfter nodes E, C and F are inserted, node B becomes unbalanced. This is an RR case because both node B and its right child node D are right heavy. A left rotation restores the tree balance.\nThe Left-Right (LR) Case\nThe Left-Right case is when the unbalanced node is left heavy, but its left child node is right heavy.\nIn this LR case, a left rotation is first done on the left child node, and then a right rotation is done on the original unbalanced node.\nStep through the animation below to see how the Left-Right case can happen, and how the rotation operations are done to restore balance.\nAs you are building the AVL Tree in the animation above, the Left-Right case happens 2 times, and rotation operations are required and done to restore balance:\nWhen K is inserted, node Q gets unbalanced with a balance factor of -2, so it is left heavy, and its left child E is right heavy, so this is a Left-Right case.\nAfter nodes C, F, and G are inserted, node K becomes unbalanced and left heavy, with its left child node E right heavy, so it is a Left-Right case.\nThe Right-Left (RL) Case\nThe Right-Left case is when the unbalanced node is right heavy, and its right child node is left heavy.\nIn this case we first do a right rotation on the unbalanced node's right child, and then we do a left rotation on the unbalanced node itself.\nStep through the animation below to see how the Right-Left case can occur, and how rotations are done to restore the balance.\nAfter inserting node B, we get a Right-Left case because node A becomes unbalanced and right heavy, and its right child is left heavy. To restore balance, a right rotation is first done on node F, and then a left rotation is done on node A.\nThe next Right-Left case occurs after nodes G, E, and D are added. This is a Right-Left case because B is unbalanced and right heavy, and its right child F is left heavy. To restore balance, a right rotation is first done on node F, and then a left rotation is done on node B.\nRetracing in AVL Trees\nAfter inserting or deleting a node in an AVL tree, the tree may become unbalanced. To find out if the tree is unbalanced, we need to update the heights and recalculate the balance factors of all ancestor nodes.\nThis process, known as retracing, is handled through recursion. As the recursive calls propagate back to the root after an insertion or deletion, each ancestor node's height is updated and the balance factor is recalculated. If any ancestor node is found to have a balance factor outside the range of -1 to 1, a rotation is performed at that node to restore the tree's balance.\nIn the simulation below, after inserting node F, the nodes C, E and H are all unbalanced, but since retracing works through recursion, the unbalance at node H is discovered and fixed first, which in this case also fixes the unbalance in nodes E and C.\nAfter node F is inserted, the code will retrace, calculating balancing factors as it propagates back up towards the root node. When node H is reached and the balancing factor -2 is calculated, a right rotation is done. Only after this rotation is done, the code will continue to retrace, calculating balancing factors further up on ancestor nodes E and C.\nBecause of the rotation, balancing factors for nodes E and C stay the same as before node F was inserted.\nAVL Insert Node Implementation\nThis code is based on the BST implementation on the previous page, for inserting nodes.\nThere is only one new attribute for each node in the AVL tree compared to the BST, and that is the height, but there are many new functions and extra code lines needed for the AVL Tree implementation because of how the AVL Tree rebalances itself.\nThe implementation below builds an AVL tree based on a list of characters, to create the AVL Tree in the simulation above. The last node to be inserted 'F', also triggers a right rotation, just like in the simulation above.\nExample\nPython:\nAVL Delete Node Implementation\nWhen deleting a node that is not a leaf node, the AVL Tree requires the minValueNode() function to find a node's next node in the in-order traversal. This is the same as when deleting a node in a Binary Search Tree, as explained on the previous page.\nTo delete a node in an AVL Tree, the same code to restore balance is needed as for the code to insert a node.\nExample\nPython:\nTime Complexity for AVL Trees\nTake a look at the unbalanced Binary Search Tree below. Searching for \"M\" means that all nodes except 1 must be compared. But searching for \"M\" in the AVL Tree below only requires us to visit 4 nodes.\nSo in worst case, algorithms like search, insert, and delete must run through the whole height of the tree. This means that keeping the height (hh ) of the tree low, like we do using AVL Trees, gives us a lower runtime.\nSee the comparison of the time complexities between Binary Search Trees and AVL Trees below, and how the time complexities relate to the height (hh) of the tree, and the number of nodes (nn) in the tree.\nThe BST is not self-balancing. This means that a BST can be very unbalanced, almost like a long chain, where the height is nearly the same as the number of nodes. This makes operations like searching, deleting and inserting nodes slow, with time complexity O(h)=O(n)O(h) = O(n).\nThe AVL Tree however is self-balancing. That means that the height of the tree is kept to a minimum so that operations like searching, deleting and inserting nodes are much faster, with time complexity O(h)=O(logn)O(h) = O( \\log n).\nO(logn)O( \\log n) Explained\nThe fact that the time complexity is O(h)=O(logn)O(h) = O( \\log n) for search, insert, and delete on an AVL Tree with height hh and nodes nn can be explained like this:\nImagine a perfect Binary Tree where all nodes have two child nodes except on the lowest level, like the AVL Tree below.\nThe number of nodes on each level in such an AVL Tree are:\n1,2,4,8,16,32,..1, 2, 4, 8, 16, 32, ..\nWhich is the same as:\n20,21,22,23,24,25,..2^0, 2^1, 2^2, 2^3, 2^4, 2^5, ..\nTo get the number of nodes nn in a perfect Binary Tree with height h=3h=3, we can add the number of nodes on each level together:\nn3=20+21+22+23=15n_3=2^0 + 2^1 + 2^2 + 2^3 = 15\nWhich is actually the same as:\nn3=24−1=15n_3=2^4 - 1 = 15\nAnd this is actually the case for larger trees as well! If we want to get the number of nodes nn in a tree with height h=5h=5 for example, we find the number of nodes like this:\nn5=26−1=63n_5=2^6 - 1 = 63\nSo in general, the relationship between the height hh of a perfect Binary Tree and the number of nodes in it nn , can be expressed like this:\nnh=2h+1−1n_h = 2^{h+1} - 1\nNote: The formula above can also be found by calculating the sum of the geometric series 20+21+22+23+...+2n2^0 + 2^1 + 2^2+ 2^3 + ... + 2^n\nWe know that the time complexity for searching, deleting, or inserting a node in an AVL tree is O(h)O(h) , but we want to argue that the time complexity is actually O(log(n))O(\\log(n)) , so we need to find the height hh described by the number of nodes nn:\nn=2h+1−1n+1=2h+1log2(n+1)=log2(2h+1)h=log2(n+1)−1O(h)=O(logn) \\begin{equation} \\begin{aligned} n & = 2^{h+1}-1 \\\\ n+1 & = 2^{h+1} \\\\ \\log_2(n+1) & = \\log_2(2^{h+1}) \\\\ h & = \\log_2(n+1) - 1 \\\\ \\\\ O(h) & = O(\\log{n}) \\end{aligned} \\end{equation}\nHow the last line above is derived might not be obvious, but for a Binary Tree with a lot of nodes (big nn), the \"+1\" and \"-1\" terms are not important when we consider time complexity. For more details on how to calculate the time complexity using Big O notation, see this page.\nThe math above shows that the time complexity for search, delete, and insert operations on an AVL Tree O(h)O(h) , can actually be expressed as O(logn)O(\\log{n}) , which is fast, a lot faster than the time complexity for BSTs which is O(n)O(n) .\nDSA Exercises\nTest Yourself With Exercises\nExercise:\nEach node in the AVL Tree below is displayed together with its balance factor:\nWhat is the balance factor?\nStart the Exercise",
      "examples": [
        "class TreeNode: def __init__(self, data): self.data = data self.left = None self.right = None self.height = 1 def getHeight(node): if not node: return 0 return node.height def getBalance(node): if not node: return 0 return getHeight(node.left) - getHeight(node.right) def rightRotate(y): print('Rotate right on node',y.data) x = y.left T2 = x.right x.right = y y.left = T2 y.height = 1 + max(getHeight(y.left), getHeight(y.right)) x.height = 1 + max(getHeight(x.left), getHeight(x.right)) return x def leftRotate(x): print('Rotate left on node',x.data) y = x.right T2 = y.left y.left = x x.right = T2 x.height = 1 + max(getHeight(x.left), getHeight(x.right)) y.height = 1 + max(getHeight(y.left), getHeight(y.right)) return y def insert(node, data): if not node: return TreeNode(data) if data < node.data: node.left = insert(node.left, data) elif data > node.data: node.right = insert(node.right, data) # Update the balance factor and balance the tree node.height = 1 + max(getHeight(node.left), getHeight(node.right)) balance = getBalance(node) # Balancing the tree # Left Left if balance > 1 and getBalance(node.left) >= 0: return rightRotate(node) # Left Right if balance > 1 and getBalance(node.left) < 0: node.left = leftRotate(node.left) return rightRotate(node) # Right Right if balance < -1 and getBalance(node.right) <= 0: return leftRotate(node) # Right Left if balance < -1 and getBalance(node.right) > 0: node.right = rightRotate(node.right) return leftRotate(node) return node def inOrderTraversal(node): if node is None: return inOrderTraversal(node.left) print(node.data, end=\", \") inOrderTraversal(node.right) # Inserting nodes root = None letters = ['C', 'B', 'E', 'A', 'D', 'H', 'G', 'F'] for letter in letters: root = insert(root, letter) inOrderTraversal(root)",
        "def minValueNode(node): current = node while current.left is not None: current = current.left return current def delete(node, data): if not node: return node if data < node.data: node.left = delete(node.left, data) elif data > node.data: node.right = delete(node.right, data) else: if node.left is None: temp = node.right node = None return temp elif node.right is None: temp = node.left node = None return temp temp = minValueNode(node.right) node.data = temp.data node.right = delete(node.right, temp.data) if node is None: return node # Update the balance factor and balance the tree node.height = 1 + max(getHeight(node.left), getHeight(node.right)) balance = getBalance(node) # Balancing the tree # Left Left if balance > 1 and getBalance(node.left) >= 0: return rightRotate(node) # Left Right if balance > 1 and getBalance(node.left) < 0: node.left = leftRotate(node.left) return rightRotate(node) # Right Right if balance < -1 and getBalance(node.right) <= 0: return leftRotate(node) # Right Left if balance < -1 and getBalance(node.right) > 0: node.right = rightRotate(node.right) return leftRotate(node) return node",
        "The balance factor is the difference between each node's left and right subtree .",
        "minValueNode()"
      ]
    },
    {
      "title": "DSA Graphs",
      "summary": "Graphs\nA Graph is a non-linear data structure that consists of vertices (nodes) and edges.\nA vertex, also called a node, is a point or an object in the Graph, and an edge is used to connect two vertices with each other.\nGraphs are non-linear because the data structure allows us to have different paths to get from one vertex to another, unlike with linear data structures like Arrays or Linked Lists.\nGraphs are used to represent and solve problems where the data consists of objects and relationships between them, such as:\nSocial Networks: Each person is a vertex, and relationships (like friendships) are the edges. Algorithms can suggest potential friends.\nMaps and Navigation: Locations, like a town or bus stops, are stored as vertices, and roads are stored as edges. Algorithms can find the shortest route between two locations when stored as a Graph.\nInternet: Can be represented as a Graph, with web pages as vertices and hyperlinks as edges.\nBiology: Graphs can model systems like neural networks or the spread of diseases.\nGraph Properties\nUse the animation below to get an understanding of the different Graph properties, and how these properties can be combined.\nWeighted\nConnected\nDirected\nCyclic\nLoop\nA weighted Graph is a Graph where the edges have values. The weight value of an edge can represent things like distance, capacity, time, or probability.\nA connected Graph is when all the vertices are connected through edges somehow. A Graph that is not connected, is a Graph with isolated (disjoint) subgraphs, or single isolated vertices.\nA directed Graph, also known as a digraph, is when the edges between the vertex pairs have a direction. The direction of an edge can represent things like hierarchy or flow.\nA cyclic Graph is defined differently depending on whether it is directed or not:\nA directed cyclic Graph is when you can follow a path along the directed edges that goes in circles. Removing the directed edge from F to G in the animation above makes the directed Graph not cyclic anymore.\nAn undirected cyclic Graph is when you can come back to the same vertex you started at without using the same edge more than once. The undirected Graph above is cyclic because we can start and end up in vertes C without using the same edge twice.\nA loop, also called a self-loop, is an edge that begins and ends on the same vertex. A loop is a cycle that only consists of one edge. By adding the loop on vertex A in the animation above, the Graph becomes cyclic.\nGraph Representations\nA Graph representation tells us how a Graph is stored in memory.\nDifferent Graph representations can:\ntake up more or less space.\nbe faster or slower to search or manipulate.\nbe better suited depending on what type of Graph we have (weighted, directed, etc.), and what we want to do with the Graph.\nbe easier to understand and implement than others.\nBelow are short introductions of the different Graph representations, but Adjacency Matrix is the representation we will use for Graphs moving forward in this tutorial, as it is easy to understand and implement, and works in all cases relevant for this tutorial.\nGraph representations store information about which vertices are adjacent, and how the edges between the vertices are. Graph representations are slightly different if the edges are directed or weighted.\nTwo vertices are adjacent, or neighbors, if there is an edge between them.\nAdjacency Matrix Graph Representation\nAdjacency Matrix is the Graph representation (structure) we will use for this tutorial.\nHow to implement an Adjacency Matrix is shown on the next page.\nThe Adjacency Matrix is a 2D array (matrix) where each cell on index (i,j) stores information about the edge from vertex i to vertex j.\nBelow is a Graph with the Adjacency Matrix representation next to it.\nThe adjacency matrix above represents an undirected Graph, so the values '1' only tells us where the edges are. Also, the values in the adjacency matrix is symmetrical because the edges go both ways (undirected Graph).\nTo create a directed Graph with an adjacency matrix, we must decide which vertices the edges go from and to, by inserting the value at the correct indexes (i,j). To represent a weighted Graph we can put other values than '1' inside the adjacency matrix.\nBelow is a directed and weighted Graph with the Adjacency Matrix representation next to it.\nIn the adjacency matrix above, the value 3 on index (0,1) tells us there is an edge from vertex A to vertex B, and the weight for that edge is 3.\nAs you can see, the weights are placed directly into the adjacency matrix for the correct edge, and for a directed Graph, the adjacency matrix does not have to be symmetric.\nAdjacency List Graph Representation\nIn case we have a 'sparse' Graph with many vertices, we can save space by using an Adjacency List compared to using an Adjacency Matrix, because an Adjacency Matrix would reserve a lot of memory on empty Array elements for edges that don't exist.\nA 'sparse' Graph is a Graph where each vertex only has edges to a small portion of the other vertices in the Graph.\nAn Adjacency List has an array that contains all the vertices in the Graph, and each vertex has a Linked List (or Array) with the vertex's edges.\nIn the adjacency list above, the vertices A to D are placed in an Array, and each vertex in the array has its index written right next to it.\nEach vertex in the Array has a pointer to a Linked List that represents that vertex's edges. More specifically, the Linked List contains the indexes to the adjacent (neighbor) vertices.\nSo for example, vertex A has a link to a Linked List with values 3, 1, and 2. These values are the indexes to A's adjacent vertices D, B, and C.\nAn Adjacency List can also represent a directed and weighted Graph, like this:\nIn the Adjacency List above, vertices are stored in an Array. Each vertex has a pointer to a Linked List with edges stored as i,w, where i is the index of the vertex the edge goes to, and w is the weight of that edge.\nNode D for example, has a pointer to a Linked List with an edge to vertex A. The values 0,4 means that vertex D has an edge to vertex on index 0 (vertex A), and the weight of that edge is 4.\nDSA Exercises\nTest Yourself With Exercises\nExercise:\nHow can the Graph below be described?\nStart the Exercise",
      "examples": [
        "The Graph is cyclic, connected, and .",
        "(i,j)",
        "i",
        "j",
        "3",
        "(0,1)",
        "i,w",
        "w",
        "0,4",
        "0",
        "4"
      ]
    },
    {
      "title": "DSA Graphs Implementation",
      "summary": "A Basic Graph Implementation\nBefore we can run algorithms on a Graph, we must first implement it somehow.\nTo implement a Graph we will use an Adjacency Matrix, like the one below.\nTo store data for each vertex, in this case the letters A, B, C, and D, the data is put in a separate array that matches the indexes in the adjacency matrix, like this:\nFor an undirected and not weighted Graph, like in the image above, an edge between vertices i and j is stored with value 1. It is stored as 1 on both places (j,i) and (i,j) because the edge goes in both directions. As you can see, the matrix becomes diagonally symmetric for such undirected Graphs.\nLet's look at something more specific. In the adjacency matrix above, vertex A is on index 0, and vertex D is on index 3, so we get the edge between A and D stored as value 1 in position (0,3) and (3,0), because the edge goes in both directions.\nBelow is a basic implementation of the undirected Graph from the image above.\nExample\nPython:\nThis implementation is basically just a two dimensional array, but to get a better sense of how the vertices are connected by edges in the Graph we have just implemented, we can run this function:\nExample\nPython:\nGraph Implementation Using Classes\nA more proper way to store a Graph is to add an abstraction layer using classes so that a Graph's vertices, edges, and relevant methods, like algorithms that we will implement later, are contained in one place.\nProgramming languages with built-in object-oriented functionality like Python and Java, make implementation of Graphs using classes much easier than languages like C, without this built-in functionality.\nHere is how the undirected Graph above can be implemented using classes.\nExample\nPython:\nIn the code above, the matrix symmetry we get for undirected Graphs is provided for on line 9 and 10, and this saves us some code when initializing the edges in the Graph on lines 29-32.\nImplementation of Directed and Weighted Graphs\nTo implement a Graph that is directed and weighted, we just need to do a few changes to previous implementation of the undirected Graph.\nTo create directed Graphs, we just need to remove line 10 in the previous example code, so that the matrix is not automatically symmetric anymore.\nThe second change we need to do is to add a weight argument to the add_edge() method, so that instead of just having value 1 to indicate that there is an edge between two vertices, we use the actual weight value to define the edge.\nBelow is the implementation of the directed and weighted Graph above.\nExample\nPython:\nLine 3: All edges are set to None initially.\nLine 7: The weight can now be added to an edge with the additional weight argument.\nLine 10: By removing line 10, the Graph can now be set up as being directed.\nOn the next page we will see how Graphs can be traversed, and on the next pages after that we will look at different algorithms that can run on the Graph data structure.\nDSA Exercises\nTest Yourself With Exercises\nExercise:\nHow are the edges in a graph implemented?\nStart the Exercise",
      "examples": [
        "vertexData = [ 'A', 'B', 'C', 'D']",
        "vertexData = ['A', 'B', 'C', 'D'] adjacency_matrix = [ [0, 1, 1, 1], # Edges for A [1, 0, 1, 0], # Edges for B [1, 1, 0, 0], # Edges for C [1, 0, 0, 0] # Edges for D ] def print_adjacency_matrix(matrix): print(\"\\nAdjacency Matrix:\") for row in matrix: print(row) print('vertexData:',vertexData) print_adjacency_matrix(adjacency_matrix)",
        "def print_connections(matrix, vertices): print(\"\\nConnections for each vertex:\") for i in range(len(vertices)): print(f\"{vertices[i]}: \", end=\"\") for j in range(len(vertices)): if matrix[i][j]: # if there is a connection print(vertices[j], end=\" \") print() # new line",
        "class Graph: def __init__(self, size): self.adj_matrix = [[0] * size for _ in range(size)] self.size = size self.vertex_data = [''] * size def add_edge(self, u, v): if 0 <= u < self.size and 0 <= v < self.size: self.adj_matrix[u][v] = 1 self.adj_matrix[v][u] = 1 def add_vertex_data(self, vertex, data): if 0 <= vertex < self.size: self.vertex_data[vertex] = data def print_graph(self): print(\"Adjacency Matrix:\") for row in self.adj_matrix: print(' '.join(map(str, row))) print(\"\\nVertex Data:\") for vertex, data in enumerate(self.vertex_data): print(f\"Vertex {vertex}: {data}\") g = Graph(4) g.add_vertex_data(0, 'A') g.add_vertex_data(1, 'B') g.add_vertex_data(2, 'C') g.add_vertex_data(3, 'D') g.add_edge(0, 1) # A - B g.add_edge(0, 2) # A - C g.add_edge(0, 3) # A - D g.add_edge(1, 2) # B - C g.print_graph()",
        "class Graph: def __init__(self, size): self.adj_matrix = [[None] * size for _ in range(size)] self.size = size self.vertex_data = [''] * size def add_edge(self, u, v, weight): if 0 <= u < self.size and 0 <= v < self.size: self.adj_matrix[u][v] = weight\nself.adj_matrix[v][u] = weight\ndef add_vertex_data(self, vertex, data): if 0 <= vertex < self.size: self.vertex_data[vertex] = data def print_graph(self): print(\"Adjacency Matrix:\") for row in self.adj_matrix: print(' '.join(map(lambda x: str(x) if x is not None else '0', row))) print(\"\\nVertex Data:\") for vertex, data in enumerate(self.vertex_data): print(f\"Vertex {vertex}: {data}\") g = Graph(4) g.add_vertex_data(0, 'A') g.add_vertex_data(1, 'B') g.add_vertex_data(2, 'C') g.add_vertex_data(3, 'D') g.add_edge(0, 1, 3) # A -> B with weight 3 g.add_edge(0, 2, 2) # A -> C with weight 2 g.add_edge(3, 0, 4) # D -> A with weight 4 g.add_edge(2, 1, 1) # C -> B with weight 1 g.print_graph()",
        "The edges, and edge weights, in a graph are normally implemented in an matrix.",
        "i",
        "j",
        "1",
        "(j,i)",
        "(i,j)",
        "0",
        "3",
        "(0,3)",
        "(3,0)",
        "weight",
        "add_edge()",
        "None"
      ]
    },
    {
      "title": "DSA Graphs Traversal",
      "summary": "Graphs Traversal\nTo traverse a Graph means to start in one vertex, and go along the edges to visit other vertices until all vertices, or as many as possible, have been visited.\nResult:\nUnderstanding how a Graph can be traversed is important for understanding how algorithms that run on Graphs work.\nThe two most common ways a Graph can be traversed are:\nDepth First Search (DFS)\nBreadth First Search (BFS)\nDFS is usually implemented using a Stack or by the use of recursion (which utilizes the call stack), while BFS is usually implemented using a Queue.\nThe Call Stack keeps functions running in the correct order.\nIf for example FunctionA calls FunctionB, FunctionB is placed on top of the call stack and starts running. Once FunctionB is finished, it is removed from the stack, and then FunctionA resumes its work.\nDepth First Search Traversal\nDepth First Search is said to go \"deep\" because it visits a vertex, then an adjacent vertex, and then that vertex' adjacent vertex, and so on, and in this way the distance from the starting vertex increases for each recursive iteration.\nHow it works:\nStart DFS traversal on a vertex.\nDo a recursive DFS traversal on each of the adjacent vertices as long as they are not already visited.\nRun the animation below to see how Depth First Search (DFS) traversal runs on a specific Graph, starting in vertex D (it is the same as the previous animation).\nResult:\nThe DFS traversal starts in vertex D, marks vertex D as visited. Then, for every new vertex visited, the traversal method is called recursively on all adjacent vertices that have not been visited yet. So when vertex A is visited in the animation above, vertex C or vertex E (depending on the implementation) is the next vertex where the traversal continues.\nExample\nPython:\nLine 60: The DFS traversal starts when the dfs() method is called.\nLine 33: The visited array is first set to false for all vertices, because no vertices are visited yet at this point.\nLine 35: The visited array is sent as an argument to the dfs_util() method. When the visited array is sent as an argument like this, it is actually just a reference to the visited array that is sent to the dfs_util() method, and not the actual array with the values inside. So there is always just one visited array in our program, and the dfs_util() method can make changes to it as nodes are visited (line 25).\nLine 28-30: For the current vertex v, all adjacent nodes are called recursively if they are not already visited.\nBreadth First Search Traversal\nBreadth First Search visits all adjacent vertices of a vertex before visiting neighboring vertices to the adjacent vertices. This means that vertices with the same distance from the starting vertex are visited before vertices further away from the starting vertex are visited.\nHow it works:\nPut the starting vertex into the queue.\nFor each vertex taken from the queue, visit the vertex, then put all unvisited adjacent vertices into the queue.\nContinue as long as there are vertices in the queue.\nRun the animation below to see how Breadth First Search (BFS) traversal runs on a specific Graph, starting in vertex D.\nResult:\nAs you can see in the animation above, BFS traversal visits vertices the same distance from the starting vertex, before visiting vertices further away. So for example, after visiting vertex A, vertex E and C are visited before visiting B, F and G because those vertices are further away.\nBreadth First Search traversal works this way by putting all adjacent vertices in a queue (if they are not already visited), and then using the queue to visit the next vertex.\nThis code example for Breadth First Search traversal is the same as for the Depth First Search code example above, except for the bfs() method:\nExample\nPython:\nLine 2-4: The bfs() method starts by creating a queue with the start vertex inside, creating a visited array, and setting the start vertex as visited.\nLine 6-13: The BFS traversal works by taking a vertex from the queue, printing it, and adding adjacent vertices to the queue if they are not visited yet, and then continue to take vertices from the queue in this way. The traversal finishes when the last element in the queue has no unvisited adjacent vertices.\nDFS and BFS Traversal of a Directed Graph\nDepth first and breadth first traversals can actually be implemented to work on directed Graphs (instead of undirected) with just very few changes.\nRun the animation below to see how a directed Graph can be traversed using DFS or BFS.\nResult:\nTo go from traversing a directed Graph instead of an undirected Graph, we just need to remove the last line in the add_edge() method:\nWe must also take care when we build our Graph because the edges are now directed.\nThe code example below contains both BFS and DFS traversal of the directed Graph from the animation above:\nExample\nPython:\nNow that we have looked at two basic algorithms for how to traverse Graphs, we will use the next pages to see how other algorithms can run on the Graph data structure.",
      "examples": [
        "class Graph: def __init__(self, size): self.adj_matrix = [[0] * size for _ in range(size)] self.size = size self.vertex_data = [''] * size def add_edge(self, u, v): if 0 <= u < self.size and 0 <= v < self.size: self.adj_matrix[u][v] = 1 self.adj_matrix[v][u] = 1 def add_vertex_data(self, vertex, data): if 0 <= vertex < self.size: self.vertex_data[vertex] = data def print_graph(self): print(\"Adjacency Matrix:\") for row in self.adj_matrix: print(' '.join(map(str, row))) print(\"\\nVertex Data:\") for vertex, data in enumerate(self.vertex_data): print(f\"Vertex {vertex}: {data}\") def dfs_util(self, v, visited): visited[v] = True print(self.vertex_data[v], end=' ') for i in range(self.size): if self.adj_matrix[v][i] == 1 and not visited[i]: self.dfs_util(i, visited) def dfs(self, start_vertex_data): visited = [False] * self.size start_vertex = self.vertex_data.index(start_vertex_data) self.dfs_util(start_vertex, visited) g = Graph(7) g.add_vertex_data(0, 'A') g.add_vertex_data(1, 'B') g.add_vertex_data(2, 'C') g.add_vertex_data(3, 'D') g.add_vertex_data(4, 'E') g.add_vertex_data(5, 'F') g.add_vertex_data(6, 'G') g.add_edge(3, 0) # D - A g.add_edge(0, 2) # A - C g.add_edge(0, 3) # A - D g.add_edge(0, 4) # A - E g.add_edge(4, 2) # E - C g.add_edge(2, 5) # C - F g.add_edge(2, 1) # C - B g.add_edge(2, 6) # C - G g.add_edge(1, 5) # B - F g.print_graph() print(\"\\nDepth First Search starting from vertex D:\") g.dfs('D')",
        "def bfs(self, start_vertex_data): queue = [self.vertex_data.index(start_vertex_data)] visited = [False] * self.size visited[queue[0]] = True while queue: current_vertex = queue.pop(0) print(self.vertex_data[current_vertex], end=' ') for i in range(self.size): if self.adj_matrix[current_vertex][i] == 1 and not visited[i]: queue.append(i) visited[i] = True",
        "def add_edge(self, u, v): if 0 <= u < self.size and 0 <= v < self.size: self.adj_matrix[u][v] = 1\nself.adj_matrix[v][u] = 1",
        "class Graph: def __init__(self, size): self.adj_matrix = [[0] * size for _ in range(size)] self.size = size self.vertex_data = [''] * size def add_edge(self, u, v): if 0 <= u < self.size and 0 <= v < self.size: self.adj_matrix[u][v] = 1 #self.adj_matrix[v][u] = 1 def add_vertex_data(self, vertex, data): if 0 <= vertex < self.size: self.vertex_data[vertex] = data def print_graph(self): print(\"Adjacency Matrix:\") for row in self.adj_matrix: print(' '.join(map(str, row))) print(\"\\nVertex Data:\") for vertex, data in enumerate(self.vertex_data): print(f\"Vertex {vertex}: {data}\") def dfs_util(self, v, visited): visited[v] = True print(self.vertex_data[v], end=' ') for i in range(self.size): if self.adj_matrix[v][i] == 1 and not visited[i]: self.dfs_util(i, visited) def dfs(self, start_vertex_data): visited = [False] * self.size start_vertex = self.vertex_data.index(start_vertex_data) self.dfs_util(start_vertex, visited) def bfs(self, start_vertex_data): queue = [self.vertex_data.index(start_vertex_data)] visited = [False] * self.size visited[queue[0]] = True while queue: current_vertex = queue.pop(0) print(self.vertex_data[current_vertex], end=' ') for i in range(self.size): if self.adj_matrix[current_vertex][i] == 1 and not visited[i]: queue.append(i) visited[i] = True g = Graph(7) g.add_vertex_data(0, 'A') g.add_vertex_data(1, 'B') g.add_vertex_data(2, 'C') g.add_vertex_data(3, 'D') g.add_vertex_data(4, 'E') g.add_vertex_data(5, 'F') g.add_vertex_data(6, 'G') g.add_edge(3, 0) # D -> A g.add_edge(3, 4) # D -> E g.add_edge(4, 0) # E -> A g.add_edge(0, 2) # A -> C g.add_edge(2, 5) # C -> F g.add_edge(2, 6) # C -> G g.add_edge(5, 1) # F -> B g.add_edge(1, 2) # B -> C g.print_graph() print(\"\\nDepth First Search starting from vertex D:\") g.dfs('D') print(\"\\n\\nBreadth First Search starting from vertex D:\") g.bfs('D')",
        "dfs()",
        "visited",
        "false",
        "dfs_util()",
        "v",
        "bfs()",
        "add_edge()"
      ]
    },
    {
      "title": "DSA Graphs Cycle Detection",
      "summary": "Cycles in Graphs\nA cycle in a Graph is a path that starts and ends at the same vertex, where no edges are repeated. It is similar to walking through a maze and ending up exactly where you started.\nIs cyclic:\nA cycle can be defined slightly different depending on the situation. A self-loop for example, where an edge goes from and to the same vertex, might or might not be considered a cycle, depending on the problem you are trying to solve.\nCycle Detection\nIt is important to be able to detect cycles in Graphs because cycles can indicate problems or special conditions in many applications like networking, scheduling, and circuit design.\nThe two most common ways to detect cycles are:\nDepth First Search (DFS): DFS traversal explores the Graph and marks vertices as visited. A cycle is detected when the current vertex has an adjacent vertex that has already been visited.\nUnion-Find: This works by initially defining each vertex as a group, or a subset. Then these groups are joined for every edge. Whenever a new edge is explored, a cycle is detected if two vertices already belong to the same group.\nHow cycle detection with DFS and Union-Find work, and how they are implemented, are explained in more detail below.\nDFS Cycle Detection for Undirected Graphs\nTo detect cycles in an undirected Graph using Depth First Search (DFS), we use a code very similar to the DFS traversal code on the previous page, with just a few changes.\nHow it works:\nStart DFS traversal on each unvisited vertex (in case the Graph is not connected).\nDuring DFS, mark vertices as visited, and run DFS on the adjacent vertices (recursively).\nIf an adjacent vertex is already visited and is not the parent of the current vertex, a cycle is detected, and True is returned.\nIf DFS traversal is done on all vertices and no cycles are detected, False is returned.\nRun the animation below to see how DFS cycle detection runs on a specific Graph, starting in vertex A (this is the same as the previous animation).\nIs cyclic:\nThe DFS traversal starts in vertex A because that is the first vertex in the adjacency matrix. Then, for every new vertex visited, the traversal method is called recursively on all adjacent vertices that have not been visited yet. The cycle is detected when vertex F is visited, and it is discovered that the adjacent vertex C has already been visited.\nExample\nPython:\nLine 66: The DFS cycle detection starts when the is_cyclic() method is called.\nLine 37: The visited array is first set to false for all vertices, because no vertices are visited yet at this point.\nLine 38-42: DFS cycle detection is run on all vertices in the Graph. This is to make sure all vertices are visited in case the Graph is not connected. If a node is already visited, there must be a cycle, and True is returned. If all nodes are visited just ones, which means no cycles are detected, False is returned.\nLine 24-34: This is the part of the DFS cycle detection that visits a vertex, and then visits adjacent vertices recursively. A cycle is detected and True is returned if an adjacent vertex has already been visited, and it is not the parent node.\nDFS Cycle Detection for Directed Graphs\nTo detect cycles in Graphs that are directed, the algorithm is still very similar as for undirected Graphs, but the code must be modified a little bit because for a directed Graph, if we come to an adjacent node that has already been visited, it does not necessarily mean that there is a cycle.\nJust consider the following Graph where two paths are explored, trying to detect a cycle:\nIn path 1, the first path to be explored, vertices A->B->C are visited, no cycles detected.\nIn the second path to be explored (path 2), vertices D->B->C are visited, and the path has no cycles, right? But without changes in our program, a false cycle would actually be detected when going from D to the adjacent vertex B, because B has already been visited in path 1. To avoid such false detections, the code is modified to detect cycles only in case a node has been visited before in the same path.\nIs cyclic:\nTo implement DFS cycle detection on a directed Graph, like in the animation above, we need to remove the symmetry we have in the adjacency matrix for undirected Graphs. We also need to use a recStack array to keep track of visited vertices in the current recursive path.\nExample\nPython:\nLine 6: This line is removed because it is only applicable for undirected Graphs.\nLine 26: The recStack array keeps an overview over which vertices have been visited during a recursive exploration of a path.\nLine 14-19: For every adjacent vertex not visited before, do a recursive DFS cycle detection. If an adjacent vertex has been visited before, also in the same recursive path (line 13), a cycle has been found, and True is returned.\nUnion-Find Cycle Detection\nDetecting cycles using Union-Find is very different from using Depth First Search.\nUnion-Find cycle detection works by first putting each node in its own subset (like a bag or container). Then, for every edge, the subsets belonging to each vertex are merged. For an edge, if the vertices already belong to the same subset, it means that we have found a cycle.\nIs cyclic:\nIn the animation above, Union-Find cycle detection explores the edges in the Graph. As edges are explored, the subset of vertex A grows to also include vertices B, C, and D. The cycle is detected when the edge between A and D is explored, and it is discovered that both A and D already belong to the same subset.\nThe edges between D, E, and F also construct a circle, but this circle is not detected because the algorithm stops (returns True) when the first circle is detected.\nUnion-Find cycle detection is only applicable for Graphs that are undirected.\nUnion-Find cycle detection is implemented using the adjacency matrix representation, so setting up the Graph structure with vertices and edges is basically the same as in previous examples.\nExample\nPython:\nLine 6: The parent array contains the root vertex for every subset. This is used to detect a cycle by checking if two vertices on either side of an edge already belong to the same subset.\nLine 17: The find method finds the root of the set that the given vertex belongs to.\nLine 22: The union method combines two subsets.\nLine 29: The is_cyclic method uses the find method to detect a cycle if two vertices x and y are already in the same subset. If a cycle is not detected, the union method is used to combine the subsets.\nDSA Exercises\nTest Yourself With Exercises\nExercise:\nWhat is a cycle in a Graph?\nStart the Exercise",
      "examples": [
        "class Graph: def __init__(self, size): self.adj_matrix = [[0] * size for _ in range(size)] self.size = size self.vertex_data = [''] * size def add_edge(self, u, v): if 0 <= u < self.size and 0 <= v < self.size: self.adj_matrix[u][v] = 1 self.adj_matrix[v][u] = 1 def add_vertex_data(self, vertex, data): if 0 <= vertex < self.size: self.vertex_data[vertex] = data def print_graph(self): print(\"Adjacency Matrix:\") for row in self.adj_matrix: print(' '.join(map(str, row))) print(\"\\nVertex Data:\") for vertex, data in enumerate(self.vertex_data): print(f\"Vertex {vertex}: {data}\") def dfs_util(self, v, visited, parent): visited[v] = True for i in range(self.size): if self.adj_matrix[v][i] == 1: if not visited[i]: if self.dfs_util(i, visited, v): return True elif parent != i: return True return False def is_cyclic(self): visited = [False] * self.size for i in range(self.size): if not visited[i]: if self.dfs_util(i, visited, -1): return True return False g = Graph(7) g.add_vertex_data(0, 'A') g.add_vertex_data(1, 'B') g.add_vertex_data(2, 'C') g.add_vertex_data(3, 'D') g.add_vertex_data(4, 'E') g.add_vertex_data(5, 'F') g.add_vertex_data(6, 'G') g.add_edge(3, 0) # D - A g.add_edge(0, 2) # A - C g.add_edge(0, 3) # A - D g.add_edge(0, 4) # A - E g.add_edge(4, 2) # E - C g.add_edge(2, 5) # C - F g.add_edge(2, 1) # C - B g.add_edge(2, 6) # C - G g.add_edge(1, 5) # B - F g.print_graph() print(\"\\nGraph has cycle:\", g.is_cyclic())",
        "class Graph: # ...... def add_edge(self, u, v): if 0 <= u < self.size and 0 <= v < self.size: self.adj_matrix[u][v] = 1\nself.adj_matrix[v][u] = 1\n# ...... def dfs_util(self, v, visited, recStack): visited[v] = True recStack[v] = True print(\"Current vertex:\",self.vertex_data[v]) for i in range(self.size): if self.adj_matrix[v][i] == 1: if not visited[i]: if self.dfs_util(i, visited, recStack): return True elif recStack[i]: return True recStack[v] = False return False def is_cyclic(self): visited = [False] * self.size recStack = [False] * self.size for i in range(self.size): if not visited[i]: print() #new line if self.dfs_util(i, visited, recStack): return True return False g = Graph(7) # ...... g.add_edge(3, 0) # D -> A g.add_edge(0, 2) # A -> C g.add_edge(2, 1) # C -> B g.add_edge(2, 4) # C -> E g.add_edge(1, 5) # B -> F g.add_edge(4, 0) # E -> A g.add_edge(2, 6) # C -> G g.print_graph() print(\"Graph has cycle:\", g.is_cyclic())",
        "class Graph: def __init__(self, size): self.adj_matrix = [[0] * size for _ in range(size)] self.size = size self.vertex_data = [''] * size self.parent = [i for i in range(size)] # Union-Find array def add_edge(self, u, v): if 0 <= u < self.size and 0 <= v < self.size: self.adj_matrix[u][v] = 1 self.adj_matrix[v][u] = 1 def add_vertex_data(self, vertex, data): if 0 <= vertex < self.size: self.vertex_data[vertex] = data def find(self, i): if self.parent[i] == i: return i return self.find(self.parent[i]) def union(self, x, y): x_root = self.find(x) y_root = self.find(y) print('Union:',self.vertex_data[x],'+',self.vertex_data[y]) self.parent[x_root] = y_root print(self.parent,'\\n') def is_cyclic(self): for i in range(self.size): for j in range(i + 1, self.size): if self.adj_matrix[i][j]: x = self.find(i) y = self.find(j) if x == y: return True self.union(x, y) return False g = Graph(7) g.add_vertex_data(0, 'A') g.add_vertex_data(1, 'B') g.add_vertex_data(2, 'C') g.add_vertex_data(3, 'D') g.add_vertex_data(4, 'E') g.add_vertex_data(5, 'F') g.add_vertex_data(6, 'G') g.add_edge(1, 0) # B - A g.add_edge(0, 3) # A - D g.add_edge(0, 2) # A - C g.add_edge(2, 3) # C - D g.add_edge(3, 4) # D - E g.add_edge(3, 5) # D - F g.add_edge(3, 6) # D - G g.add_edge(4, 5) # E - F print(\"Graph has cycle:\", g.is_cyclic())",
        "A cycle in a Graph is a path that starts and ends at the same , where no are repeated.",
        "True",
        "False",
        "is_cyclic()",
        "visited",
        "false",
        "recStack",
        "parent",
        "find",
        "union",
        "is_cyclic",
        "x",
        "y"
      ]
    },
    {
      "title": "DSA Shortest Path",
      "summary": "The Shortest Path Problem\nThe shortest path problem is famous in the field of computer science.\nTo solve the shortest path problem means to find the shortest possible route or path between two vertices (or nodes) in a Graph.\nIn the shortest path problem, a Graph can represent anything from a road network to a communication network, where the vertices can be intersections, cities, or routers, and the edges can be roads, flight paths, or data links.\nThe shortest path from vertex D to vertex F in the Graph above is D->E->C->F, with a total path weight of 2+4+4=10. Other paths from D to F are also possible, but they have a higher total weight, so they can not be considered to be the shortest path.\nSolutions to The Shortest Path Problem\nDijkstra's algorithm and the Bellman-Ford algorithm find the shortest path from one start vertex, to all other vertices.\nTo solve the shortest path problem means to check the edges inside the Graph until we find a path where we can move from one vertex to another using the lowest possible combined weight along the edges.\nThis sum of weights along the edges that make up a path is called a path cost or a path weight.\nAlgorithms that find the shortest paths, like Dijkstra's algorithm or the Bellman-Ford algorithm, find the shortest paths from one start vertex to all other vertices.\nTo begin with, the algorithms set the distance from the start vertex to all vertices to be infinitely long. And as the algorithms run, edges between the vertices are checked over and over, and shorter paths might be found many times until the shortest paths are found at the end.\nEvery time an edge is checked and it leads to a shorter distance to a vertex being found and updated, it is called a relaxation, or relaxing an edge.\nPositive and Negative Edge Weights\nSome algorithms that find the shortest paths, like Dijkstra's algorithm, can only find the shortest paths in graphs where all the edges are positive. Such graphs with positive distances are also the easiest to understand because we can think of the edges between vertices as distances between locations.\nIf we interpret the edge weights as money lost by going from one vertex to another, a positive edge weight of 4 from vertex A to C in the graph above means that we must spend $4 to go from A to C.\nBut graphs can also have negative edges, and for such graphs the Bellman-Ford algorithm can be used to find the shortest paths.\nAnd similarly, if the edge weights represent money lost, the negative edge weight -3 from vertex C to A in the graph above can be understood as an edge where there is more money to be made than money lost by going from C to A. So if for example the cost of fuel is $5 going from C to A, and we get paid $8 for picking up packages in C and delivering them in A, money lost is -3, meaning we are actually earning $3 in total.\nNegative Cycles in Shortest Path Problems\nFinding the shortest paths becomes impossible if a graph has negative cycles.\nHaving a negative cycle means that there is a path where you can go in circles, and the edges that make up this circle have a total path weight that is negative.\nIn the graph below, the path A->E->B->C->A is a negative cycle because the total path weight is 5+2-4-4=-1.\nThe reason why it is impossible to find the shortest paths in a graph with negative cycles is that it will always be possible to continue running an algorithm to find even shorter paths.\nLet's say for example that we are looking for the shortest distance from vertex D in graph above, to all other vertices. At first we find the distance from D to E to be 3, by just walking the edge D->E. But after this, if we walk one round in the negative cycle E->B->C->A->E, then the distance to E becomes 2. After walking one more round the distance becomes 1, which is even shorter, and so on. We can always walk one more round in the negative cycle to find a shorter distance to E, which means the shortest distance can never be found.\nLuckily, the the Bellman-Ford algorithm, that runs on graphs with negative edges, can be implemented with detection for negative cycles.",
      "examples": []
    },
    {
      "title": "DSA Dijkstra's Algorithm",
      "summary": "Dijkstra's shortest path algorithm was invented in 1956 by the Dutch computer scientist Edsger W. Dijkstra during a twenty minutes coffee break, while out shopping with his fiancée in Amsterdam.\nThe reason for inventing the algorithm was to test a new computer called ARMAC.\nDijkstra's Algorithm\nDijkstra's algorithm finds the shortest path from one vertex to all other vertices.\nIt does so by repeatedly selecting the nearest unvisited vertex and calculating the distance to all the unvisited neighboring vertices.\nDijkstra's algorithm is often considered to be the most straightforward algorithm for solving the shortest path problem.\nDijkstra's algorithm is used for solving single-source shortest path problems for directed or undirected paths. Single-source means that one vertex is chosen to be the start, and the algorithm will find the shortest path from that vertex to all other vertices.\nDijkstra's algorithm does not work for graphs with negative edges. For graphs with negative edges, the Bellman-Ford algorithm that is described on the next page, can be used instead.\nTo find the shortest path, Dijkstra's algorithm needs to know which vertex is the source, it needs a way to mark vertices as visited, and it needs an overview of the current shortest distance to each vertex as it works its way through the graph, updating these distances when a shorter distance is found.\nHow it works:\nSet initial distances for all vertices: 0 for the source vertex, and infinity for all the other.\nChoose the unvisited vertex with the shortest distance from the start to be the current vertex. So the algorithm will always start with the source as the current vertex.\nFor each of the current vertex's unvisited neighbor vertices, calculate the distance from the source and update the distance if the new, calculated, distance is lower.\nWe are now done with the current vertex, so we mark it as visited. A visited vertex is not checked again.\nGo back to step 2 to choose a new current vertex, and keep repeating these steps until all vertices are visited.\nIn the end we are left with the shortest path from the source vertex to every other vertex in the graph.\nIn the animation above, when a vertex is marked as visited, the vertex and its edges become faded to indicate that Dijkstra's algorithm is now done with that vertex, and will not visit it again.\nNote: This basic version of Dijkstra's algorithm gives us the value of the shortest path cost to every vertex, but not what the actual path is. So for example, in the animation above, we get the shortest path cost value 10 to vertex F, but the algorithm does not give us which vertices (D->E->C->D->F) that make up this shortest path. We will add this functionality further down here on this page.\nA Detailed Dijkstra Simulation\nRun the simulation below to get a more detailed understanding of how Dijkstra's algorithm runs on a specific graph, finding the shortest distances from vertex D.\nThis simulation shows how distances are calculated from vertex D to all other vertices, by always choosing the next vertex to be the closest unvisited vertex from the starting point.\nFollow the step-by-step description below to get all the details of how Dijkstra's algorithm calculates the shortest distances.\nManual Run Through\nConsider the Graph below.\nWe want to find the shortest path from the source vertex D to all other vertices, so that for example the shortest path to C is D->E->C, with path weight 2+4=6.\nTo find the shortest path, Dijkstra's algorithm uses an array with the distances to all other vertices, and initially sets these distances to infinite, or a very big number. And the distance to the vertex we start from (the source) is set to 0.\nThe image below shows the initial infinite distances to other vertices from the starting vertex D. The distance value for vertex D is 0 because that is the starting point.\nDijkstra's algorithm then sets vertex D as the current vertex, and looks at the distance to the adjacent vertices. Since the initial distance to vertices A and E is infinite, the new distance to these are updated with the edge weights. So vertex A gets the distance changed from inf to 4, and vertex E gets the distance changed to 2. As mentioned on the previous page, updating the distance values in this way is called 'relaxing'.\nAfter relaxing vertices A and E, vertex D is considered visited, and will not be visited again.\nThe next vertex to be chosen as the current vertex must the vertex with the shortest distance to the source vertex (vertex D), among the previously unvisited vertices. Vertex E is therefore chosen as the current vertex after vertex D.\nThe distance to all adjacent and not previously visited vertices from vertex E must now be calculated, and updated if needed.\nThe calculated distance from D to vertex A, via E, is 2+4=6. But the current distance to vertex A is already 4, which is lower, so the distance to vertex A is not updated.\nThe distance to vertex C is calculated to be 2+4=6, which is less than infinity, so the distance to vertex C is updated.\nSimilarly, the distance to node G is calculated and updated to be 2+5=7.\nThe next vertex to be visited is vertex A because it has the shortest distance from D of all the unvisited vertices.\nThe calculated distance to vertex C, via A, is 4+3=7, which is higher than the already set distance to vertex C, so the distance to vertex C is not updated.\nVertex A is now marked as visited, and the next current vertex is vertex C because that has the lowest distance from vertex D between the remaining unvisited vertices.\nVertex F gets updated distance 6+5=11, and vertex B gets updated distance 6+2=8.\nCalculated distance to vertex G via vertex C is 6+5=11 which is higher than the already set distance of 7, so distance to vertex G is not updated.\nVertex C is marked as visited, and the next vertex to be visited is G because is has the lowest distance between the remaining unvisited vertices.\nVertex F already has a distance of 11. This is lower than the calculated distance from G, which is 7+5=12, so the distance to vertex F is not updated.\nVertex G is marked as visited, and B becomes the current vertex because it has the lowest distance of the remaining unvisited vertices.\nThe new distance to F via B is 8+2=10, because it is lower than F's existing distance of 11.\nVertex B is marked as visited, and there is nothing to check for the last unvisited vertex F, so Dijkstra's algorithm is finished.\nEvery vertex has been visited only once, and the result is the lowest distance from the source vertex D to every other vertex in the graph.\nImplementation of Dijkstra's Algorithm\nTo implement Dijkstra's algorithm, we create a Graph class. The Graph represents the graph with its vertices and edges:\nLine 3: We create the adj_matrix to hold all the edges and edge weights. Initial values are set to 0.\nLine 4: size is the number of vertices in the graph.\nLine 5: The vertex_data holds the names of all the vertices.\nLine 7-10: The add_edge method is used to add an edge from vertex u to vertex v, with edge weight weight.\nLine 12-14: The add_vertex_data method is used to add a vertex to the graph. The index where the vertex should belong is given with the vertex argument, and data is the name of the vertex.\nThe Graph class also contains the method that runs Dijkstra's algorithm:\nLine 18-19: The initial distance is set to infinity for all vertices in the distances array, except for the start vertex, where the distance is 0.\nLine 20: All vertices are initially set to False to mark them as not visited in the visited array.\nLine 23-28: The next current vertex is found. Outgoing edges from this vertex will be checked to see if shorter distances can be found. It is the unvisited vertex with the lowest distance from the start.\nLine 30-31: If the next current vertex has not been found, the algorithm is finished. This means that all vertices that are reachable from the source have been visited.\nLine 33: The current vertex is set as visited before relaxing adjacent vertices. This is more effective because we avoid checking the distance to the current vertex itself.\nLine 35-39: Distances are calculated for not visited adjacent vertices, and updated if the new calculated distance is lower.\nAfter defining the Graph class, the vertices and edges must be defined to initialize the specific graph, and the complete code for this Dijkstra's algorithm example looks like this:\nExample\nPython:\nDijkstra's Algorithm on Directed Graphs\nTo run Dijkstra's algorithm on directed graphs, very few changes are needed.\nSimilarly to the change we needed for cycle detection for directed graphs, we just need to remove one line of code so that the adjacency matrix is not symmetric anymore.\nLet's implement this directed graph and run Dijkstra's algorithm from vertex D.\nHere is the implementation of Dijkstra's algorithm on the directed graph, with D as the source vertex:\nExample\nPython:\nThe image below shows us the shortest distances from vertex D as calculated by Dijkstra's algorithm.\nThis result is similar to the previous example using Dijkstra's algorithm on the undirected graph. However, there's a key difference: in this case, vertex B cannot be visited from D, and this means that the shortest distance from D to F is now 11, not 10, because the path can no longer go through vertex B.\nReturning The Paths from Dijkstra's Algorithm\nWith a few adjustments, the actual shortest paths can also be returned by Dijkstra's algorithm, in addition to the shortest path values. So for example, instead of just returning that the shortest path value is 10 from vertex D to F, the algorithm can also return that the shortest path is \"D->E->C->B->F\".\nTo return the path, we create a predecessors array to keep the previous vertex in the shortest path for each vertex. The predecessors array can be used to backtrack to find the shortest path for every vertex.\nExample\nPython:\nLine 7 and 29: The predecessors array is first initialized with None values, then it is updated with the correct predecessor for each vertex as the shortest path values are updated.\nLine 33-42: The get_path method uses the predecessors array and returns a string with the shortest path from start to end vertex.\nDijkstra's Algorithm with a Single Destination Vertex\nLet's say we are only interested in finding the shortest path between two vertices, like finding the shortest distance between vertex D and vertex F in the graph below.\nDijkstra's algorithm is normally used for finding the shortest path from one source vertex to all other vertices in the graph, but it can also be modified to only find the shortest path from the source to a single destination vertex, by just stopping the algorithm when the destination is reached (visited).\nThis means that for the specific graph in the image above, Dijkstra's algorithm will stop after visiting F (the destination vertex), before visiting vertices H, I and J because they are farther away from D than F is.\nBelow we can see the status of the calculated distances when Dijkstra's algorithm has found the shortest distance from D to F, and stops running.\nIn the image above, vertex F has just got updated with distance 10 from vertex B. Since F is the unvisited vertex with the lowest distance from D, it would normally be the next current vertex, but since it is the destination, the algorithm stops. If the algorithm did not stop, J would be the next vertex to get an updated distance 11+2=13, from vertex I.\nThe code below is Dijkstra's algorithm implemented to find the shortest path to a single destination vertex:\nExample\nPython:\nLine 20-23: If we are about to choose the destination vertex as the current vertex and mark it as visited, it means we have already calculated the shortest distance to the destination vertex, and Dijkstra's algorithm can be stopped in this single destination case.\nTime Complexity for Dijkstra's Algorithm\nWith VV as the number of vertices in our graph, the time complexity for Dijkstra's algorithm is\nO(V2) O( V^2 )\nThe reason why we get this time complexity is that the vertex with the lowest distance must to be search for to choose the next current vertex, and that takes O(V)O(V) time. And since this must to be done for every vertex connected to the source, we need to factor that in, and so we get time complexity O(V2)O(V^2) for Dijkstra's algorithm.\nBy using a Min-heap or Fibonacci-heap data structure for the distances instead (not yet explained in this tutorial), the time needed to search for the minimum distance vertex is reduced from O(V)O(V) to O(logV)O( \\log{V}), which results in an improved time complexity for Dijkstra's algorithm\nO(V⋅logV+E) O( V \\cdot \\log{V} + E )\nWhere VV is the number of vertices in the graph, and EE is the number of edges.\nThe improvement we get from using a Min-heap data structure for Dijkstra's algorithm is especially good if we have a large and sparse graph, which means a graph with a large number of vertices, but not as many edges.\nThe implementation of Dijkstra's algorithm with the Fibonacci-heap data structure is better for dense graphs, where each vertex has an edge to almost every other vertex.\nDSA Exercises\nTest Yourself With Exercises\nExercise:\nUsing Dijkstra's algorithm to find the shortest paths from vertex C in this graph:\nWhat is the next vertex to be visited after C is visited?\nStart the Exercise",
      "examples": [
        "distances = [inf, inf, inf, 0, inf, inf, inf] #vertices [ A , B , C , D, E , F , G ]",
        "class Graph: def __init__(self, size): self.adj_matrix = [[0] * size for _ in range(size)] self.size = size self.vertex_data = [''] * size def add_edge(self, u, v, weight): if 0 <= u < self.size and 0 <= v < self.size: self.adj_matrix[u][v] = weight self.adj_matrix[v][u] = weight # For undirected graph def add_vertex_data(self, vertex, data): if 0 <= vertex < self.size: self.vertex_data[vertex] = data",
        "def dijkstra(self, start_vertex_data): start_vertex = self.vertex_data.index(start_vertex_data) distances = [float('inf')] * self.size distances[start_vertex] = 0 visited = [False] * self.size for _ in range(self.size): min_distance = float('inf') u = None for i in range(self.size): if not visited[i] and distances[i] < min_distance: min_distance = distances[i] u = i if u is None: break visited[u] = True for v in range(self.size): if self.adj_matrix[u][v] != 0 and not visited[v]: alt = distances[u] + self.adj_matrix[u][v] if alt < distances[v]: distances[v] = alt return distances",
        "class Graph: def __init__(self, size): self.adj_matrix = [[0] * size for _ in range(size)] self.size = size self.vertex_data = [''] * size def add_edge(self, u, v, weight): if 0 <= u < self.size and 0 <= v < self.size: self.adj_matrix[u][v] = weight self.adj_matrix[v][u] = weight # For undirected graph def add_vertex_data(self, vertex, data): if 0 <= vertex < self.size: self.vertex_data[vertex] = data def dijkstra(self, start_vertex_data): start_vertex = self.vertex_data.index(start_vertex_data) distances = [float('inf')] * self.size distances[start_vertex] = 0 visited = [False] * self.size for _ in range(self.size): min_distance = float('inf') u = None for i in range(self.size): if not visited[i] and distances[i] < min_distance: min_distance = distances[i] u = i if u is None: break visited[u] = True for v in range(self.size): if self.adj_matrix[u][v] != 0 and not visited[v]: alt = distances[u] + self.adj_matrix[u][v] if alt < distances[v]: distances[v] = alt return distances g = Graph(7) g.add_vertex_data(0, 'A') g.add_vertex_data(1, 'B') g.add_vertex_data(2, 'C') g.add_vertex_data(3, 'D') g.add_vertex_data(4, 'E') g.add_vertex_data(5, 'F') g.add_vertex_data(6, 'G') g.add_edge(3, 0, 4) # D - A, weight 5 g.add_edge(3, 4, 2) # D - E, weight 2 g.add_edge(0, 2, 3) # A - C, weight 3 g.add_edge(0, 4, 4) # A - E, weight 4 g.add_edge(4, 2, 4) # E - C, weight 4 g.add_edge(4, 6, 5) # E - G, weight 5 g.add_edge(2, 5, 5) # C - F, weight 5 g.add_edge(2, 1, 2) # C - B, weight 2 g.add_edge(1, 5, 2) # B - F, weight 2 g.add_edge(6, 5, 5) # G - F, weight 5 # Dijkstra's algorithm from D to all vertices print(\"\\nDijkstra's Algorithm starting from vertex D:\") distances = g.dijkstra('D') for i, d in enumerate(distances): print(f\"Distance from D to {g.vertex_data[i]}: {d}\")",
        "class Graph: def __init__(self, size): self.adj_matrix = [[0] * size for _ in range(size)] self.size = size self.vertex_data = [''] * size def add_edge(self, u, v, weight): if 0 <= u < self.size and 0 <= v < self.size: self.adj_matrix[u][v] = weight #self.adj_matrix[v][u] = weight For undirected graph def add_vertex_data(self, vertex, data): if 0 <= vertex < self.size: self.vertex_data[vertex] = data def dijkstra(self, start_vertex_data): start_vertex = self.vertex_data.index(start_vertex_data) distances = [float('inf')] * self.size distances[start_vertex] = 0 visited = [False] * self.size for _ in range(self.size): min_distance = float('inf') u = None for i in range(self.size): if not visited[i] and distances[i] < min_distance: min_distance = distances[i] u = i if u is None: break visited[u] = True for v in range(self.size): if self.adj_matrix[u][v] != 0 and not visited[v]: alt = distances[u] + self.adj_matrix[u][v] if alt < distances[v]: distances[v] = alt return distances g = Graph(7) g.add_vertex_data(0, 'A') g.add_vertex_data(1, 'B') g.add_vertex_data(2, 'C') g.add_vertex_data(3, 'D') g.add_vertex_data(4, 'E') g.add_vertex_data(5, 'F') g.add_vertex_data(6, 'G') g.add_edge(3, 0, 4) # D -> A, weight 5 g.add_edge(3, 4, 2) # D -> E, weight 2 g.add_edge(0, 2, 3) # A -> C, weight 3 g.add_edge(0, 4, 4) # A -> E, weight 4 g.add_edge(4, 2, 4) # E -> C, weight 4 g.add_edge(4, 6, 5) # E -> G, weight 5 g.add_edge(2, 5, 5) # C -> F, weight 5 g.add_edge(1, 2, 2) # B -> C, weight 2 g.add_edge(1, 5, 2) # B -> F, weight 2 g.add_edge(6, 5, 5) # G -> F, weight 5 # Dijkstra's algorithm from D to all vertices print(\"Dijkstra's Algorithm starting from vertex D:\\n\") distances = g.dijkstra('D') for i, d in enumerate(distances): print(f\"Shortest distance from D to {g.vertex_data[i]}: {d}\")",
        "class Graph: # ... (rest of the Graph class) def dijkstra(self, start_vertex_data): start_vertex = self.vertex_data.index(start_vertex_data) distances = [float('inf')] * self.size predecessors = [None] * self.size distances[start_vertex] = 0 visited = [False] * self.size for _ in range(self.size): min_distance = float('inf') u = None for i in range(self.size): if not visited[i] and distances[i] < min_distance: min_distance = distances[i] u = i if u is None: break visited[u] = True for v in range(self.size): if self.adj_matrix[u][v] != 0 and not visited[v]: alt = distances[u] + self.adj_matrix[u][v] if alt < distances[v]: distances[v] = alt predecessors[v] = u return distances, predecessors def get_path(self, predecessors, start_vertex, end_vertex): path = [] current = self.vertex_data.index(end_vertex) while current is not None: path.insert(0, self.vertex_data[current]) current = predecessors[current] if current == self.vertex_data.index(start_vertex): path.insert(0, start_vertex) break return '->'.join(path) # Join the vertices with '->' g = Graph(7) # ... (rest of the graph setup) # Dijkstra's algorithm from D to all vertices print(\"Dijkstra's Algorithm starting from vertex D:\\n\") distances, predecessors = g.dijkstra('D') for i, d in enumerate(distances): path = g.get_path(predecessors, 'D', g.vertex_data[i]) print(f\"{path}, Distance: {d}\")",
        "class Graph: # ... (existing methods) def dijkstra(self, start_vertex_data, end_vertex_data): start_vertex = self.vertex_data.index(start_vertex_data) end_vertex = self.vertex_data.index(end_vertex_data) distances = [float('inf')] * self.size predecessors = [None] * self.size distances[start_vertex] = 0 visited = [False] * self.size for _ in range(self.size): min_distance = float('inf') u = None for i in range(self.size): if not visited[i] and distances[i] < min_distance: min_distance = distances[i] u = i if u is None or u == end_vertex: print(f\"Breaking out of loop. Current vertex: {self.vertex_data[u]}\") print(f\"Distances: {distances}\") break visited[u] = True print(f\"Visited vertex: {self.vertex_data[u]}\") for v in range(self.size): if self.adj_matrix[u][v] != 0 and not visited[v]: alt = distances[u] + self.adj_matrix[u][v] if alt < distances[v]: distances[v] = alt predecessors[v] = u return distances[end_vertex], self.get_path(predecessors, start_vertex_data, end_vertex_data) # Example usage g = Graph(7) # ... (rest of the graph setup) distance, path = g.dijkstra('D', 'F') print(f\"Path: {path}, Distance: {distance}\")",
        "Using Dijkstra's algorithm, the next vertex to be visited after vertex C is vertex .",
        "Graph",
        "adj_matrix",
        "0",
        "size",
        "vertex_data",
        "add_edge",
        "u",
        "v",
        "weight",
        "add_vertex_data",
        "vertex",
        "data",
        "distances",
        "False",
        "visited",
        "predecessors",
        "None",
        "get_path"
      ]
    },
    {
      "title": "DSA Bellman-Ford Algorithm",
      "summary": "The Bellman-Ford Algorithm\nThe Bellman-Ford algorithm is best suited to find the shortest paths in a directed graph, with one or more negative edge weights, from the source vertex to all other vertices.\nIt does so by repeatedly checking all the edges in the graph for shorter paths, as many times as there are vertices in the graph (minus 1).\nThe Bellman-Ford algorithm can also be used for graphs with positive edges (both directed and undirected), like we can with Dijkstra's algorithm, but Dijkstra's algorithm is preferred in such cases because it is faster.\nUsing the Bellman-Ford algorithm on a graph with negative cycles will not produce a result of shortest paths because in a negative cycle we can always go one more round and get a shorter path.\nA negative cycle is a path we can follow in circles, where the sum of the edge weights is negative.\nLuckily, the Bellman-Ford algorithm can be implemented to safely detect and report the presence of negative cycles.\nHow it works:\nSet initial distance to zero for the source vertex, and set initial distances to infinity for all other vertices.\nFor each edge, check if a shorter distance can be calculated, and update the distance if the calculated distance is shorter.\nCheck all edges (step 2) V−1V-1 times. This is as many times as there are vertices (VV), minus one.\nOptional: Check for negative cycles. This will be explained in better detail later.\nThe animation of the Bellman-Ford algorithm above only shows us when checking of an edge leads to an updated distance, not all the other edge checks that do not lead to updated distances.\nManual Run Through\nThe Bellman-Ford algorithm is actually quite straight forward, because it checks all edges, using the adjacency matrix. Each check is to see if a shorter distance can be made by going from the vertex on one side of the edge, via the edge, to the vertex on the other side of the edge.\nAnd this check of all edges is done V−1V - 1 times, with VV being the number of vertices in the graph.\nThis is how the Bellman-Ford algorithm checks all the edges in the adjacency matrix in our graph 5-1=4 times:\nChecked all edges 0 times.\nThe first four edges that are checked in our graph are A->C, A->E, B->C, and C->A. These first four edge checks do not lead to any updates of the shortest distances because the starting vertex of all these edges has an infinite distance.\nAfter the edges from vertices A, B, and C are checked, the edges from D are checked. Since the starting point (vertex D) has distance 0, the updated distances for A, B, and C are the edge weights going out from vertex D.\nThe next edges to be checked are the edges going out from vertex E, which leads to updated distances for vertices B and C.\nThe Bellman-Ford algorithm have now checked all edges 1 time. The algorithm will check all edges 3 more times before it is finished, because Bellman-Ford will check all edges as many times as there are vertices in the graph, minus 1.\nThe algorithm starts checking all edges a second time, starting with checking the edges going out from vertex A. Checking the edges A->C and A->E do not lead to updated distances.\nThe next edge to be checked is B->C, going out from vertex B. This leads to an updated distance from vertex D to C of 5-4=1.\nChecking the next edge C->A, leads to an updated distance 1-3=-2 for vertex A.\nThe check of edge C->A in round 2 of the Bellman-Ford algorithm is actually the last check that leads to an updated distance for this specific graph. The algorithm will continue to check all edges 2 more times without updating any distances.\nChecking all edges V−1V-1 times in the Bellman-Ford algorithm may seem like a lot, but it is done this many times to make sure that the shortest distances will always be found.\nImplementation of The Bellman-Ford Algorithm\nImplementing the Bellman-Ford algorithm is very similar to how we implemented Dijkstra's algorithm.\nWe start by creating the Graph class, where the methods __init__, add_edge, and add_vertex will be used to create the specific graph we want to run the Bellman-Ford algorithm on to find the shortest paths.\nThe bellman_ford method is also placed inside the Graph class. It is this method that runs the Bellman-Ford algorithm.\nLine 18-19: At the beginning, all vertices are set to have an infinite long distance from the starting vertex, except for the starting vertex itself, where the distance is set to 0.\nLine 21: All edges are checked V−1V-1 times.\nLine 22-23: A double for-loop checks all the edges in the adjacency matrix. For every vertex u, check edges going to vertices v.\nLine 24-26: If the edge exist, and if the calculated distance is shorter than the existing distance, update the distance to that vertex v.\nThe complete code, including the initialization of our specific graph and code for running the Bellman-Ford algorithm, looks like this:\nExample\nPython:\nNegative Edges in The Bellman-Ford Algorithm\nTo say that the Bellman-Ford algorithm finds the \"shortest paths\" is not intuitive, because how can we draw or imagine distances that are negative? So, to make it easier to understand we could instead say that it is the \"cheapest paths\" that are found with Bellman-Ford.\nIn practice, the Bellman-Ford algorithm could for example help us to find delivering routes where the edge weights represent the cost of fuel and other things, minus the money to be made by driving that edge between those two vertices.\nWith this interpretation in mind, the -3 weight on edge C->A could mean that the fuel cost is $5 driving from C to A, and that we get paid $8 for picking up packages in C and delivering them in A. So we end up earning $3 more than we spend. Therefore, a total of $2 can be made by driving the delivery route D->E->B->C->A in our graph above.\nNegative Cycles in The Bellman-Ford Algorithm\nIf we can go in circles in a graph, and the sum of edges in that circle is negative, we have a negative cycle.\nBy changing the weight on edge C->A from -3 to -9, we get two negative cycles: A->C->A and A->E->C->A. And every time we check these edges with the Bellman-Ford algorithm, the distances we calculate and update just become lower and lower.\nThe problem with negative cycles is that a shortest path does not exist, because we can always go one more round to get a path that is shorter.\nThat is why it is useful to implement the Bellman-Ford algorithm with detection for negative cycles.\nDetection of Negative Cycles in the Bellman-Ford Algorithm\nAfter running the Bellman-Ford algorithm, checking all edges in a graph V−1V-1 times, all the shortest distances are found.\nBut, if the graph contains negative cycles, and we go one more round checking all edges, we will find at least one shorter distance in this last round, right?\nSo to detect negative cycles in the Bellman-Ford algorithm, after checking all edges V−1V-1 times, we just need to check all edges one more time, and if we find a shorter distance this last time, we can conclude that a negative cycle must exist.\nBelow is the bellman_ford method, with negative cycle detection included, running on the graph above with negative cycles due to the C->A edge weight of -9:\nExample\nPython:\nLine 30-33: All edges are checked one more time to see if there are negative cycles.\nLine 34: Returning True indicates that a negative cycle exists, and None is returned instead of the shortest distances, because finding the shortest distances in a graph with negative cycles does not make sense (because a shorter distance can always be found by checking all edges one more time).\nLine 36: Returning False means that there is no negative cycles, and the distances can be returned.\nReturning The Paths from The Bellman-Ford Algorithm\nWe are currently finding the total weight of the the shortest paths, so that for example \"Distance from D to A: -2\" is a result from running the Bellman-Ford algorithm.\nBut by recording the predecessor of each vertex whenever an edge is relaxed, we can use that later in our code to print the result including the actual shortest paths. This means we can give more information in our result, with the actual path in addition to the path weight: \"D->E->B->C->A, Distance: -2\".\nThis last code example is the complete code for the Bellman-Ford algorithm, with everything we have discussed up until now: finding the weights of shortest paths, detecting negative cycles, and finding the actual shortest paths:\nExample\nPython:\nLine 19: The predecessors array holds each vertex' predecessor vertex in the shortest path.\nLine 28: The predecessors array gets updated with the new predecessor vertex every time an edge is relaxed.\nLine 40-49: The get_path method uses the predecessors array to generate the shortest path string for each vertex.\nTime Complexity for The Bellman-Ford Algorithm\nThe time complexity for the Bellman-Ford algorithm mostly depends on the nested loops.\nThe outer for-loop runs V−1V-1 times, or VV times in case we also have negative cycle detection. For graphs with many vertices, checking all edges one less time than there are vertices makes little difference, so we can say that the outer loop contributes with O(V)O(V) to the time complexity.\nThe two inner for-loops checks all edges in the graph. If we assume a worst case scenario in terms of time complexity, then we have a very dense graph where every vertex has an edge to every other vertex, so for all vertex VV the edge to all other vertices VV must be checked, which contributes with O(V2)O(V^2) to the time complexity.\nSo in total, we get the time complexity for the Bellman-Ford algorithm:\nO(V3) O(V^3)\nHowever, in practical situations and especially for sparse graphs, meaning each vertex only has edges to a small portion of the other vertices, time complexity of the two inner for-loops checking all edges can be approximated from O(V2)O(V^2) to O(E)O(E), and we get the total time complexity for Bellman-Ford:\nO(V⋅E) O(V \\cdot E)\nThe time complexity for the Bellman-Ford algorithm is slower than for Dijkstra's algorithm, but Bellman-Ford can find the shortest paths in graphs with negative edges and it can detect negative cycles, which Dijkstra's algorithm cannot do.\nDSA Exercises\nTest Yourself With Exercises\nExercise:\nIn the adjacency matrix below:\nWhat is the edge weight of the edge going from D to E?\nStart the Exercise",
      "examples": [
        "class Graph: def __init__(self, size): self.adj_matrix = [[0] * size for _ in range(size)] self.size = size self.vertex_data = [''] * size def add_edge(self, u, v, weight): if 0 <= u < self.size and 0 <= v < self.size: self.adj_matrix[u][v] = weight #self.adj_matrix[v][u] = weight # For undirected graph def add_vertex_data(self, vertex, data): if 0 <= vertex < self.size: self.vertex_data[vertex] = data",
        "def bellman_ford(self, start_vertex_data): start_vertex = self.vertex_data.index(start_vertex_data) distances = [float('inf')] * self.size distances[start_vertex] = 0 for i in range(self.size - 1): for u in range(self.size): for v in range(self.size): if self.adj_matrix[u][v] != 0: if distances[u] + self.adj_matrix[u][v] < distances[v]: distances[v] = distances[u] + self.adj_matrix[u][v] print(f\"Relaxing edge {self.vertex_data[u]}-{self.vertex_data[v]}, Updated distance to {self.vertex_data[v]}: {distances[v]}\") return distances",
        "class Graph: def __init__(self, size): self.adj_matrix = [[0] * size for _ in range(size)] self.size = size self.vertex_data = [''] * size def add_edge(self, u, v, weight): if 0 <= u < self.size and 0 <= v < self.size: self.adj_matrix[u][v] = weight #self.adj_matrix[v][u] = weight # For undirected graph def add_vertex_data(self, vertex, data): if 0 <= vertex < self.size: self.vertex_data[vertex] = data def bellman_ford(self, start_vertex_data): start_vertex = self.vertex_data.index(start_vertex_data) distances = [float('inf')] * self.size distances[start_vertex] = 0 for i in range(self.size - 1): for u in range(self.size): for v in range(self.size): if self.adj_matrix[u][v] != 0: if distances[u] + self.adj_matrix[u][v] < distances[v]: distances[v] = distances[u] + self.adj_matrix[u][v] print(f\"Relaxing edge {self.vertex_data[u]}-{self.vertex_data[v]}, Updated distance to {self.vertex_data[v]}: {distances[v]}\") return distances g = Graph(5) g.add_vertex_data(0, 'A') g.add_vertex_data(1, 'B') g.add_vertex_data(2, 'C') g.add_vertex_data(3, 'D') g.add_vertex_data(4, 'E') g.add_edge(3, 0, 4) # D -> A, weight 4 g.add_edge(3, 2, 7) # D -> C, weight 7 g.add_edge(3, 4, 3) # D -> E, weight 3 g.add_edge(0, 2, 4) # A -> C, weight 4 g.add_edge(2, 0, -3) # C -> A, weight -3 g.add_edge(0, 4, 5) # A -> E, weight 5 g.add_edge(4, 2, 3) # E -> C, weight 3 g.add_edge(1, 2, -4) # B -> C, weight -4 g.add_edge(4, 1, 2) # E -> B, weight 2 # Running the Bellman-Ford algorithm from D to all vertices print(\"\\nThe Bellman-Ford Algorithm starting from vertex D:\") distances = g.bellman_ford('D') for i, d in enumerate(distances): print(f\"Distance from D to {g.vertex_data[i]}: {d}\")",
        "def bellman_ford(self, start_vertex_data): start_vertex = self.vertex_data.index(start_vertex_data) distances = [float('inf')] * self.size distances[start_vertex] = 0 for i in range(self.size - 1): for u in range(self.size): for v in range(self.size): if self.adj_matrix[u][v] != 0: if distances[u] + self.adj_matrix[u][v] < distances[v]: distances[v] = distances[u] + self.adj_matrix[u][v] print(f\"Relaxing edge {self.vertex_data[u]}->{self.vertex_data[v]}, Updated distance to {self.vertex_data[v]}: {distances[v]}\") # Negative cycle detection for u in range(self.size): for v in range(self.size): if self.adj_matrix[u][v] != 0: if distances[u] + self.adj_matrix[u][v] < distances[v]: return (True, None) # Indicate a negative cycle was found return (False, distances) # Indicate no negative cycle and return distances",
        "class Graph: def __init__(self, size): self.adj_matrix = [[0] * size for _ in range(size)] self.size = size self.vertex_data = [''] * size def add_edge(self, u, v, weight): if 0 <= u < self.size and 0 <= v < self.size: self.adj_matrix[u][v] = weight #self.adj_matrix[v][u] = weight # For undirected graph def add_vertex_data(self, vertex, data): if 0 <= vertex < self.size: self.vertex_data[vertex] = data def bellman_ford(self, start_vertex_data): start_vertex = self.vertex_data.index(start_vertex_data) distances = [float('inf')] * self.size predecessors = [None] * self.size distances[start_vertex] = 0 for i in range(self.size - 1): for u in range(self.size): for v in range(self.size): if self.adj_matrix[u][v] != 0: if distances[u] + self.adj_matrix[u][v] < distances[v]: distances[v] = distances[u] + self.adj_matrix[u][v] predecessors[v] = u print(f\"Relaxing edge {self.vertex_data[u]}->{self.vertex_data[v]}, Updated distance to {self.vertex_data[v]}: {distances[v]}\") # Negative cycle detection for u in range(self.size): for v in range(self.size): if self.adj_matrix[u][v] != 0: if distances[u] + self.adj_matrix[u][v] < distances[v]: return (True, None, None) # Indicate a negative cycle was found return (False, distances, predecessors) # Indicate no negative cycle and return distances def get_path(self, predecessors, start_vertex, end_vertex): path = [] current = self.vertex_data.index(end_vertex) while current is not None: path.insert(0, self.vertex_data[current]) current = predecessors[current] if current == self.vertex_data.index(start_vertex): path.insert(0, start_vertex) break return '->'.join(path) g = Graph(5) g.add_vertex_data(0, 'A') g.add_vertex_data(1, 'B') g.add_vertex_data(2, 'C') g.add_vertex_data(3, 'D') g.add_vertex_data(4, 'E') g.add_edge(3, 0, 4) # D -> A, weight 4 g.add_edge(3, 2, 7) # D -> C, weight 7 g.add_edge(3, 4, 3) # D -> E, weight 3 g.add_edge(0, 2, 4) # A -> C, weight 4 g.add_edge(2, 0, -3) # C -> A, weight -3 g.add_edge(0, 4, 5) # A -> E, weight 5 g.add_edge(4, 2, 3) # E -> C, weight 3 g.add_edge(1, 2, -4) # B -> C, weight -4 g.add_edge(4, 1, 2) # E -> B, weight 2 # Running the Bellman-Ford algorithm from D to all vertices print(\"\\nThe Bellman-Ford Algorithm starting from vertex D:\") negative_cycle, distances, predecessors = g.bellman_ford('D') if not negative_cycle: for i, d in enumerate(distances): if d != float('inf'): path = g.get_path(predecessors, 'D', g.vertex_data[i]) print(f\"{path}, Distance: {d}\") else: print(f\"No path from D to {g.vertex_data[i]}, Distance: Infinity\") else: print(\"Negative weight cycle detected. Cannot compute shortest paths.\")",
        "The D->E edge weight is .",
        "Graph",
        "__init__",
        "add_edge",
        "add_vertex",
        "bellman_ford",
        "u",
        "v",
        "True",
        "None",
        "False",
        "distances",
        "predecessors",
        "get_path"
      ]
    },
    {
      "title": "DSA Minimum Spanning Tree",
      "summary": "The Minimum Spanning Tree Problem\nThe Minimum Spanning Tree (MST) is the collection of edges required to connect all vertices in an undirected graph, with the minimum total edge weight.\nThe animation above runs Prim's algorithm to find the MST. Another way to find the MST, which also works for unconnected graphs, is to run Kruskal's algorithm.\nIt is called a Minimum Spanning Tree, because it is a connected, acyclic, undirected graph, which is the definition of a tree data structure.\nIn the real world, finding the Minimum Spanning Tree can help us find the most effective way to connect houses to the internet or to the electrical grid, or it can help us finding the fastest route to deliver packages.\nAn MST Thought Experiment\nLet's imagine that the circles in the animation above are villages that are without electrical power, and you want to connect them to the electrical grid. After one village is given electrical power, the electrical cables must be spread out from that village to the others. The villages can be connected in a lot of different ways, each route having a different cost.\nThe electrical cables are expensive, and digging ditches for the cables, or stretching the cables in the air is expensive as well. The terrain can certainly be a challenge, and then there is perhaps a future cost for maintenance that is different depending on where the cables end up.\nAll these route costs can be factored in as edge weights in a graph. Every vertex represents a village, and every edge represents a possible route for the electrical cable between two villages.\nAfter such a graph is created, the Minimum Spanning Tree (MST) can be found, and that will be the most effective way to connect these villages to the electrical grid.\nAnd this is actually what the first MST algorithm (Borůvka's algorithm) was made for in 1926: To find the best way to connect the historical region of Moravia, in the Check Republic, to the electrical grid.\nMST Algorithms\nThe next two pages in this tutorial explains two algorithms that finds the Minimum Spanning Tree in a graph: Prim's algorithm, and Kruskal's algorithm.",
      "examples": []
    },
    {
      "title": "DSA Prim's Algorithm",
      "summary": "Prim's algorithm was invented in 1930 by the Czech mathematician Vojtěch Jarník.\nThe algorithm was then rediscovered by Robert C. Prim in 1957, and also rediscovered by Edsger W. Dijkstra in 1959. Therefore, the algorithm is also sometimes called \"Jarník's algorithm\", or the \"Prim-Jarník algorithm\".\nPrim's Algorithm\nPrim's algorithm finds the Minimum Spanning Tree (MST) in a connected and undirected graph.\nThe MST found by Prim's algorithm is the collection of edges in a graph, that connects all vertices, with a minimum sum of edge weights.\nPrim's algorithm finds the MST by first including a random vertex to the MST. The algorithm then finds the vertex with the lowest edge weight from the current MST, and includes that to the MST. Prim's algorithm keeps doing this until all nodes are included in the MST.\nPrim's algorithm is greedy, and has a straightforward way to create a minimum spanning tree.\nFor Prim's algorithm to work, all the nodes must be connected. To find the MST's in an unconnected graph, Kruskal's algorithm can be used instead. You can read about Kruskal's algorithm on the next page.\nHow it works:\nChoose a random vertex as the starting point, and include it as the first vertex in the MST.\nCompare the edges going out from the MST. Choose the edge with the lowest weight that connects a vertex among the MST vertices to a vertex outside the MST.\nAdd that edge and vertex to the MST.\nKeep doing step 2 and 3 until all vertices belong to the MST.\nNOTE: Since the starting vertex is chosen at random, it is possible to have different edges included in the MST for the same graph, but the total edge weight of the MST will still have the same minimum value.\nManual Run Through\nLet's run through Prim's algorithm manually on the graph below, so that we understand the detailed step-by-step operations before we try to program it.\nPrim's algorithm starts growing the Minimum Spanning Tree (MST) from a random vertex, but for this demonstration vertex A is chosen as the starting vertex.\nFrom vertex A, the MST grows along the edge with the lowest weight. So vertices A and D now belong to the group of vertices that belong to the Minimum Spanning Tree.\nA parents array is central to how Prim's algorithm grows the edges in the MST.\nAt this point, the parents array looks like this:\nVertex A, the starting vertex, has no parent, and has therefore value -1. Vertex D's parent is A, that is why D's parent value is 0 (vertex A is located at index 0). B's parent is also A, and D is the parent of E and F.\nThe parents array helps us to keep the MST tree structure (a vertex can only have one parent).\nAlso, to avoid cycles and to keep track of which vertices are currently in the MST, the in_mst array is used.\nThe in_mst array currently looks like this:\nThe next step in Prim's algorithm is to include one more vertex as part of the MST, and the vertex closest to the current MST nodes A and D is chosen.\nSince both A-B and D-F have the same lowest edge weight 4, either B or F can be chosen as the next MST vertex. We choose B as the next MST vertex for this demonstration.\nAs you can see, the MST edge to E came from vertex D before, now it comes from vertex B, because B-E with weight 6 is lower than D-E with weight 7. Vertex E can only have one parent in the MST tree structure (and in the parents array), so B-E and D-E cannot both be MST edges to E.\nThe next vertex in the MST is vertex C, because edge B-C with weight 3 is the shortest edge weight from the current MST vertices.\nAs vertex C is included in the MST, edges out from C are checked to see if there are edges with a lower weight from this MST vertex to vertices outside the MST. Edge C-E has a lower weight (3) than the previous B-E MST edge (6), and the C-H edge gets included in the MST with edge weight 2.\nVertex H is the next to be included in the MST, as it has the lowest edge weight 6, and vertex H becomes the parent of vertex G in the parents array.\nThe next vertex to be included in the MST is either E or F because they have both the lowest edge weight to them: 4.\nWe choose vertex E as the next vertex to be included in the MST for this demonstration.\nThe next and last two vertices to be added to the MST are vertices F and G. D-F is the MST edge to F, and E-G is the MST edge to G because these edges are the edges with the lowest weight from the current MST.\nRun the simulation below to see Prim's algorithm doing the manual steps that we have just done.\nImplementation of Prim's Algorithm\nFor Prim's algorithm to find a Minimum Spanning Tree (MST), we create a Graph class. We will use the methods inside this Graph class later to create the graph from the example above, and to run Prim's algorithm on it.\nLine 3-5: At first, the adjacency matrix is empty, meaning there are no edges in the graph. Also, the vertices have no names to start with.\nLine 7-10: The add_edge method is for adding an edge, with an edge weight value, to the undirected graph.\nLine 12-14: The add_vertex_data method is used for giving names to the vertices, like for example 'A' or 'B'.\nNow that the structure for creating a graph is in place, we can implement Prim's algorithm as a method inside the Graph class:\nLine 17: The in_mst array holds the status of which vertices are currently in the MST. Initially, none of the vertices are part of the MST.\nLine 18: The key_values array holds the current shortest distance from the MST vertices to each vertex outside the MST.\nLine 19: The MST edges are stored in the parents array. Each MST edge is stored by storing the parent index for each vertex.\nLine 21: To keep it simple, and to make this code run like in the \"Manual Run Through\" animation/example above, the first vertex (vertex A at index 0) is set as the staring vertex. Changing the index to 4 will run Prim's algorithm from vertex E, and that works just as well.\nLine 25: The index is found for the vertex with the lowest key value that is not yet part of the MST. Check out these explanations for min and lambda to better understand this Python code line.\nLine 32-35: After a new vertex is added to the MST (line 27), this part of the code checks to see if there are now edges from this newly added MST vertex that can lower the key values to other vertices outside the MST. If that is the case, the key_values and parents arrays are updated accordingly. This can be seen clearly in the animation when a new vertex is added to the MST and becomes the active (current) vertex.\nNow let's create the graph from the \"Manual Run Through\" above and run Prim's algorithm on it:\nExample\nPython:\nLine 32: We can actually avoid the last loop in Prim's algorithm by changing this line to for _ in range(self.size - 1):. This is because when there is just one vertex not yet in the MST, the parent vertex for that vertex is already set correctly in the parents array, so the MST is actually already found at this point.\nTime Complexity for Prim's Algorithm\nFor a general explanation of what time complexity is, visit this page.\nWith \\(V\\) as the number of vertices in our graph, the time complexity for Prim's algorithm is\n\\[ O( V^2 ) \\]\nThe reason why we get this time complexity is because of the nested loops inside the Prim's algorithm (one for-loop with two other for-loops inside it).\nThe first for-loop (line 24) goes through all the vertices in the graph. This has time complexity \\(O(V)\\).\nThe second for-loop (line 25) goes through all the adjacent vertices in the graph to find the vertex with the lowest key value that is outside the MST, so that it can be the next vertex included in the MST. This has time complexity \\(O(V)\\).\nAfter a new vertex is included in the MST, a third for-loop (line 32) checks all other vertices to see if there are outgoing edges from the newly added MST vertex to vertices outside the MST that can lead to lower key values and updated parent relations. This also has time complexity \\(O(V)\\).\nPutting the time complexities together we get:\n\\[ \\begin{equation} \\begin{aligned} O(V)\\cdot (O(V)+O(V)) & = O(V)\\cdot (2\\cdot O(V)) \\\\ & = O(V\\cdot 2\\cdot V) \\\\ & = O(2\\cdot V^2) \\\\\\\\ & = O(V^2) \\end{aligned} \\end{equation} \\]\nBy using a priority queue data structure to manage key values, instead of using an array like we do here, the time complexity for Prim's algorithm can be reduced to:\n\\[ O( E \\cdot \\log{V}) \\]\nWhere \\(E\\) is the number of edges in the graph, and \\(V\\) is the number of vertices.\nSuch an implementation of Prim's algorithm using a priority queue is best for sparse graphs. A graph is sparse when the each vertex is just connected to a few of the other vertices.",
      "examples": [
        "parents = [-1, 0, -1, 0, 3, 3, -1, -1] #vertices [ A, B, C, D, E, F, G, H]",
        "in_mst = [ true, false, false, true, false, false, false, false] #vertices [ A, B, C, D, E, F, G, H]",
        "class Graph: def __init__(self, size): self.adj_matrix = [[0] * size for _ in range(size)] self.size = size self.vertex_data = [''] * size def add_edge(self, u, v, weight): if 0 <= u < self.size and 0 <= v < self.size: self.adj_matrix[u][v] = weight self.adj_matrix[v][u] = weight # For undirected graph def add_vertex_data(self, vertex, data): if 0 <= vertex < self.size: self.vertex_data[vertex] = data",
        "def prims_algorithm(self): in_mst = [False] * self.size key_values = [float('inf')] * self.size parents = [-1] * self.size key_values[0] = 0 # Starting vertex print(\"Edge \\tWeight\") for _ in range(self.size): u = min((v for v in range(self.size) if not in_mst[v]), key=lambda v: key_values[v]) in_mst[u] = True if parents[u] != -1: # Skip printing for the first vertex since it has no parent print(f\"{self.vertex_data[parents[u]]}-{self.vertex_data[u]} \\t{self.adj_matrix[u][parents[u]]}\") for v in range(self.size): if 0 < self.adj_matrix[u][v] < key_values[v] and not in_mst[v]: key_values[v] = self.adj_matrix[u][v] parents[v] = u",
        "class Graph: def __init__(self, size): self.adj_matrix = [[0] * size for _ in range(size)] self.size = size self.vertex_data = [''] * size def add_edge(self, u, v, weight): if 0 <= u < self.size and 0 <= v < self.size: self.adj_matrix[u][v] = weight self.adj_matrix[v][u] = weight # For undirected graph def add_vertex_data(self, vertex, data): if 0 <= vertex < self.size: self.vertex_data[vertex] = data def prims_algorithm(self): in_mst = [False] * self.size key_values = [float('inf')] * self.size parents = [-1] * self.size key_values[0] = 0 # Starting vertex print(\"Edge \\tWeight\") for _ in range(self.size): u = min((v for v in range(self.size) if not in_mst[v]), key=lambda v: key_values[v]) in_mst[u] = True if parents[u] != -1: # Skip printing for the first vertex since it has no parent print(f\"{self.vertex_data[parents[u]]}-{self.vertex_data[u]} \\t{self.adj_matrix[u][parents[u]]}\") for v in range(self.size): if 0 < self.adj_matrix[u][v] < key_values[v] and not in_mst[v]: key_values[v] = self.adj_matrix[u][v] parents[v] = u g = Graph(8) g.add_vertex_data(0, 'A') g.add_vertex_data(1, 'B') g.add_vertex_data(2, 'C') g.add_vertex_data(3, 'D') g.add_vertex_data(4, 'E') g.add_vertex_data(5, 'F') g.add_vertex_data(6, 'G') g.add_vertex_data(7, 'H') g.add_edge(0, 1, 4) # A - B g.add_edge(0, 3, 3) # A - D g.add_edge(1, 2, 3) # B - C g.add_edge(1, 3, 5) # B - D g.add_edge(1, 4, 6) # B - E g.add_edge(2, 4, 4) # C - E g.add_edge(2, 7, 2) # C - H g.add_edge(3, 4, 7) # D - E g.add_edge(3, 5, 4) # D - F g.add_edge(4, 5, 5) # E - F g.add_edge(4, 6, 3) # E - G g.add_edge(5, 6, 7) # F - G g.add_edge(6, 7, 5) # G - H print(\"Prim's Algorithm MST:\") g.prims_algorithm()",
        "parents",
        "-1",
        "0",
        "in_mst",
        "4",
        "6",
        "7",
        "3",
        "2",
        "Graph",
        "add_edge",
        "add_vertex_data",
        "key_values",
        "min",
        "lambda",
        "for _ in range(self.size - 1):"
      ]
    },
    {
      "title": "DSA Kruskal's Algorithm",
      "summary": "Kruskal's Algorithm\nKruskal's algorithm finds the Minimum Spanning Tree (MST), or Minimum Spanning Forest, in an undirected graph.\nConnected\nThe MST (or MSTs) found by Kruskal's algorithm is the collection of edges that connect all vertices (or as many as possible) with the minimum total edge weight.\nKruskal's algorithm adds edges to the MST (or Minimum Spanning Forest), starting with the edges with the lowest edge weights.\nEdges that would create a cycle are not added to the MST. These are the red blinking lines in the animation above.\nKruskal's algorithm checks all edges in the graph, but the animation above is made to stop when the MST or Minimum Spanning forest is completed, so that you don't have to wait for the longest edges to be checked.\nMinimum Spanning Forest is what it is called when a graph has more than one Minimum Spanning Tree. This happens when a graph is not connected. Try it yourself by using the checkbox in the animation above.\nUnlike Prim's algorithm, Kruskal's algorithm can be used for such graphs that are not connected, which means that it can find more than one MST, and that is what we call a Minimum Spanning Forest.\nTo find out if an edge will create a cycle, we will use Union-Find cycle detection inside Kruskal's algorithm.\nHow it works:\nSort the edges in the graph from the lowest to the highest edge weight.\nFor each edge, starting with the one with the lowest edge weight:\nWill this edge create a cycle in the current MST?\nIf no: Add the edge as an MST edge.\nWill this edge create a cycle in the current MST?\nIf no: Add the edge as an MST edge.\nIf no: Add the edge as an MST edge.\nManual Run Through\nLet's run through Kruskal's algorithm manually on the graph below, so that we understand the detailed step-by-step operations before we try to program it.\nThe first three edges are added to the MST. These three edges have the lowest edge weights and do not create any cycles:\nC-E, weight 2\nD-E, weight 3\nA-B, weight 4\nAfter that, edge C-D (indicated in red) cannot be added as it would lead to a cycle.\nThe next four edges Kruskal's algorithm tries to add to the MST are:\nE-G, weight 6\nC-G, weight 7 (not added)\nD-F, weight 7\nB-C, weight 8\nEdge C-G (indicated in red) cannot be added to the MST because it would create a cycle.\nAs you can see, the MST is already created at this point, but Kruskal's algorithm will continue to run until all edges are tested to see if they can be added to the MST.\nThe last three edges Kruskal's algorithm tries to add to the MST are the ones with the highest edge weights:\nA-C, weight 9 (not added)\nA-G, weight 10 (not added)\nF-G, weight 11 (not added)\nEach of these edges would create a cycle in the MST, so they cannot be added.\nKruskal's algorithm is now finished.\nRun the simulation below to see Kruskal's algorithm doing the manual steps that we have just done.\nNote: Although Kruskal's algorithm checks all edges in the graph, the animation at the top of this page stops right after the last edge is added to the MST or Minimum Spanning Forest so that we don't have to look at all the red edges that can't be added.\nThis is possible because for a connected graph, there is just one MST, and the search can stop when the number of edges in the MST is one less than there are vertices in the graph (V−1V-1). For the unconnected graph, there are two MSTs in our animation, and the algorithm stops when the MSTs have reached a size of V−2V-2 edges in total.\nImplementation of Kruskal's Algorithm\nFor Kruskal's algorithm to find a Minimum Spanning Tree (MST), or a Minimum Spanning Forest, we create a Graph class. We will use the methods inside this Graph class later to create the graph from the example above, and to run Kruskal's algorithm on it.\nLine 8 and 12: Checks if the input arguments u, v, and vertex, are within the possible range of index values.\nTo do Union-Find cycle detection in Kruskal's algorithm, these two methods find and union are also defined inside the Graph class:\nLine 15-18: The find method uses the parent array to recursively find the root of a vertex. For each vertex, the parent array holds a pointer (index) to the parent of that vertex. The root vertex is found when the find method comes to a vertex in the parent array that points to itself. Keep reading to see how the find method and the parent array are used inside the kruskals_algorithm method.\nLine 20-29: When an edge is added to the MST, the union method uses the parent array to merge (union) two trees. The rank array holds a rough estimate of the tree height for every root vertex. When merging two trees, the root with a lesser rank becomes a child of the other tree's root vertex.\nHere is how Kruskal's algorithm is implemented as a method inside the Graph class:\nLine 35: The edges must be sorted before Kruskal's algorithm starts trying to add the edges to the MST.\nLine 40-41: The parent and rank arrays are initialized. To start with, every vertex is its own root (every element in the parent array points to itself), and every vertex has no height (0 values in the rank array).\nLine 44-45: Pick the smallest edge, and increment i so that the correct edge is picked in the next iteration.\nLine 47-51: If the vertices u and v at each end of the current edge have different roots x and y, it means there will be no cycle for the new edge and the trees are merged. To merge the trees, the current edge is added to the result array, and we run the union method to make sure the trees are merged correctly, so that there is only one root vertex in the resulting merged tree.\nNow let's create the graph from the \"Manual Run Through\" above and run Kruskal's algorithm on it:\nExample\nPython:\nTime Complexity for Kruskal's Algorithm\nFor a general explanation of what time complexity is, visit this page.\nWith EE as the number of edges in our graph, the time complexity for Kruskal's algorithm is\nO(E⋅logE) O( E \\cdot log{E} )\nWe get this time complexity because the edges must be sorted before Kruskal's can start adding edges to the MST. Using a fast algorithm like Quick Sort or Merge Sort gives us a time complexity of O(E⋅logE) O( E \\cdot log{E} ) for this sorting alone.\nAfter the edges are sorted, they are all checked one by one, to see if they will create a cycle, and if not, they are added to the MST.\nAlthough it looks like a lot of work to check if a cycle will be created using the find method, and then to include an edge to the MST using the union method, this can still be viewed as one operation. The reason we can see this as just one operation is that it takes approximately constant time. That means that the time this operation takes grows very little as the graph grows, and so it does actually not contribute to the overall time complexity.\nSince the time complexity for Kruskal's algorithm only varies with the number of edges EE, it is especially fast for sparse graphs where the ratio between the number of edges EE and the number of vertices VV is relatively low.",
      "examples": [
        "class Graph: def __init__(self, size): self.size = size self.edges = [] # For storing edges as (weight, u, v) self.vertex_data = [''] * size # Store vertex names def add_edge(self, u, v, weight): if 0 <= u < self.size and 0 <= v < self.size: self.edges.append((weight, u, v)) # Add edge with weight def add_vertex_data(self, vertex, data): if 0 <= vertex < self.size: self.vertex_data[vertex] = data",
        "def find(self, parent, i): if parent[i] == i: return i return self.find(parent, parent[i]) def union(self, parent, rank, x, y): xroot = self.find(parent, x) yroot = self.find(parent, y) if rank[xroot] < rank[yroot]: parent[xroot] = yroot elif rank[xroot] > rank[yroot]: parent[yroot] = xroot else: parent[yroot] = xroot rank[xroot] += 1",
        "def kruskals_algorithm(self): result = [] # MST i = 0 # edge counter self.edges = sorted(self.edges, key=lambda item: item[2]) parent, rank = [], [] for node in range(self.size): parent.append(node) rank.append(0) while i < len(self.edges): u, v, weight = self.edges[i] i += 1 x = self.find(parent, u) y = self.find(parent, v) if x != y: result.append((u, v, weight)) self.union(parent, rank, x, y) print(\"Edge \\tWeight\") for u, v, weight in result: print(f\"{self.vertex_data[u]}-{self.vertex_data[v]} \\t{weight}\")",
        "class Graph: def __init__(self, size): self.size = size self.edges = [] # For storing edges as (weight, u, v) self.vertex_data = [''] * size # Store vertex names def add_edge(self, u, v, weight): if 0 <= u < self.size and 0 <= v < self.size: self.edges.append((u, v, weight)) # Add edge with weight def add_vertex_data(self, vertex, data): if 0 <= vertex < self.size: self.vertex_data[vertex] = data def find(self, parent, i): if parent[i] == i: return i return self.find(parent, parent[i]) def union(self, parent, rank, x, y): xroot = self.find(parent, x) yroot = self.find(parent, y) if rank[xroot] < rank[yroot]: parent[xroot] = yroot elif rank[xroot] > rank[yroot]: parent[yroot] = xroot else: parent[yroot] = xroot rank[xroot] += 1 def kruskals_algorithm(self): result = [] # MST i = 0 # edge counter self.edges = sorted(self.edges, key=lambda item: item[2]) parent, rank = [], [] for node in range(self.size): parent.append(node) rank.append(0) while i < len(self.edges): u, v, weight = self.edges[i] i += 1 x = self.find(parent, u) y = self.find(parent, v) if x != y: result.append((u, v, weight)) self.union(parent, rank, x, y) print(\"Edge \\tWeight\") for u, v, weight in result: print(f\"{self.vertex_data[u]}-{self.vertex_data[v]} \\t{weight}\") g = Graph(7) g.add_vertex_data(0, 'A') g.add_vertex_data(1, 'B') g.add_vertex_data(2, 'C') g.add_vertex_data(3, 'D') g.add_vertex_data(4, 'E') g.add_vertex_data(5, 'F') g.add_vertex_data(6, 'G') g.add_edge(0, 1, 4) #A-B, 4 g.add_edge(0, 6, 10) #A-G, 10 g.add_edge(0, 2, 9) #A-C, 9 g.add_edge(1, 2, 8) #B-C, 8 g.add_edge(2, 3, 5) #C-D, 5 g.add_edge(2, 4, 2) #C-E, 2 g.add_edge(2, 6, 7) #C-G, 7 g.add_edge(3, 4, 3) #D-E, 3 g.add_edge(3, 5, 7) #D-F, 7 g.add_edge(4, 6, 6) #E-G, 6 g.add_edge(5, 6, 11) #F-G, 11 print(\"Kruskal's Algorithm MST:\") g.kruskals_algorithm()",
        "Graph",
        "u",
        "v",
        "vertex",
        "find",
        "union",
        "parent",
        "kruskals_algorithm",
        "rank",
        "0",
        "i",
        "x",
        "y",
        "result"
      ]
    },
    {
      "title": "DSA Maximum Flow",
      "summary": "The Maximum Flow Problem\nThe Maximum Flow problem is about finding the maximum flow through a directed graph, from one place in the graph to another.\nMore specifically, the flow comes from a source vertex ss, and ends up in a sink vertex tt, and each edge in the graph is defined with a flow and a capacity, where the capacity is the maximum flow that edge can have.\nMax flow: 0\nFinding the maximum flow can be very useful:\nFor planning roads in a city to avoid future traffic jams.\nTo assess the effect of removing a water pipe, or electrical wire, or network cable.\nTo find out where in the flow network expanding the capacity will lead to the highest maximum flow, with the purpose of increasing for example traffic, data traffic, or water flow.\nTerminology And Concepts\nA flow network if often what we call a directed graph with a flow flowing through it.\nThe capacity cc of an edge tells us how much flow is allowed to flow through that edge.\nEach edge also has a flow value that tells how much the current flow is in that edge.\nThe edge in the image above v1→v2 v_1 \\rightarrow v_2 , going from vertex v1 v_1 to vertex v2 v_2 , has its flow and capacity described as 0/7, which means the flow is 0, and the capacity is 7. So the flow in this edge can be increased up to 7, but not more.\nIn its simplest form, flow network has one source vertex ss where the flow comes out, and one sink vertex tt where the flow goes in. The other vertices just have flow passing through them.\nFor all vertices except ss and tt, there is a conservation of flow, which means that the same amount of flow that goes into a vertex, must also come out of it.\nThe maximum flow is found by algorithms such as Ford-Fulkerson, or Edmonds-Karp, by sending more and more flow through the edges in the flow network until the capacity of the edges are such that no more flow can be sent through. Such a path where more flow can be sent through is called an augmented path.\nThe Ford-Fulkerson and Edmonds-Karp algorithms are implemented using something called a residual network. This will be explained in more detail on the next pages.\nThe residual network is set up with the residual capacities on each edge, where the residual capacity of an edge is the capacity on that edge, minus the flow. So when flow is increased in a an edge, the residual capacity is decreased with the same amount.\nFor each edge in the residual network, there is also a reversed edge that points in the opposite direction of the original edge. The residual capacity of a reversed edge is the flow of the original edge. Reversed edges are important for sending flow back on an edge as part of the maximum flow algorithms.\nThe image below shows the reversed edges in the graph from the simulation at the top of this page. Each reversed edge points in the opposite direction, and because there is no flow in the graph to begin with, the residual capacities for the reversed edges are 0.\nSome of these concepts, like the residual network and the reversed edge, can be hard to understand. That is why these concepts are explained more in detail, and with examples, on the next two pages.\nWhen the maximum flow is found, we get a value for how much flow can be sent through the flow network in total.\nMultiple Source and Sink Vertices\nThe Ford-Fulkerson and Edmonds-Karp algorithms expects one source vertex and one sink vertex to be able to find the maximum flow.\nIf the graph has more than one source vertex, or more than one sink vertex, the graph should be modified to find the maximum flow.\nTo modify the graph so that you can run the Ford-Fulkerson or Edmonds-Karp algorithm on it, create an extra super-source vertex if you have multiple source vertices, and create an extra super-sink vertex if you have multiple sink-vertices.\nFrom the super-source vertex, create edges to the original source vertices, with infinite capacities. And create edges from the sink vertices to the super-sink vertex similarly, with infinite capacities.\nThe image below shows such a graph with two sources s1s_1 and s2s_2, and three sinks t1t_1, t2t_2, and t3t_3.\nTo run Ford-Fulkerson or Edmonds-Karp on this graph, a super source SS is created with edges with infinite capacities to the original source nodes, and a super sink TT is created with edges with infinite capacities to it from the original sinks.\nThe Ford-Fulkerson or Edmonds-Karp algorithm is now able to find maximum flow in a graph with multiple source and sink vertices, by going from the super source SS, to the super sink TT.\nThe Max-flow Min-cut Theorem\nTo understand what this theorem says we first need to know what a cut is.\nWe create two sets of vertices: one with only the source vertex inside it called \"S\", and one with all the other vertices inside it (including the sink vertex) called \"T\".\nNow, starting in the source vertex, we can choose to expand set S by including adjacent vertices, and continue to include adjacent vertices as much as we want as long as we do not include the sink vertex.\nExpanding set S will shrink set T, because any vertex belongs either to set S or set T.\nIn such a setup, with any vertex belonging to either set S or set T, there is a \"cut\" between the sets. The cut consists of all the edges stretching from set S to set T.\nIf we add all the capacities from edges going from set S to set T, we get the capacity of the cut, which is the total possible flow from source to sink in this cut.\nThe minimum cut is the cut we can make with the lowest total capacity, which will be the bottleneck.\nIn the image below, three different cuts are done in the graph from the simulation in the top of this page.\nCut A: This cut has vertices ss and v1v_1 in set S, and the other vertices are in set T. The total capacity of the edges leaving set S in this cut, from sink to source, is 3+4+7=14. We are not adding the capacity from edge v2→v1v_2 \\rightarrow v_1, because this edge goes in the opposite direction, from sink to source. So the maximum possible flow across cut A is 14.\nCut B: The maximum possible flow is 3+4+3=10 across cut B.\nCut C: The maximum possible flow is 2+6=8 across cut C. If we checked all other cuts in the graph, we would not find a cut with a lower total capacity. This is the minimum cut. Have you run the simulation finding the maximum flow in the top of this page? Then you also know that 8 is the maximum flow, which is exactly what the max-flow min-cut theorem says.\nThe max-flow min-cut theorem says that finding the minimum cut in a graph, is the same as finding the maximum flow, because the value of the minimum cut will be the same value as the maximum flow.\nPractical Implications of The Max-flow Min-cut Theorem\nFinding the maximum flow in a graph using an algorithm like Ford-Fulkerson also helps us to understand where the minimum cut is: The minimum cut will be where the edges have reached full capacity.\nThe minimum cut will be where the bottleneck is, so if we want to increase flow beyond the maximum limit, which is often the case in practical situations, we now know which edges in the graph that needs to be modified to increase the overall flow.\nModifying edges in the minimum cut to allow more flow can be very useful in many situations:\nBetter traffic flow can be achieved because city planners now know where to create extra lanes, where to re-route traffic, or where to optimize traffic signals.\nIn manufacturing, a higher production output can be reached by targeting improvements where the bottleneck is, by upgrading equipment or reallocating resources for example.\nIn logistics, knowing where the bottleneck is, the supply chain can be optimized by changing routes, or increase capacity at critical points, ensuring that goods are moved more effectively from warehouses to consumers.\nSo using maximum flow algorithms to find the minimum cut, helps us to understand where the system can be modified to allow an even higher throughput.\nThe Maximum Flow Problem Described Mathematically\nThe maximum flow problem is not just a topic in Computer Science, it is also a type of Mathematical Optimization, that belongs to the field of Mathematics.\nIn case you want to understand this better mathematically, the maximum flow problem is described in mathematical terms below.\nAll edges (EE) in the graph, going from a vertex (uu) to a vertex (vv), have a flow (ff) that is less than, or equal to, the capacity (cc) of that edge:\n∀(u,v)∈E:f(u,v)≤c(u,v) \\forall (u,v) \\in E: f(u,v) \\leq c(u,v)\nThis basically just means that the flow in an edge is limited by the capacity in that edge.\nAlso, for all edges (EE), a flow in one direction from uu to vv is the same as having a negative flow in the reverse direction, from vv to uu:\n∀(u,v)∈E:f(u,v)=−f(v,u) \\forall (u,v) \\in E: f(u,v) = -f(v,u)\nAnd the expression below states that conservation of flow is kept for all vertices (uu) except for the source vertex (ss) and for the sink vertex (tt):\n∀u∈V∖{s,t}⇒∑w∈Vf(u,w)=0 \\forall u \\in V \\setminus \\{s,t\\} \\Rightarrow \\sum_{w \\in V} f(u,w) = 0\nThis just means that the amount of flow going into a vertex, is the same amount of flow that comes out of that vertex (except for the source and sink vertices).\nAnd at last, all flow leaving the source vertex ss, must end up in the sink vertex tt:\n∑(s,u)∈Ef(s,u)=∑(v,t)∈Ef(v,t) \\sum_{(s,u) \\in E} f(s,u) = \\sum_{(v,t) \\in E} f(v,t)\nThe equation above states that adding all flow going out on edges from the source vertex will give us the same sum as adding the flow in all edges going into the sink vertex.",
      "examples": [
        "0/7",
        "0",
        "7"
      ]
    },
    {
      "title": "DSA Ford-Fulkerson Algorithm",
      "summary": "The Ford-Fulkerson algorithm solves the maximum flow problem.\nFinding the maximum flow can be helpful in many areas: for optimizing network traffic, for manufacturing, for supply chain and logistics, or for airline scheduling.\nThe Ford-Fulkerson Algorithm\nThe Ford-Fulkerson algorithm solves the maximum flow problem for a directed graph.\nThe flow comes from a source vertex (ss) and ends up in a sink vertex (tt), and each edge in the graph allows a flow, limited by a capacity.\nMax flow: 0\nThe Ford-Fulkerson algorithm works by looking for a path with available capacity from the source to the sink (called an augmented path), and then sends as much flow as possible through that path.\nThe Ford-Fulkerson algorithm continues to find new paths to send more flow through until the maximum flow is reached.\nIn the simulation above, the Ford-Fulkerson algorithm solves the maximum flow problem: It finds out how much flow can be sent from the source vertex ss, to the sink vertex tt, and that maximum flow is 8.\nThe numbers in the simulation above are written in fractions, where the first number is the flow, and the second number is the capacity (maximum possible flow in that edge). So for example, 0/7 on edge s→v2s \\rightarrow v_2, means there is 0 flow, with a capacity of 7 on that edge.\nNote: The Ford-Fulkerson algorithm is often described as a method instead of as an algorithm, because it does not specify how to find a path where flow can be increased. This means it can be implemented in different ways, resulting in different time complexities. But for this tutorial we will call it an algorithm, and use Depth-First-Search to find the paths.\nYou can see the basic step-by-step description of how the Ford-Fulkerson algorithm works below, but we need to go into more detail later to actually understand it.\nHow it works:\nStart with zero flow on all edges.\nFind an augmented path where more flow can be sent.\nDo a bottleneck calculation to find out how much flow can be sent through that augmented path.\nIncrease the flow found from the bottleneck calculation for each edge in the augmented path.\nRepeat steps 2-4 until max flow is found. This happens when a new augmented path can no longer be found.\nResidual Network in Ford-Fulkerson\nThe Ford-Fulkerson algorithm actually works by creating and using something called a residual network, which is a representation of the original graph.\nIn the residual network, every edge has a residual capacity, which is the original capacity of the edge, minus the the flow in that edge. The residual capacity can be seen as the leftover capacity in an edge with some flow.\nFor example, if there is a flow of 2 in the v3→v4 v_3 \\rightarrow v_4 edge, and the capacity is 3, the residual flow is 1 in that edge, because there is room for sending 1 more unit of flow through that edge.\nReversed Edges in Ford-Fulkerson\nThe Ford-Fulkerson algorithm also uses something called reversed edges to send flow back. This is useful to increase the total flow.\nFor example, the last augmented path s→v2→v4→v3→ts \\rightarrow v_2 \\rightarrow v_4 \\rightarrow v_3 \\rightarrow t in the animation above and in the manual run through below shows how the total flow is increased by one more unit, by actually sending flow back on edge v4→v3 v_4 \\rightarrow v_3 , sending the flow in the reverse direction.\nSending flow back in the reverse direction on edge v3→v4 v_3 \\rightarrow v_4 in our example meas that this 1 unit of flow going out of vertex v3 v_3 , now leaves v3 v_3 on edge v3→t v_3 \\rightarrow t instead of v3→v4 v_3 \\rightarrow v_4 .\nTo send flow back, in the opposite direction of the edge, a reverse edge is created for each original edge in the network. The Ford-Fulkerson algorithm can then use these reverse edges to send flow in the reverse direction.\nA reversed edge has no flow or capacity, just residual capacity. The residual capacity for a reversed edge is always the same as the flow in the corresponding original edge. In our example, the edge v3→v4 v_3 \\rightarrow v_4 has a flow of 2, which means there is a residual capacity of 2 on the corresponding reversed edge v4→v3 v_4 \\rightarrow v_3 .\nThis just means that when there is a flow of 2 on the original edge v3→v4 v_3 \\rightarrow v_4 , there is a possibility of sending that same amount of flow back on that edge, but in the reversed direction. Using a reversed edge to push back flow can also be seen as undoing a part of the flow that is already created.\nThe idea of a residual network with residual capacity on edges, and the idea of reversed edges, are central to how the Ford-Fulkerson algorithm works, and we will go into more detail about this when we implement the algorithm further down on this page.\nManual Run Through\nThere is no flow in the graph to start with.\nTo find the maximum flow, the Ford-Fulkerson algorithm must increase flow, but first it needs to find out where the flow can be increased: it must find an augmented path.\nThe Ford-Fulkerson algorithm actually does not specify how such an augmented path is found (that is why it is often described as a method instead of an algorithm), but we will use Depth First Search (DFS) to find the augmented paths for the Ford-Fulkerson algorithm in this tutorial.\nThe first augmented path Ford-Fulkerson finds using DFS is s→v1→v3→v4→ts \\rightarrow v_1 \\rightarrow v_3 \\rightarrow v_4 \\rightarrow t.\nAnd using the bottleneck calculation, Ford-Fulkerson finds that 3 is the highest flow that can be sent through the augmented path, so the flow is increased by 3 for all the edges in this path.\nThe next iteration of the Ford-Fulkerson algorithm is to do these steps again:\nFind a new augmented path\nFind how much the flow in that path can be increased\nIncrease the flow along the edges in that path accordingly\nThe next augmented path is found to be s→v2→v1→v4→v3→ts \\rightarrow v_2 \\rightarrow v_1 \\rightarrow v_4 \\rightarrow v_3 \\rightarrow t, which includes the reversed edge v4→v3v_4 \\rightarrow v_3, where flow is sent back.\nThe Ford-Fulkerson concept of reversed edges comes in handy because it allows the path finding part of the algorithm to find an augmented path where reversed edges can also be included.\nIn this specific case that means that a flow of 2 can be sent back on edge v3→v4v_3 \\rightarrow v_4, going into v3→tv_3 \\rightarrow t instead.\nThe flow can only be increased by 2 in this path because that is the capacity in the v3→t v_3 \\rightarrow t edge.\nThe next augmented path is found to be s→v2→v1→v4→ts \\rightarrow v_2 \\rightarrow v_1 \\rightarrow v_4 \\rightarrow t.\nThe flow can be increased by 2 in this path. The bottleneck (limiting edge) is v1→v4 v_1 \\rightarrow v_4 because there is only room for sending two more units of flow in that edge.\nThe next and last augmented path is s→v2→v4→ts \\rightarrow v_2 \\rightarrow v_4 \\rightarrow t.\nThe flow can only be increased by 1 in this path because of edge v4→t v_4 \\rightarrow t being the bottleneck in this path with only space for one more unit of flow (capacity−flow=1capacity-flow=1).\nAt this point, a new augmenting path cannot be found (it is not possible to find a path where more flow can be sent through from ss to tt), which means the max flow has been found, and the Ford-Fulkerson algorithm is finished.\nThe maximum flow is 8. As you can see in the image above, the flow (8) is the same going out of the source vertex ss, as the flow going into the sink vertex tt.\nAlso, if you take any other vertex than ss or tt, you can see that the amount of flow going into a vertex, is the same as the flow going out of it. This is what we call conservation of flow, and this must hold for all such flow networks (directed graphs where each edge has a flow and a capacity).\nImplementation of The Ford-Fulkerson Algorithm\nTo implement the Ford-Fulkerson algorithm, we create a Graph class. The Graph represents the graph with its vertices and edges:\nLine 3: We create the adj_matrix to hold all the edges and edge capacities. Initial values are set to 0.\nLine 4: size is the number of vertices in the graph.\nLine 5: The vertex_data holds the names of all the vertices.\nLine 7-8: The add_edge method is used to add an edge from vertex u to vertex v, with capacity c.\nLine 10-12: The add_vertex_data method is used to add a vertex name to the graph. The index of the vertex is given with the vertex argument, and data is the name of the vertex.\nThe Graph class also contains the dfs method to find augmented paths, using Depth-First-Search:\nLine 15-18: The visited array helps to avoid revisiting the same vertices during the search for an augmented path. Vertices that belong to the augmented path are stored in the path array.\nLine 20-21: The current vertex is marked as visited, and then added to the path.\nLine 23-24: If the current vertex is the sink node, we have found an augmented path from the source vertex to the sink vertex, so that path can be returned.\nLine 26-30: Looping through all edges in the adjacency matrix starting from the current vertex s, ind represents an adjacent node, and val is the residual capacity on the edge to that vertex. If the adjacent vertex is not visited, and has residual capacity on the edge to it, go to that node and continue searching for a path from that vertex.\nLine 32: None is returned if no path is found.\nThe fordFulkerson method is the last method we add to the Graph class:\nInitially, the max_flow is 0, and the while loop keeps increasing the max_flow as long as there is an augmented path to increase flow in.\nLine 37: The augmented path is found.\nLine 39-42: Every edge in the augmented path is checked to find out how much flow can be sent through that path.\nLine 44-46: The residual capacity (capacity minus flow) for every forward edge is reduced as a result of increased flow.\nLine 47: This represents the reversed edge, used by the Ford-Fulkerson algorithm so that flow can be sent back (undone) on the the original forward edges. It is important to understand that these reversed edges are not in the original graph, they are fictitious edges introduced by Ford-Fulkerson to make the algorithm work.\nLine 49: Every time flow is increased over an augmented path, max_flow is increased by the same value.\nLine 51-52: This is just for printing the augmented path before the algorithm starts the next iteration.\nAfter defining the Graph class, the vertices and edges must be defined to initialize the specific graph, and the complete code for the Ford-Fulkerson algorithm example looks like this:\nExample\nPython:\nTime Complexity for The Ford-Fulkerson Algorithm\nThe time complexity for the Ford-Fulkerson varies with the number of vertices VV, the number of edges EE, and it actually varies with the maximum flow ff in the graph as well.\nThe reason why the time complexity varies with the maximum flow ff in the graph, is because in a graph with a high throughput, there will be more augmented paths that increase flow, and that means the DFS method that finds these augmented paths will have to run more times.\nDepth-first search (DFS) has time complexity O(V+E)O(V+E).\nDFS runs once for every new augmented path. If we assume that each augmented graph increase flow by 1 unit, DFS must run ff times, as many times as the value of maximum flow.\nThis means that time complexity for the Ford-Fulkerson algorithm, using DFS, is\nO((V+E)⋅f) O( (V+E) \\cdot f )\nFor dense graphs, where E>V E > V , time complexity for DFS can be simplified to O(E)O(E), which means that the time complexity for the Ford-Fulkerson algorithm also can be simplified to\nO(E⋅f) O( E \\cdot f )\nA dense graph does not have an accurate definition, but it is a graph with many edges.\nThe next algorithm we will describe that finds maximum flow is the Edmonds-Karp algorithm.\nThe Edmonds-Karp algorithm is very similar to Ford-Fulkerson, but it uses BFS instead of DFS to find augmented paths, which leads to fewer iterations to find maximum flow.",
      "examples": [
        "class Graph: def __init__(self, size): self.adj_matrix = [[0] * size for _ in range(size)] self.size = size self.vertex_data = [''] * size def add_edge(self, u, v, c): self.adj_matrix[u][v] = c def add_vertex_data(self, vertex, data): if 0 <= vertex < self.size: self.vertex_data[vertex] = data",
        "def dfs(self, s, t, visited=None, path=None): if visited is None: visited = [False] * self.size if path is None: path = [] visited[s] = True path.append(s) if s == t: return path for ind, val in enumerate(self.adj_matrix[s]): if not visited[ind] and val > 0: result_path = self.dfs(ind, t, visited, path.copy()) if result_path: return result_path return None",
        "def fordFulkerson(self, source, sink): max_flow = 0 path = self.dfs(source, sink) while path: path_flow = float(\"Inf\") for i in range(len(path) - 1): u, v = path[i], path[i + 1] path_flow = min(path_flow, self.adj_matrix[u][v]) for i in range(len(path) - 1): u, v = path[i], path[i + 1] self.adj_matrix[u][v] -= path_flow self.adj_matrix[v][u] += path_flow max_flow += path_flow path_names = [self.vertex_data[node] for node in path] print(\"Path:\", \" -> \".join(path_names), \", Flow:\", path_flow) path = self.dfs(source, sink) return max_flow",
        "class Graph: def __init__(self, size): self.adj_matrix = [[0] * size for _ in range(size)] self.size = size self.vertex_data = [''] * size def add_edge(self, u, v, c): self.adj_matrix[u][v] = c def add_vertex_data(self, vertex, data): if 0 <= vertex < self.size: self.vertex_data[vertex] = data def dfs(self, s, t, visited=None, path=None): if visited is None: visited = [False] * self.size if path is None: path = [] visited[s] = True path.append(s) if s == t: return path for ind, val in enumerate(self.adj_matrix[s]): if not visited[ind] and val > 0: result_path = self.dfs(ind, t, visited, path.copy()) if result_path: return result_path return None def fordFulkerson(self, source, sink): max_flow = 0 path = self.dfs(source, sink) while path: path_flow = float(\"Inf\") for i in range(len(path) - 1): u, v = path[i], path[i + 1] path_flow = min(path_flow, self.adj_matrix[u][v]) for i in range(len(path) - 1): u, v = path[i], path[i + 1] self.adj_matrix[u][v] -= path_flow self.adj_matrix[v][u] += path_flow max_flow += path_flow path_names = [self.vertex_data[node] for node in path] print(\"Path:\", \" -> \".join(path_names), \", Flow:\", path_flow) path = self.dfs(source, sink) return max_flow g = Graph(6) vertex_names = ['s', 'v1', 'v2', 'v3', 'v4', 't'] for i, name in enumerate(vertex_names): g.add_vertex_data(i, name) g.add_edge(0, 1, 3) # s -> v1, cap: 3 g.add_edge(0, 2, 7) # s -> v2, cap: 7 g.add_edge(1, 3, 3) # v1 -> v3, cap: 3 g.add_edge(1, 4, 4) # v1 -> v4, cap: 4 g.add_edge(2, 1, 5) # v2 -> v1, cap: 5 g.add_edge(2, 4, 3) # v2 -> v4, cap: 3 g.add_edge(3, 4, 3) # v3 -> v4, cap: 3 g.add_edge(3, 5, 2) # v3 -> t, cap: 2 g.add_edge(4, 5, 6) # v4 -> t, cap: 6 source = 0; sink = 5 print(\"The maximum possible flow is %d \" % g.fordFulkerson(source, sink))",
        "0/7",
        "0",
        "7",
        "Graph",
        "adj_matrix",
        "size",
        "vertex_data",
        "add_edge",
        "u",
        "v",
        "c",
        "add_vertex_data",
        "vertex",
        "data",
        "dfs",
        "visited",
        "path",
        "s",
        "ind",
        "val",
        "None",
        "fordFulkerson",
        "max_flow",
        "while"
      ]
    },
    {
      "title": "DSA Edmonds-Karp Algorithm",
      "summary": "The Edmonds-Karp algorithm solves the maximum flow problem.\nFinding the maximum flow can be helpful in many areas: for optimizing network traffic, for manufacturing, for supply chain and logistics, or for airline scheduling.\nThe Edmonds-Karp Algorithm\nThe Edmonds-Karp algorithm solves the maximum flow problem for a directed graph.\nThe flow comes from a source vertex (ss) and ends up in a sink vertex (tt), and each edge in the graph allows a flow, limited by a capacity.\nThe Edmonds-Karp algorithm is very similar to the Ford-Fulkerson algorithm, except the Edmonds-Karp algorithm uses Breadth First Search (BFS) to find augmented paths to increase flow.\nMax flow: 0\nThe Edmonds-Karp algorithm works by using Breadth-First Search (BFS) to find a path with available capacity from the source to the sink (called an augmented path), and then sends as much flow as possible through that path.\nThe Edmonds-Karp algorithm continues to find new paths to send more flow through until the maximum flow is reached.\nIn the simulation above, the Edmonds-Karp algorithm solves the maximum flow problem: It finds out how much flow can be sent from the source vertex ss, to the sink vertex tt, and that maximum flow is 8.\nThe numbers in the simulation above are written in fractions, where the first number is the flow, and the second number is the capacity (maximum possible flow in that edge). So for example, 0/7 on edge s→v2s \\rightarrow v_2, means there is 0 flow, with a capacity of 7 on that edge.\nYou can see the basic step-by-step description of how the Edmonds-Karp algorithm works below, but we need to go into more detail later to actually understand it.\nHow it works:\nStart with zero flow on all edges.\nUse BFS to find an augmented path where more flow can be sent.\nDo a bottleneck calculation to find out how much flow can be sent through that augmented path.\nIncrease the flow found from the bottleneck calculation for each edge in the augmented path.\nRepeat steps 2-4 until max flow is found. This happens when a new augmented path can no longer be found.\nResidual Network in Edmonds-Karp\nThe Edmonds-Karp algorithm works by creating and using something called a residual network, which is a representation of the original graph.\nIn the residual network, every edge has a residual capacity, which is the original capacity of the edge, minus the the flow in that edge. The residual capacity can be seen as the leftover capacity in an edge with some flow.\nFor example, if there is a flow of 2 in the v3→v4 v_3 \\rightarrow v_4 edge, and the capacity is 3, the residual flow is 1 in that edge, because there is room for sending 1 more unit of flow through that edge.\nReversed Edges in Edmonds-Karp\nThe Edmonds-Karp algorithm also uses something called reversed edges to send flow back. This is useful to increase the total flow.\nTo send flow back, in the opposite direction of the edge, a reverse edge is created for each original edge in the network. The Edmonds-Karp algorithm can then use these reverse edges to send flow in the reverse direction.\nA reversed edge has no flow or capacity, just residual capacity. The residual capacity for a reversed edge is always the same as the flow in the corresponding original edge.\nIn our example, the edge v1→v3 v_1 \\rightarrow v_3 has a flow of 2, which means there is a residual capacity of 2 on the corresponding reversed edge v3→v1 v_3 \\rightarrow v_1 .\nThis just means that when there is a flow of 2 on the original edge v1→v3 v_1 \\rightarrow v_3 , there is a possibility of sending that same amount of flow back on that edge, but in the reversed direction. Using a reversed edge to push back flow can also be seen as undoing a part of the flow that is already created.\nThe idea of a residual network with residual capacity on edges, and the idea of reversed edges, are central to how the Edmonds-Karp algorithm works, and we will go into more detail about this when we implement the algorithm further down on this page.\nManual Run Through\nThere is no flow in the graph to start with.\nThe Edmonds-Karp algorithm starts with using Breadth-First Search to find an augmented path where flow can be increased, which is s→v1→v3→ts \\rightarrow v_1 \\rightarrow v_3 \\rightarrow t.\nAfter finding the augmented path, a bottleneck calculation is done to find how much flow can be sent through that path, and that flow is: 2.\nSo a flow of 2 is sent over each edge in the augmented path.\nThe next iteration of the Edmonds-Karp algorithm is to do these steps again: Find a new augmented path, find how much the flow in that path can be increased, and increase the flow along the edges in that path accordingly.\nThe next augmented path is found to be s→v1→v4→ts \\rightarrow v_1 \\rightarrow v_4 \\rightarrow t .\nThe flow can only be increased by 1 in this path because there is only room for one more unit of flow in the s→v1 s \\rightarrow v_1 edge.\nThe next augmented path is found to be s→v2→v4→ts \\rightarrow v_2 \\rightarrow v_4 \\rightarrow t.\nThe flow can be increased by 3 in this path. The bottleneck (limiting edge) is v2→v4 v_2 \\rightarrow v_4 because the capacity is 3.\nThe last augmented path found is s→v2→v1→v4→ts \\rightarrow v_2 \\rightarrow v_1 \\rightarrow v_4 \\rightarrow t.\nThe flow can only be increased by 2 in this path because of edge v4→t v_4 \\rightarrow t being the bottleneck in this path with only space for 2 more units of flow (capacity−flow=1capacity-flow=1).\nAt this point, a new augmenting path cannot be found (it is not possible to find a path where more flow can be sent through from ss to tt), which means the max flow has been found, and the Edmonds-Karp algorithm is finished.\nThe maximum flow is 8. As you can see in the image above, the flow (8) is the same going out of the source vertex ss, as the flow going into the sink vertex tt.\nAlso, if you take any other vertex than ss or tt, you can see that the amount of flow going into a vertex, is the same as the flow going out of it. This is what we call conservation of flow, and this must hold for all such flow networks (directed graphs where each edge has a flow and a capacity).\nImplementation of The Edmonds-Karp Algorithm\nTo implement the Edmonds-Karp algorithm, we create a Graph class.\nThe Graph represents the graph with its vertices and edges:\nLine 3: We create the adj_matrix to hold all the edges and edge capacities. Initial values are set to 0.\nLine 4: size is the number of vertices in the graph.\nLine 5: The vertex_data holds the names of all the vertices.\nLine 7-8: The add_edge method is used to add an edge from vertex u to vertex v, with capacity c.\nLine 10-12: The add_vertex_data method is used to add a vertex name to the graph. The index of the vertex is given with the vertex argument, and data is the name of the vertex.\nThe Graph class also contains the bfs method to find augmented paths, using Breadth-First-Search:\nLine 15-18: The visited array helps to avoid revisiting the same vertices during the search for an augmented path. The queue holds vertices to be explored, the search always starts with the source vertex s.\nLine 20-21: As long as there are vertices to be explored in the queue, take the first vertex out of the queue so that a path can be found from there to the next vertex.\nLine 23: For every adjacent vertex to the current vertex.\nLine 24-27: If the adjacent vertex is not visited yet, and there is a residual capacity on the edge to that vertex: add it to the queue of vertices that needs to be explored, mark it as visited, and set the parent of the adjacent vertex to be the current vertex u.\nThe parent array holds the parent of a vertex, creating a path from the sink vertex, backwards to the source vertex. The parent is used later in the Edmonds-Karp algorithm, outside the bfs method, to increase flow in the augmented path.\nLine 29: The last line returns visited[t], which is true if the augmented path ends in the sink node t. Returning true means that an augmenting path has been found.\nThe edmonds_karp method is the last method we add to the Graph class:\nInitially, the parent array holds invalid index values, because there is no augmented path to begin with, and the max_flow is 0, and the while loop keeps increasing the max_flow as long as there is an augmented path to increase flow in.\nLine 35: The outer while loop makes sure the Edmonds-Karp algorithm keeps increasing flow as long as there are augmented paths to increase flow along.\nLine 36-37: The initial flow along an augmented path is infinite, and the possible flow increase will be calculated starting with the sink vertex.\nLine 38-40: The value for path_flow is found by going backwards from the sink vertex towards the source vertex. The lowest value of residual capacity along the path is what decides how much flow can be sent on the path.\nLine 42: path_flow is increased by the path_flow.\nLine 44-48: Stepping through the augmented path, going backwards from sink to source, the residual capacity is decreased with the path_flow on the forward edges, and the residual capacity is increased with the path_flow on the reversed edges.\nLine 50-58: This part of the code is just for printing so that we are able to track each time an augmented path is found, and how much flow is sent through that path.\nAfter defining the Graph class, the vertices and edges must be defined to initialize the specific graph, and the complete code for the Edmonds-Karp algorithm example looks like this:\nExample\nPython:\nTime Complexity for The Edmonds-Karp Algorithm\nThe difference between Edmonds-Karp and Ford-Fulkerson is that Edmonds-Karp uses Breadth-First Search (BFS) to find augmented paths, while Ford-Fulkerson uses Depth-First Search (DFS).\nThis means that the time it takes to run Edmonds-Karp is easier to predict than Ford-Fulkerson, because Edmonds-Karp is not affected by the maximum flow value.\nWith the number of vertices VV, the number of edges EE, the time complexity for the Edmonds-Karp algorithm is\nO(V⋅E2) O(V \\cdot E^2)\nThis means Edmonds-Karp does not depend on the maximum flow, like Ford-Fulkerson does, but on how many vertices and edges we have.\nThe reason we get this time complexity for Edmonds-Karp is that it runs BFS which has time complexity O(E+V)O(E+V).\nBut if we assume a bad case scenario for Edmonds-Karp, with a dense graph, where the number of edges EE is much greater than the number of vertices VV, time complexity for BFS becomes O(E)O(E).\nBFS must run one time for every augmented path, and there can actually be found close to V⋅EV \\cdot E augmented paths during running of the Edmonds-Karp algorithm.\nSo, BFS with time complexity O(E)O(E) can run close to V⋅EV \\cdot E times in the worst case, which means we get a total time complexity for Edmonds-Karp: O(V⋅E⋅E)=O(V⋅E2) O(V \\cdot E \\cdot E) = O(V \\cdot E^2) .",
      "examples": [
        "class Graph: def __init__(self, size): self.adj_matrix = [[0] * size for _ in range(size)] self.size = size self.vertex_data = [''] * size def add_edge(self, u, v, c): self.adj_matrix[u][v] = c def add_vertex_data(self, vertex, data): if 0 <= vertex < self.size: self.vertex_data[vertex] = data",
        "def bfs(self, s, t, parent): visited = [False] * self.size queue = [] # Using list as a queue queue.append(s) visited[s] = True while queue: u = queue.pop(0) # Pop from the start of the list for ind, val in enumerate(self.adj_matrix[u]): if not visited[ind] and val > 0: queue.append(ind) visited[ind] = True parent[ind] = u return visited[t]",
        "def edmonds_karp(self, source, sink): parent = [-1] * self.size max_flow = 0 while self.bfs(source, sink, parent): path_flow = float(\"Inf\") s = sink while(s != source): path_flow = min(path_flow, self.adj_matrix[parent[s]][s]) s = parent[s] max_flow += path_flow v = sink while(v != source): u = parent[v] self.adj_matrix[u][v] -= path_flow self.adj_matrix[v][u] += path_flow v = parent[v] path = [] v = sink while(v != source): path.append(v) v = parent[v] path.append(source) path.reverse() path_names = [self.vertex_data[node] for node in path] print(\"Path:\", \" -> \".join(path_names), \", Flow:\", path_flow) return max_flow",
        "class Graph: def __init__(self, size): self.adj_matrix = [[0] * size for _ in range(size)] self.size = size self.vertex_data = [''] * size def add_edge(self, u, v, c): self.adj_matrix[u][v] = c def add_vertex_data(self, vertex, data): if 0 <= vertex < self.size: self.vertex_data[vertex] = data def bfs(self, s, t, parent): visited = [False] * self.size queue = [] # Using list as a queue queue.append(s) visited[s] = True while queue: u = queue.pop(0) # Pop from the start of the list for ind, val in enumerate(self.adj_matrix[u]): if not visited[ind] and val > 0: queue.append(ind) visited[ind] = True parent[ind] = u return visited[t] def edmonds_karp(self, source, sink): parent = [-1] * self.size max_flow = 0 while self.bfs(source, sink, parent): path_flow = float(\"Inf\") s = sink while(s != source): path_flow = min(path_flow, self.adj_matrix[parent[s]][s]) s = parent[s] max_flow += path_flow v = sink while(v != source): u = parent[v] self.adj_matrix[u][v] -= path_flow self.adj_matrix[v][u] += path_flow v = parent[v] path = [] v = sink while(v != source): path.append(v) v = parent[v] path.append(source) path.reverse() path_names = [self.vertex_data[node] for node in path] print(\"Path:\", \" -> \".join(path_names), \", Flow:\", path_flow) return max_flow # Example usage: g = Graph(6) vertex_names = ['s', 'v1', 'v2', 'v3', 'v4', 't'] for i, name in enumerate(vertex_names): g.add_vertex_data(i, name) g.add_edge(0, 1, 3) # s -> v1, cap: 3 g.add_edge(0, 2, 7) # s -> v2, cap: 7 g.add_edge(1, 3, 3) # v1 -> v3, cap: 3 g.add_edge(1, 4, 4) # v1 -> v4, cap: 4 g.add_edge(2, 1, 5) # v2 -> v1, cap: 5 g.add_edge(2, 4, 3) # v2 -> v4, cap: 3 g.add_edge(3, 4, 3) # v3 -> v4, cap: 3 g.add_edge(3, 5, 2) # v3 -> t, cap: 2 g.add_edge(4, 5, 6) # v4 -> t, cap: 6 source = 0; sink = 5 print(\"The maximum possible flow is %d \" % g.edmonds_karp(source, sink))",
        "0/7",
        "0",
        "7",
        "Graph",
        "adj_matrix",
        "size",
        "vertex_data",
        "add_edge",
        "u",
        "v",
        "c",
        "add_vertex_data",
        "vertex",
        "data",
        "bfs",
        "visited",
        "queue",
        "s",
        "parent",
        "visited[t]",
        "true",
        "t",
        "edmonds_karp",
        "max_flow",
        "while",
        "path_flow"
      ]
    },
    {
      "title": "DSA Time Complexity",
      "summary": "Runtime\nTo fully understand algorithms we must understand how to evaluate the time an algorithm needs to do its job, the runtime.\nExploring the runtime of algorithms is important because using an inefficient algorithm could make our program slow or even unworkable.\nBy understanding algorithm runtime we can choose the right algorithm for our need, and we can make our programs run faster and handle larger amounts of data effectively.\nActual Runtime\nWhen considering the runtime for different algorithms, we will not look at the actual time an implemented algorithm uses to run, and here is why.\nIf we implement an algorithm in a programming language, and run that program, the actual time it will use depends on many factors:\nthe programming language used to implement the algorithm\nhow the programmer writes the program for the algorithm\nthe compiler or interpreter used so that the implemented algorithm can run\nthe hardware on the computer the algorithm is running on\nthe operating system and other tasks going on on the computer\nthe amount of data the algorithm is working on\nWith all these different factors playing a part in the actual runtime for an algorithm, how can we know if one algorithm is faster than another? We need to find a better measure of runtime.\nTime Complexity\nTo evaluate and compare different algorithms, instead of looking at the actual runtime for an algorithm, it makes more sense to use something called time complexity.\nTime complexity is more abstract than actual runtime, and does not consider factors such as programming language or hardware.\nTime complexity is the number of operations needed to run an algorithm on large amounts of data. And the number of operations can be considered as time because the computer uses some time for each operation.\nFor example, in the algorithm that finds the lowest value in an array, each value in the array must be compared one time. Every such comparison can be considered an operation, and each operation takes a certain amount of time. So the total time the algorithm needs to find the lowest value depends on the number of values in the array.\nThe time it takes to find the lowest value is therefore linear with the number of values. 100 values results in 100 comparisons, and 5000 values results in 5000 comparisons.\nThe relationship between time and the number of values in the array is linear, and can be displayed in a graph like this:\n\"One Operation\"\nWhen talking about \"operations\" here, \"one operation\" might take one or several CPU cycles, and it really is just a word helping us to abstract, so that we can understand what time complexity is, and so that we can find the time complexity for different algorithms.\nOne operation in an algorithm can be understood as something we do in each iteration of the algorithm, or for each piece of data, that takes constant time.\nFor example: Comparing two array elements, and swapping them if one is bigger than the other, like the Bubble sort algorithm does, can be understood as one operation. Understanding this as one, two, or three operations actually does not affect the time complexity for Bubble sort, because it takes constant time.\nWe say that an operation takes \"constant time\" if it takes the same time regardless of the amount of data (nn) the algorithm is processing. Comparing two specific array elements, and swapping them if one is bigger than the other, takes the same time if the array contains 10 or 1000 elements.\nBig O Notation\nIn mathematics, Big O notation is used to describe the upper bound of a function.\nIn computer science, Big O notation is used more specifically to find the worst case time complexity for an algorithm.\nBig O notation uses a capital letter O with parenthesis O()O() , and inside the parenthesis there is an expression that indicates the algorithm runtime. Runtime is usually expressed using nn , which is the number of values in the data set the algorithm is working on.\nBelow are some examples of Big O notation for different algorithms, just to get the idea:\nBubble sort, Selection sort and Insertion sort are algorithms with this time complexity. The reason for their time complexities are explained on the pages for these algorithms.\nLarge data sets slows down these algorithms significantly. With just an increase in nn from 100 to 200 values, the number of operations can increase by as much as 30000!\nHere is how time increases when the number of values nn increase for different algorithms:\nBest, Average and Worst Case\n'Worst case' time complexity has already been mentioned when explaining Big O notation, but how can an algorithm have a worst case scenario?\nThe algorithm that finds the lowest value in an array with nn values requires nn operations to do so, and that is always the same. So this algorithm has the same best, average, and worst case scenarios.\nBut for many other algorithms we will look at, if we keep the number of values nn fixed, the runtime can still change a lot depending on the actual values.\nWithout going into all the details, we can understand that a sorting algorithm can have different runtimes, depending on the values it is sorting.\nJust imagine you have to sort 20 values manually from lowest to highest:\nThis will take you some seconds to finish.\nNow, imagine you have to sort 20 values that are almost sorted already:\nYou can sort the values really fast, by just moving 20 to the end of the list and you are done, right?\nAlgorithms work similarly: For the same amount of data they can sometimes be slow and sometimes fast. So to be able to compare different algorithms' time complexities, we usually look at the worst-case scenario using Big O notation.\nBig O Explained Mathematically\nDepending on your background in Mathematics, this section might be hard to understand. It is meant to create a more solid mathematical basis for those who need Big O explained more thoroughly.\nIf you do not understand this now, don't worry, you can come back later. And if the math here is way over your head, don't worry too much about it, you can still enjoy the different algorithms in this tutorial, learn how to program them, and understand how fast or slow they are.\nIn Mathematics, Big O notation is used to create an upper bound for a function, and in Computer Science, Big O notation is used to describe how the runtime of an algorithm increases when the number of data values nn increase.\nFor example, consider the function:\nf(n)=0.5n3−0.75n2+1f(n) = 0.5n^3 -0.75n^2+1\nThe graph for the function ff looks like this:\nConsider another function:\ng(n)=n3g(n) = n^3\nWhich we can draw like this:\nUsing Big O notation we can say that O(g(n))O(g(n)) is an upper bound for f(n)f(n) because we can choose a constant CC so that C⋅g(n)>f(n)C \\cdot g(n)>f(n) as long as nn is big enough.\nOk, let's try. We choose C=0.75C=0.75 so that C⋅g(n)=0.75⋅n3C \\cdot g(n) = 0.75 \\cdot n^3.\nNow let's draw 0.75⋅g(n)0.75 \\cdot g(n) and f(n)f(n) in the same plot:\nWe can see that O(g(n))=O(n3)O(g(n))=O(n^3) is the upper bound for f(n)f(n) because 0.75⋅g(n)>f(n)0.75 \\cdot g(n) > f(n) for all nn larger than 1.\nIn the example above nn must be larger than 1 for O(n3)O(n^3) to be an upper bound. We call this limit n0n_0.\nDefinition\nLet f(n)f(n) and g(n)g(n) be two functions. We say that f(n)f(n) is O(g(n))O(g(n)) if and only if there are positive constants CC and n0n_0 such that\nC⋅g(n)>f(n) C \\cdot g(n) > f(n)\nfor all n>n0n>n_0.\nWhen evaluating time complexity for an algorithm, it is ok that O()O() is only true for a large number of values nn, because that is when time complexity becomes important. To put it differently: if we are sorting, 10, 20 or 100 values, the time complexity for the algorithm is not so interesting, because the computer will sort the values in a short time anyway.",
      "examples": [
        "print( my_array[97] )",
        "8, 16, 19, 15, 2, 17, 4, 11, 6, 1, 7, 13, 5, 3, 9, 12, 14, 20, 18, 10",
        "1, 2, 3, 4, 5, 20, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19"
      ]
    },
    {
      "title": "DSA Bubble Sort Time Complexity",
      "summary": "See the previous page for a general explanation of what time complexity is.\nBubble Sort Time Complexity\nThe Bubble Sort algorithm goes through an array of nn values n−1n-1 times in a worst case scenario.\nThe first time the algorithm runs through the array, every value is compared to the next, and swaps the values if the left value is larger than the right. This means that the highest value bubbles up, and the unsorted part of the array becomes shorter and shorter until the sorting is done. So on average, n2\\frac{n}{2} elements are considered when the algorithm goes through the array comparing and swapping values.\nWe can start calculating the number of operations done by the Bubble Sort algorithm on nn values:\nOperations=(n−1)⋅n2=n22−n2Operations = (n-1)\\cdot \\frac{n}{2} = \\frac{n^2}{2} - \\frac{n}{2}\nWhen looking at the time complexity for algorithms, we look at very large data sets, meaning nn is a very big number. And for a very big number nn, the term n22\\frac{n^2}{2} becomes a lot bigger than the term n2\\frac{n}{2}. So large in fact, that we can approximate by simply removing that second term n2\\frac{n}{2}.\nOperations=n22−n2≈n22=12⋅n2Operations = \\frac{n^2}{2} - \\frac{n}{2} \\approx \\frac{n^2}{2} = \\frac{1}{2} \\cdot n^2\nWhen we are looking at time complexity like we are here, using Big O notation, factors are disregarded, so factor 12\\frac{1}{2} is omitted. This means that the run time for the Bubble Sort algorithm can be described with time complexity, using Big O notation like this:\nO(12⋅n2)=O(n2)__ O( \\frac{1}{2} \\cdot n^2) = \\underline{\\underline{O(n^2)}}\nAnd the graph describing the Bubble Sort time complexity looks like this:\nAs you can see, the run time increases really fast when the size of the array is increased.\nLuckily there are sorting algorithms that are faster than this, like Quicksort.\nBubble Sort Simulation\nChoose the number of values in an array, and run this simulation to see how the number of operations Bubble Sort needs on an array of nn elements is O(n2)O(n^2):\nSet values: 300\nRandom\nWorst Case\nBest Case\n10 Random\nOperations: 0\nThe red line above represents the upper bound time complexity O(n2)O(n^2), and the actual function in this case is 1.05⋅n21.05 \\cdot n^2.\nA function f(n)f(n) is said to be O(g(n))O(g(n)) if we have a positive constant CC so that C⋅g(n)>f(n)C \\cdot g(n)>f(n) for a large number of values nn.\nIn this case f(n)f(n) is the number of operations used by Buble Sort, g(n)=n2g(n)=n^2 and C=1.05C=1.05.\nRead more about Big O notation and time complexity on this page.",
      "examples": []
    },
    {
      "title": "DSA Selection Sort Time Complexity",
      "summary": "See this page for a general explanation of what time complexity is.\nSelection Sort Time Complexity\nThe Selection Sort algorithm goes through all elements in an array, finds the lowest value, and moves it to the front of the array, and does this over and over until the array is sorted.\nSelection Sort goes through an array of nn values n−1n-1 times. This is because when the algorithm has sorted all values except the last, the last value must also be in its correct place.\nThe first time the algorithm runs through the array, every value is compared to find out which one is the lowest.\nThe second time the algorithm runs through the array, every value except the first value is compared to find out which is the lowest.\nAnd in this way the unsorted part of the array becomes shorter and shorter until the sorting is done. So on average, n2\\frac{n}{2} elements are considered when the algorithm goes through the array finding the lowest value and moving it to the front of the array.\nIn addition to all the compares needed, the number of swap considerings needed is n−1n -1 .\nWe can start calculating the number of operations for the Selection Sort algorithm:\nOperations=(n−1)⋅n2+(n−1)=n22−n2+(n−1)=n22+n2 \\begin{equation} \\begin{aligned} Operations {} & = (n-1)\\cdot \\frac{n}{2} + (n - 1) \\\\ & = \\frac{n^2}{2} - \\frac{n}{2} + (n - 1) \\\\ & = \\frac{n^2}{2} + \\frac{n}{2} \\end{aligned} \\end{equation}\nWhen looking at the run time for algorithms, we look at very large data sets, meaning nn is a very big number. And for a very big number nn, the term n22\\frac{n^2}{2} becomes so much bigger than the term n2\\frac{n}{2} that we can approximate by simply removing that second term n2\\frac{n}{2}.\nOperations=n22+n2≈n22=12⋅n2Operations = \\frac{n^2}{2} + \\frac{n}{2} \\approx \\frac{n^2}{2} = \\frac{1}{2} \\cdot n^2\nUsing Big O notation to describe the time complexity for the Selection Sort algorithm, we get:\nO(12⋅n2)=O(n2)__ O( \\frac{1}{2} \\cdot n^2) = \\underline{\\underline{O(n^2)}}\nAnd time complexity for the Selection Sort algorithm can be displayed in a graph like this:\nAs you can see, the run time is the same as for Bubble Sort: The run time increases really fast when the size of the array is increased.\nSelection Sort Simulation\nRun the simulation for different number of values in an array, and see how the number of operations Selection Sort needs on an array of nn elements is O(n2)O(n^2):\nSet values: 300\nRandom\nWorst Case\nBest Case\n10 Random\nOperations: 0\nThe most significant difference from Bubble sort that we can notice in this simulation is that best and worst case is actually almost the same for Selection Sort (O(n2)O(n^2)), but for Bubble Sort the best case runtime is only O(n)O(n).\nThe difference in best and worst case for Selection Sort is mainly the number of swaps. In the best case scenario Selection Sort does not have to swap any of the values because the array is already sorted. And in the worst case scenario, where the array already sorted, but in the wrong order, so Selection Sort must do as many swaps as there are values in the array.",
      "examples": []
    },
    {
      "title": "DSA Insertion Sort Time Complexity",
      "summary": "See this page for a general explanation of what time complexity is.\nInsertion Sort Time Complexity\nThe worst case scenario for Insertion Sort is if the array is already sorted, but with the highest values first. That is because in such a scenario, every new value must \"move through\" the whole sorted part of the array.\nThese are the operations that are done by the Insertion Sort algorithm for the first elements:\nThe 1st value is already in the correct position.\nThe 2nd value must be compared and moved past the 1st value.\nThe 3rd value must be compared and moved past two values.\nThe 3rd value must be compared and moved past three values.\nAnd so on..\nIf we continue this pattern, we get the total number of operations for nn values:\n1+2+3+...+(n−1)1+2+3+...+(n-1)\nThis is a well known series in mathematics that can be written like this:\nn(n−1)2=n22−n2 \\frac{n(n-1)}{2} = \\frac{n^2}{2} - \\frac{n}{2}\nFor very large nn, the n22\\frac{n^2}{2} term dominates, so we can simplify by removing the second term n2\\frac{n}{2}.\nUsing Big O notation, we get this time complexity for the Insertion Sort algorithm:\nO(n22)=O(12⋅n2)=O(n2)__ O(\\frac{n^2}{2}) = O(\\frac{1}{2} \\cdot n^2) = \\underline{\\underline{O(n^2)}}\nThe time complexity can be displayed like this:\nAs you can see, the time used by Insertion Sort increases fast when the number of values is nn increased.\nInsertion Sort Simulation\nUse the simulation below to see how the theoretical time complexity O(n2)O(n^2) (red line) compares with the number of operations of actual Insertion Sorts.\nSet values: 300\nRandom\nWorst Case\nBest Case\n10 Random\nOperations: 0\nFor Insertion Sort, there is a big difference between best, average and worst case scenarios. You can see that by running the different simulations above.\nThe red line above represents the theoretical upper bound time complexity O(n2)O(n^2), and the actual function in this case is 1.07⋅n21.07 \\cdot n^2.\nRemember that a function f(n)f(n) is said to be O(g(n))O(g(n)) if we have a positive constant CC so that C⋅g(n)>f(n)C \\cdot g(n)>f(n).\nIn this case f(n)f(n) is the number of operations used by Insertion Sort, g(n)=n2g(n)=n^2 and C=1.07C=1.07.",
      "examples": []
    },
    {
      "title": "DSA Time Complexity for Specific Algorithms",
      "summary": "See this page for a general explanation of what time complexity is.\nQuicksort Time Complexity\nThe Quicksort algorithm chooses a value as the 'pivot' element, and moves the other values so that higher values are on the right of the pivot element, and lower values are on the left of the pivot element.\nThe Quicksort algorithm then continues to sort the sub-arrays on the left and right side of the pivot element recursively until the array is sorted.\nWorst Case\nTo find the time complexity for Quicksort, we can start by looking at the worst case scenario.\nThe worst case scenario for Quicksort is if the array is already sorted. In such a scenario, there is only one sub-array after each recursive call, and new sub-arrays are only one element shorter than the previous array.\nThis means that Quicksort must call itself recursively nn times, and each time it must do n2\\frac{n}{2} comparisons on average.\nWorst case time complexity is:\nO(n⋅n2)=O(n2)__ O(n \\cdot \\frac{n}{2}) = \\underline{\\underline{O(n^2)}}\nAverage Case\nOn average, Quicksort is actually much faster.\nQuicksort is fast on average because the array is split approximately in half each time Quicksort runs recursively, and so the size of the sub-arrays decrease really fast, which means that not so many recursive calls are needed, and Quicksort can finish sooner than in the worst case scenario.\nThe image below shows how an array of 23 values is split into sub-arrays when sorted with Quicksort.\nThe pivot element (green) is moved into the middle, and the array is split into sub-arrays (yellow). There are 5 recursion levels with smaller and smaller sub-arrays, where about nn values are touched somehow on each level: compared, or moved, or both.\nlog2 \\log_2 tells us how many times a number can be split in 2, so log2 \\log_2 is a good estimate for how many levels of recursions there are. log2(23)≈4.5 \\log_2(23) \\approx 4.5 which is a good enough approximation of the number of recursion levels in the specific example above.\nIn reality, the sub-arrays are not split exactly in half each time, and there are not exactly nn values compared or moved on each level, but we can say that this is the average case to find the time complexity:\nO(n⋅log2n)__ \\underline{\\underline{O(n \\cdot \\log_2n)}}\nBelow you can see the significant improvement in time complexity for Quicksort in an average scenario, compared to the previous sorting algorithms Bubble, Selection and Insertion Sort:\nThe recursion part of the Quicksort algorithm is actually a reason why the average sorting scenario is so fast, because for good picks of the pivot element, the array will be split in half somewhat evenly each time the algorithm calls itself. So the number of recursive calls do not double, even if the number of values nn double.\nQuicksort Simulation\nUse the simulation below to see how the theoretical time complexity O(n2)O(n^2) (red line) compares with the number of operations of actual Quicksort runs.\nSet values: 300\nRandom\nDescending\nAscending\n10 Random\nOperations: 0\nThe red line above represents the theoretical upper bound time complexity O(n2)O(n^2) for the worst case scenario, and the green line represents the average case scenario time complexity with random values O(nlog2n)O(n \\log_2n).\nFor Quicksort, there is a big difference between average random case scenarios and scenarios where arrays are already sorted. You can see that by running the different simulations above.\nThe reason why the already ascending sorted array needs so many operations is that it requires the most swapping of elements, because of the way it is implemented. In this case, the last element is chosen as the pivot element, and the last element is also the highest number. So all other values in every sub-array is swapped around to land on the left side of the pivot element (where they are positioned already).",
      "examples": []
    },
    {
      "title": "DSA Counting Sort Time Complexity",
      "summary": "See this page for a general explanation of what time complexity is.\nCounting Sort Time Complexity\nCounting Sort works by first counting the occurrence of different values, and then uses that to recreate the array in a sorted order.\nAs a rule of thumb, the Counting Sort algorithm runs fast when the range of possible values kk is smaller than the number of values nn.\nTo represent the time complexity with Big O notation we need to first count the number of operations the algorithm does:\nFinding the maximum value: Every value must be evaluated once to find out if it is the maximum value, so nn operations are needed.\nInitializing the counting array: With kk as the maximum value in the array, we need k+1k+1 elements in the counting array to include 0. Every element in the counting array must be initialized, so k+1k+1 operations are needed.\nEvery value we want to sort is counted once, then removed, so 2 operations per count, 2⋅n2 \\cdot n operations in total.\nBuilding the sorted array: Create nn elements in the sorted array: nn operations.\nIn total we get:\nOperations=n+(k+1)+(2⋅n)+n=4⋅n+k+1≈4⋅n+k \\begin{equation} \\begin{aligned} Operations {} & = n + (k+1) + (2 \\cdot n) + n \\\\ & = 4 \\cdot n + k + 1 \\\\ & \\approx 4 \\cdot n + k \\end{aligned} \\end{equation}\nBased on what have seen about time complexity earlier, we can create a simplified expression using Big O notiation to represent time complexity:\nO(4⋅n+k)=O(4⋅n)+O(k)=O(n)+O(k)=O(n+k)__ \\begin{equation} \\begin{aligned} O(4 \\cdot n + k) {} & = O(4 \\cdot n) + O(k) \\\\ & = O(n) + O(k) \\\\ & = \\underline{\\underline{O(n+k)}} \\end{aligned} \\end{equation}\nIt has already been mentioned that Counting Sort is effective when the range of different values kk is relatively small compared to the total number of values to be sorted nn. We can now see this directly from the Big O expression O(n+k)O(n+k).\nJust imagine for example that the range of different numbers kk is 10 times as large as the number of the values to be sorted. In such a case we can see that the algorithm uses most of its time on handling the range of different numbers kk, although the actual number of values to be sorted nn is small in comparison.\nIt is not straight forward to show the time complexity for Counting Sort in a graph, or to have a simulation for the time complexity as we have had for the previous algorithms, because the time complexity is so greatly affected by the range of values kk.\nBelow is a plot that shows how much the time complexity for Counting Sort can vary, followed by an explanation for the best and worst case scenarios.\nThe best case scenario for Counting Sort would be to have the range kk just a fraction of nn, let's say k(n)=0.1⋅nk(n)=0.1 \\cdot n. As an example of this, for 100 values, the range would be from 0 to 10, or for 1000 values the range would be from 0 to 100. In this case we get time complexity O(n+k)=O(n+0.1⋅n)=O(1.1⋅n)O(n+k)=O(n+0.1 \\cdot n) = O(1.1 \\cdot n) which is simplified to O(n)O(n).\nThe worst case however would be if the range is a lot larger than the input. Let's say for an input of just 10 values the the range is between 0 and 100, or similarly, for an input of 1000 values, the range is between 0 and 1000000. In such a scenario, the growth of kk is quadratic with respect to nn, like this: k(n)=n2k(n)=n^2, and we get time complexity O(n+k)=O(n+n2)O(n+k)=O(n+n^2) which is simplified to O(n2)O(n^2). A case that is even worse than this could also be constructed, but this case is chosen because it is relatively easy to understand, and perhaps not that unrealistic either.\nAs you can see, it is important to consider the range of values compared to the number of values to be sorted before choosing Counting Sort as your algorithm. Also, as mentioned at the top of the page, keep in mind that Counting sort only works for non negative integer values.\nCounting Sort Simulation\nRun different simulations of Counting Sort to see how the number of operations falls between the worst case scenario O(n2)O(n^2) (red line) and best case scenario O(n)O(n) (green line).\nSet values (n): 300\nRange (k), from 0 to: 1000\nRandom\nDescending\nAscending\n10 Random\nOperations: 0\nAs mentioned previously: if the numbers to be sorted varies a lot in value (large kk), and there are few numbers to sort (small nn), the Counting Sort algorithm is not effective.\nIf we hold nn and kk fixed, the \"Random\", \"Descending\" and \"Ascending\" alternatives in the simulation above results in the same number of operations. This is because the same thing happens in all three cases: A counting array is set up, the numbers are counted, and the new sorted array is created.",
      "examples": []
    },
    {
      "title": "DSA Radix Sort Time Complexity",
      "summary": "See this page for a general explanation of what time complexity is.\nRadix Sort Time Complexity\nThe Radix Sort algorithm sorts non negative integers, one digit at a time.\nThere are nn values that need to be sorted, and kk is the number of digits in the highest value.\nWhen Radix Sort runs, every value is moved to the radix array, and then every value is moved back into the initial array. So nn values are moved into the radix array, and nn values are moved back. This gives us n+n=2⋅nn + n=2 \\cdot n operations.\nAnd the moving of values as described above needs to be done for every digit. This gives us a total of 2⋅n⋅k2 \\cdot n \\cdot k operations.\nThis gives us time complexity for Radix Sort:\nO(2⋅n⋅k)=O(n⋅k)__ O(2 \\cdot n \\cdot k) = \\underline{\\underline{O(n \\cdot k)}}\nRadix Sort is perhaps the fastest sorting algorithms there is, as long as the number of digits kk is kept relatively small compared to the number of values nn.\nWe can imagine a scenario where the number of digits kk is the same as the number of values nn, in such a case we get time complexity O(n⋅k)=O(n2)O(n \\cdot k)=O(n^2) which is quite slow, and has the same time complexity as for example Bubble Sort.\nWe can also image a scenario where the number of digits kk grow as the number of values nn grow, so that k(n)=lognk(n)= \\log n. In such a scenario we get time complexity O(n⋅k)=O(n⋅logn)O(n \\cdot k)=O(n \\cdot \\log n ), which is the same as for example Quicksort.\nSee the time complexity for Radix Sort in the image below.\nRadix Sort Simulation\nRun different simulations of Radix Sort to see how the number of operations falls between the worst case scenario O(n2)O(n^2) (red line) and best case scenario O(n)O(n) (green line).\nSet values (n): 300\nDigits (k): 4\nRandom\nDescending\nAscending\n10 Random\nOperations: 0\nThe bars representing the different values are scaled to fit the window, so that it looks ok. This means that values with 7 digits look like they are just 5 times bigger than values with 2 digits, but in reality, values with 7 digits are actually 5000 times bigger than values with 2 digits!\nIf we hold nn and kk fixed, the \"Random\", \"Descending\" and \"Ascending\" alternatives in the simulation above results in the same number of operations. This is because the same thing happens in all three cases.",
      "examples": []
    },
    {
      "title": "DSA Merge Sort Time Complexity",
      "summary": "See this page for a general explanation of what time complexity is.\nMerge Sort Time Complexity\nThe Merge Sort algorithm breaks the array down into smaller and smaller pieces.\nThe array becomes sorted when the sub-arrays are merged back together so that the lowest values come first.\nThe array that needs to be sorted has nn values, and we can find the time complexity by start looking at the number of operations needed by the algorithm.\nThe main operations Merge Sort does is to split, and then merge by comparing elements.\nTo split an array from start until the sub-arrays only consists of one value, Merge Sort does a total of n−1n-1 splits. Just imaging an array with 16 values. It is split one time into sub-arrays of length 8, split again and again, and the size of the sub-arrays reduces to 4, 2 and finally 1. The number of splits for an array of 16 elements is 1+2+4+8=151+2+4+8=15.\nThe image below shows that 15 splits are needed for an array of 16 numbers.\nThe number of merges is actually also n−1n-1, the same as the number of splits, because every split needs a merge to build the array back together. And for each merge there is a comparison between values in the sub-arrays so that the merged result is sorted.\nDuring merging of two sub-arrays, the worst case scenario that generates the most comparisons, is if the sub-arrays are equally big. Just consider merging [1,4,6,9] and [2,3,7,8]. In this case the following comparisons are needed:\nComparing 1 and 2, result: [1]\nComparing 4 and 2, result: [1,2]\nComparing 4 and 3, result: [1,2,3]\nComparing 4 and 7, result: [1,2,3,4]\nComparing 6 and 7, result: [1,2,3,4,6]\nComparing 9 and 7, result: [1,2,3,4,6,7]\nComparing 9 and 8, result: [1,2,3,4,6,7,8]\nAt the end of the merge, only the value 9 is left in one array, the other array is empty, so no comparison is needed to put the last value in, and the resulting merged array is [1,2,3,4,6,7,8,9]. We see that we need 7 comparisons to merge 8 values (4 values in each of the initial sub-arrays). In general, in a worst case scenario, n−1n-1 comparisons are needed to get a merged array of nn values.\nFor simplicity, let's say that we need nn comparisons instead of n−1n-1 when merging nn values. This is an ok assumption when nn is large and we want to calculate an upper bound using Big O notation.\nSo, at each level merging happens, nn comparisons are needed, but how many levels are there? Well, for n=16n=16 we have n=16=24n=16=2^4, so 4 levels of merging. For n=32=25n=32=2^5 there are 5 levels of merging, and at each level, nn comparisons are needed. For n=1024=210n=1024=2^{10} 10 levels of merging is needed. To find out what power of 2 gives us 1024, we use a base-2 logarithm. The answer is 10. Mathematically it looks like this: log21024=10 \\log_{2}1024=10.\nAs we can see from the figure above, nn comparisons are needed on each level, and there are log2n \\log_{2}n levels, so there are n⋅log2n n \\cdot \\log_{2}n comparison operations in total.\nTime complexity can be calculated based on the number of split operations and the number of merge operations:\nO((n−1)+n⋅log2n)=O(n⋅log2n) \\begin{equation} \\begin{aligned} O( (n-1) + n \\cdot \\log_{2}n) {} & = O( n \\cdot \\log_{2}n ) \\end{aligned} \\end{equation}\nThe number of splitting operations (n−1)(n-1) can be removed from the Big O calculation above because n⋅log2n n \\cdot \\log_{2}n will dominate for large n n, and because of how we calculate time complexity for algorithms.\nThe figure below shows how the time increases when running Merge Sort on an array with nn values.\nThe difference between best and worst case scenarios for Merge Sort is not as big as for many other sorting algorithms.\nMerge Sort Simulation\nRun the simulation for different number of values in an array, and see how the number of operations Merge Sort needs on an array of nn elements is O(nlogn)O(n \\log n):\nSet values: 300\nRandom\nDescending\nAscending\n10 Random\nOperations: 0\nIf we hold the number of values nn fixed, the number of operations needed for the \"Random\", \"Descending\" and \"Ascending\" is almost the same.\nMerge Sort performs almost the same every time because the array is split, and merged using comparison, both if the array is already sorted or not.",
      "examples": []
    },
    {
      "title": "DSA Linear Search Time Complexity",
      "summary": "See this page for a general explanation of what time complexity is.\nLinear Search Time Complexity\nFor a general explanation of what time complexity is, visit this page.\nFor a more thorough and detailed explanation of Insertion Sort time complexity, visit this page.\nLinear Search compares each value with the value it is looking for. If the value is found, the index is returned, and if it is not found -1 is returned.\nTo find the time complexity for Linear Search, let's see if we can fins out how many compare operations are needed to find a value in an array with nn values.\nBest Case Scenario is if the value we are looking for is the first value in the array. In such a case only one compare is needed and the time complexity is O(1)O(1).\nWorst Case Scenario is if the whole array is looked through without finding the target value. In such a case all values in the array are compared with the target value, and the time complexity is O(n)O(n).\nAverage Case Scenario is not so easy to pinpoint. What is the possibility to finding the target value? That depends on the values in the array right? But if we assume that exactly one of the values in the array is equal to the target value, and that the position of that value can be anywhere, the average time needed for Linear Search is half of the time time needed in the worst case scenario.\nTime complexity for Linear Search is O(n)O(n).\nIf we draw how much time Linear Search needs to find a value in an array of nn values, we get this graph:\nLinear Search Simulation\nRun the simulation for different number of values in an array, and see how many compares are needed for Linear Search to find a value in an array of nn values:\nSet values: 300\nRandom\nDescending\nAscending\n10 Random\nOperations: 0\nNot found!\nAs you can see when running simulations of Linear Search, the search requires few compares if the value is found fast, but if the value we are looking for is not found, the maximum of compares are done.",
      "examples": []
    },
    {
      "title": "DSA Selection Sort Time Complexity",
      "summary": "See this page for a general explanation of what time complexity is.\nBinary Search Time Complexity\nBinary Search finds the target value in an already sorted array by checking the center value. If the center value is not the target value, Linear Search selects the left or right sub-array and continues the search until the target value is found.\nTo find the time complexity for Binary Search, let's see how many compare operations are needed to find the target value in an array with nn values.\nThe best case scenario is if the first middle value is the same as the target value. If this happens the target value is found straight away, with only one compare, so the time complexity is O(1)O(1) in this case.\nThe worst case scenario is if the search area must be cut in half over and over until the search area is just one value. When this happens, it does not affect the time complexity if the target value is found or not.\nLet's consider array lengths that are powers of 2, like 2, 4, 8, 16, 32 64 and so on.\nHow many times must 2 be cut in half until we are looking at just one value? It is just one time, right?\nHow about 8? We must cut an array of 8 values in half 3 times to arrive at just one value.\nAn array of 32 values must be cut in half 5 times.\nWe can see that 2=212=2^1, 8=238=2^3 and 32=2532=2^5. So the number of times we must cut an array to arrive at just one element can be found in the power with base 2. Another way to look at it is to ask \"how many times must I multiply 2 with itself to arrive at this number?\". Mathematically we can use the base-2 logarithm, so that we can find out that an array of length nn can be split in half log2(n) \\log_{2}(n) times.\nThis means that time complexity for Binary Search is\nO(log2n)__ \\underline{\\underline{O( \\log_{2} n )}}\nThe average case scenario is not so easy to pinpoint, but since we understand time complexity of an algorithm as the upper bound of the worst case scenario, using Big O notation, the average case scenario is not that interesting.\nNote: Time complexity for Binary Search O(log2n)O( \\log_{2}n) is a lot faster than Linear Search O(n)O(n), but it is important to remember that Binary Search requires a sorted array, and Linear Search does not.\nIf we draw how much time Binary Search needs to find a value in an array of nn values, compared to Linear Search, we get this graph:\nBinary Search Simulation\nRun the simulation for different number of values nn in an array, and see how many compares are needed for Binary Search to find the target value:\nSet values: 300\nAscending\n10 Ascending\nRandom\nDescending\nOperations: 0\nNot found!\nAs you can see when running simulations of Binary Search, the search requires very few compares, even if the the array is big and the value we are looking for is not found.",
      "examples": []
    },
    {
      "title": "The Euclidean Algorithm",
      "summary": "Named after the ancient Greek mathematician Euclid, the Euclidean algorithm is the oldest known non-trivial algorithm, described in Euclid's famous book \"Elements\" from 300 BCE.\nThe Euclidean Algorithm\nThe Euclidean algorithm finds the greatest common divisor (gcd) of two numbers aa and bb.\nThe greatest common divisor is the largest number that divides both aa and bb without leaving a remainder.\nFinding the greatest common divisor using division.\na=a=50\nb=b=15\nResult:\nThe algorithm uses division with remainder. It takes the remainder from the previous step to set up the calculation in the next step.\nHow it works:\nStart with the two initial numbers aa and bb.\nDo a division with remainder: a=q0⋅b+r0a=q_0 \\cdot b + r_0\nUse the remainder (r0r_0) and the divisor (bb) from the last calculation to set up the next calculation: b=q1⋅r0+r1b=q_1 \\cdot r_0 + r_1\nRepeat steps 2 and 3 until the remainder is 00.\nThe second last remainder calculated is the greatest common divisor.\nContinue reading to see how the Euclidean algorithm can be done by hand, with programming, and to understand how and why the algorithm actually works.\nMathematical Terminology\nBelow are words used to describe the Euclidean Algorithm that you need to know to understand the explanations on this page.\nDivisor: A number we can use to divide a number by, without leaving a remainder. We say that 3 is a divisor of 6 because 6/3=26/3=2, without leaving a remainder (the remainder is 0).\nRemainder: The part you are left with after dividing a number with another number. Dividing 7 by 3 is 2, with a remainder of 1. (So 3 is not a divisor of 7.)\nCommon divisor: For numbers aa and bb, a common divisor is a number that can divide both aa and bb without leaving a remainder. The common divisors of 18 and 12 are 2, 3, and 6, because both 18 and 12 can be divided by 2, 3, and 6 without producing a remainder.\nGreatest common divisor: The largest of the common divisors. The greatest common divisor of 18 and 12 is 6 because that is the largest of the common divisors 2, 3, and 6.\nThe greatest common divisor is used in the mathematical field of Number Theory, and in cryptography for encrypting messages.\nNote: All numbers used by the Euclidean algorithm are integers.\nManual Run Through\nTo understand how the Euclidean algorithm works, and to write the code for it, let's first run it manually to find the greatest common divisor of 120120 and 2525.\nTo do this we use division with remainder.\nStep 1: We start with dividing 120120 with 2525:\n120=4⋅25+20 \\begin{equation} \\begin{aligned} 120 & = 4 \\cdot 25 + 20 \\end{aligned} \\end{equation}\nHow many times can we fit 2525 inside 120120? It is 44 times, right? 4⋅254 \\cdot 25 is 100100. We get the remainder 2020 by subtracting 100100 from 120120.\nStep 2: We use the previous remainder 2020 in the next step to divide 2525:\n25=1⋅20+5 \\begin{equation} \\begin{aligned} 25 & = 1 \\cdot 20 + 5 \\end{aligned} \\end{equation}\nWe can fit 2020 inside 2525 one time. We get the remainder 55 by subtracting 2020 from 2525.\nStep 3: In the next calculation we divide 2020 with the previous remainder 55:\n20=4⋅5+0 \\begin{equation} \\begin{aligned} 20 & = 4 \\cdot 5 + 0 \\end{aligned} \\end{equation}\nWe get 00 as the remainder, and that means that we are done with the calculations.\nThe greatest common divisor of 120120 and 2525 is 55.\nImplementation of The Euclidean Algorithm\nTo find the greatest common divisor using division, we continue running the algorithm until the remainder calculated is 00.\nThis is the same as saying we continue to run the algorithm as long as bb is not 00. That is why b != 0 is the condition in the while loop below.\nExample\nFinding the greatest common divisor of 120 and 25 using the Euclidean algorithm:\nThe Original Euclidean Algorithm\nInstead of using division like we did above, the original Euclidean algorithm as described in the book \"Elements\" over 2000 years ago uses subtraction.\nFinding the greatest common divisor using subtraction.\na=a=50\nb=b=15\nResult:\nHow the Euclidean algorithm with subtraction works:\nStart with the two initial numbers aa and bb.\nFind the difference a−b=c a-b=c. The difference cc shares the same greatest common divisor as aa and bb.\nTake the two lowest numbers of aa, bb, and cc, and find the difference between them.\nRepeat steps 2 and 3 until the difference is 00.\nThe second last difference calculated is the greatest common divisor.\nUsing subtraction instead of division is not as fast, but both the division method and the subtraction method uses the same mathematical principle:\nThe greatest common divisor of numbers aa and bb, is also the greatest common divisor of the difference between aa and bb.\nThis can be shown in just a few lines.\nNumbers aa and bb have a greatest common divisor xx.\nThis means that both aa and bb can be factorized like this:\na=k⋅xb=l⋅x \\begin{equation} \\begin{aligned} a & = k \\cdot x \\\\ b & = l \\cdot x \\end{aligned} \\end{equation}\nAfter factorization, subtracting bb from aa gives us a very interesting result:\na−b=k⋅x−l⋅x=(k−l)⋅x \\begin{equation} \\begin{aligned} a-b & = k \\cdot x - l \\cdot x \\\\ & = (k-l) \\cdot x \\end{aligned} \\end{equation}\nWe can see that the greatest common divisor (xx) of aa and bb is also the greatest common divisor of the difference between aa and bb!\nThis principle is why the Euclidean algorithm works, it is what makes it possible.\nFinding The Greatest Common Divisor Using Subtraction\nUsing the principle described above, that the difference between aa and bb also shares the same greatest common divisor, we can use subtraction to find the greatest common divisor, like Euclid's original algorithm does.\nLet's find the greatest common divisor of 120120 from 2525 using subtraction.\n120−25=95 \\begin{equation} \\begin{aligned} 120 - 25 & = 95 \\end{aligned} \\end{equation}\nAccording to the mathematical principle already described, the numbers 120120, 2525, and 9595 all share the same greatest common divisor.\nThis means we can further reduce our problem by subtracting 2525 from 9595:\n95−25=70 \\begin{equation} \\begin{aligned} 95 - 25 & = 70 \\end{aligned} \\end{equation}\nIf we continue like this, always taking the two lowest numbers from the previous step and finding the difference between them, we get these calculations:\n70−25=4545−25=2025−20=520−5=1515−5=1010−5=5_5−5=0 \\begin{equation} \\begin{aligned} 70 - 25 & = 45 \\\\ 45 - 25 & = 20 \\\\ 25 - 20 & = 5 \\\\ 20 - 5 & = 15 \\\\ 15 - 5 & = 10 \\\\ 10 - 5 & = \\underline{\\textbf{5}} \\\\ 5 - 5 & = 0 \\end{aligned} \\end{equation}\nThe Euclidean algorithm using subtraction is done when the difference is 00.\nThe greatest common divisor of 120120 and 2525 can be found in the previous step, it is 55.\nNow that we can calculate the greatest common divisor using subtraction by hand, it is easier to implement it in a programming language.\nImplementation of The Euclidean Algorithm Using Subtraction\nTo find the greatest common divisor using subtraction, we continue running the algorithm until the difference between aa and bb is 00, like we have just seen.\nThis is the same as saying we continue running the algorithm as long as aa and bb are different values. That is why a != b is the condition in the while loop below.\nExample\nFinding the greatest common divisor of 120 and 25 using the Euclidean algorithm with subtraction:\nComparing The Subtraction Method With The Division Method\nTo see how good the division method can be to find the greatest common divisor, and how the methods are similar, we will:\nUse subtraction to find the greatest common divisor of 120120 and 2525.\nUse division with remainder to find the greatest common divisor of 120120 and 2525.\nCompare the subtraction and division methods.\n1. Using Subtraction\nFinding the greatest common divisor of 120120 and 2525 using subtraction:\n120−25=9595−25=7070−25=4545−25=2025−20=520−5=1515−5=1010−5=5_5−5=0 \\begin{equation} \\begin{aligned} 120 - 25 & = 95 \\\\ 95 - 25 & = 70 \\\\ 70 - 25 & = 45 \\\\ 45 - 25 & = 20 \\\\ 25 - 20 & = 5 \\\\ 20 - 5 & = 15 \\\\ 15 - 5 & = 10 \\\\ 10 - 5 & = \\underline{\\textbf{5}} \\\\ 5 - 5 & = 0 \\end{aligned} \\end{equation}\nUsing subtraction, the algorithm is finished when the difference is 00.\nIn the second last calculation we see that the greatest common divisor of 120120 and 2525 is 55.\nNotice how 2525 and 55 must be subtracted many times, until finding the GCD.\n2. Using Division\nFinding the greatest common divisor of 120120 and 2525 using division with remainder looks like this:\n120=4⋅25+2025=1⋅20+5_20=4⋅5+0 \\begin{equation} \\begin{aligned} 120 & = 4 \\cdot 25 + 20 \\\\ 25 & = 1 \\cdot 20 + \\underline{\\textbf{5}} \\\\ 20 & = 4 \\cdot 5 + 0 \\end{aligned} \\end{equation}\nUsing division, the Euclidean algorithm is finished when the remainder is 00.\nThe previous remainder 55 is the greatest common divisor of 120120 and 2525.\n3. Comparison\nTake a look at the subtraction and division methods above.\nTo easier see that the division calculations are basically the same as the subtraction calculations, we can write the division with remainder calculations like this:\n120−4⋅25=2025−1⋅20=5_20−4⋅5=0 \\begin{equation} \\begin{aligned} 120 - 4 \\cdot 25 & = 20 \\\\ 25 - 1 \\cdot 20 & = \\underline{\\textbf{5}} \\\\ 20 - 4 \\cdot 5 & = 0 \\end{aligned} \\end{equation}\nUsing subtraction, 2525 is subtracted from 120120 a total of 44 times, while the division method does this in just one step.\nSimilarly, the subtraction method subtracts 55 a total of 44 times, while the division method does the same in just one calculation.\nAs you can see, the methods do the same thing, the division method just does many subtractions in the same calculation, so that it finds the greatest common divisor faster.",
      "examples": [
        "def gcd_division(a, b): while b != 0: remainder = a % b print(f\"{a} = {a//b} * {b} + {remainder}\") a = b b = remainder return a a = 120 b = 25 print(\"The Euclidean algorithm using division:\\n\") print(f\"The GCD of {a} and {b} is: {gcd_division(a, b)}\")",
        "def gcd_subtraction(a, b): while a != b: if a > b: print(f\"{a} - {b} = {a-b}\") a = a - b else: print(f\"{b} - {a} = {b-a}\") b = b - a return a a = 120 b = 25 print(\"The Euclidean algorithm using subtraction:\\n\") print(f\"The GCD of {a} and {b} is: {gcd_subtraction(a, b)}\")",
        "b != 0",
        "while",
        "a != b"
      ]
    },
    {
      "title": "Huffman Coding",
      "summary": "Huffman Coding\nHuffman Coding is an algorithm used for lossless data compression.\nHuffman Coding is also used as a component in many different compression algorithms. It is used as a component in lossless compressions such as zip, gzip, and png, and even as part of lossy compression algorithms like mp3 and jpeg.\nUse the animation below to see how a text can be compressed using Huffman Coding.\n0 bits\nResultThe Huffman code is -1% of the original size.\nThe animation shows how the letters in a text are normally stored using UTF-8, and how Huffman Coding makes it possible to store the same text with fewer bits.\nHow it works:\nCount how often each piece of data occurs.\nBuild a binary tree, starting with the nodes with the lowest count. The new parent node has the combined count of its child nodes.\nThe edge from a parent gets '0' for the left child, and '1' for the edge to the right child.\nIn the finished binary tree, follow the edges from the root node, adding '0' or '1' for each branch, to find the new Huffman code for each piece of data.\nCreate the Huffman code by converting the data, piece-by-piece, into a binary code using the binary tree.\nHuffman Coding uses a variable length of bits to represent each piece of data, with a shorter bit representation for the pieces of data that occurs more often.\nFurthermore, Huffman Coding ensures that no code is the prefix of another code, which makes the compressed data easy to decode.\nData compression is when the original data size is reduced, but the information is mostly, or fully, kept. Sound or music files are for example usually stored in a compressed format, roughly just 10% of the original data size, but with most of the information kept.\nLossless means that even after the data is compressed, all the information is still there. This means that for example a compressed text still has all the same letters and characters as the original.\nLossy is the other variant of data compression, where some of the original information is lost, or sacrificed, so that the data can be compressed even more. Music, images, and video is normally stored and streamed with lossy compression like mp3, jpeg, and mp4.\nCreating A Huffman Code Manually\nTo get a better understanding of how Huffman Coding works, let's create a Huffman code manually, using the same text as in the animation: 'lossless'.\nA text is normally stored in the computer using UTF-8, which means that each letter is stored using 8 bits for normal latin letters, like we have in 'lossless'. Other letters or symbols such as '€' or '🦄' are stored using more bits.\nTo compress the text 'lossless' using Huffman Coding, we start by counting each letter.\nAs you can see in the nodes above, 's' occurs 4 times, 'l' occurs 2 times, and 'o' and 'e' occurs just 1 time each.\nWe start building the tree with the least occurring letters 'o' and 'e', and their parent node gets count '2', because the counts for letter 'o' and 'e' are summarized.\nThe next nodes that get a new parent node, are the nodes with the lowest count: 'l', and the parent node of 'o' and 'e'.\nNow, the last node 's' must be added to the binary tree. Letter node 's' and the parent node with count '4' get a new parent node with count '8'.\nFollowing the edges from the root node, we can now determine the Huffman code for each letter in the word 'lossless'.\nThe Huffman code for each letter can now be found under each letter node in the image above. A good thing about Huffman coding is that the most used data pieces get the shortest code, so just '0' is the code for the letter 's'.\nAs mentioned earlier, such normal latin letters are usually stored with UTF-8, which means they take up 8 bits each. So for example the letter 'o' is stored as '01101111' with UTF-8, but it is stored as '110' with our Huffman code for the word 'lossless'.\nNote: With UTF-8, a letter has always the same binary code, but with Huffman code, the binary code for each letter (piece of data) changes with text (data set) we are compressing.\nTo summarize, we have now compressed the word 'lossless' from its UTF-8 code\n01101100 01101111 01110011 01110011 01101100 01100101 01110011 01110011\nto just\n10 110 0 0 10 111 0 0\nusing Huffman Coding, which is a huge improvement.\nBut if data is stored with Huffman Coding as 10 110 0 0 10 111 0 0, or the code is sent to us, how can it be decoded so that we see what information the Huffman code contains?\nFurthermore, the binary code is really 10110001011100, without the spaces, and with variable bit lengths for each piece of data, so how can the computer understand where the binary code for each piece of data starts and ends?\nDecoding Huffman Code\nJust like with code stored as UTF-8, which our computers can already decode to the correct letters, the computer needs to know which bits represent which piece of data in the Huffman code.\nSo along with a Huffman code, there must also be a conversion table with information about what the Huffman binary code is for each piece of data, so that it can be decoded.\nSo, for this Huffman code:\n100110110\nWith this conversion table:\nAre you able to decode the Huffman code?\nHow it works:\nStart from the left in the Huffman code, and look up each bit sequence in the table.\nMatch each code to the corresponding letter.\nContinue until the entire Huffman code is decoded.\nWe start with the first bit:\nThere is no letter in the table with just 1 as the Huffman code, so we continue and include the next bit as well.\nWe can see from the table that 10 is 'b', so now we have the first letter. We check the next bit:\nWe find that 0 is 'a', so now we have the two first letters 'ba' stored in the Huffman code.\nWe continue looking up Huffman codes in the table:\nCode 11 is 'n'.\nCode 0 is 'a'.\nCode 11 is 'n'.\nCode 0 is 'a'.\nThe Huffman code is now decoded, and the word is 'banana'!\nHuffman Code Prefixes\nAn interesting and very useful part of the Huffman coding algorithm is that it ensures that there is no code that is the prefix of another code.\nJust image if the conversion table we just used, looked like this:\nIf this was the case, we would get confused right from the start of the decoding, right?\nBecause how would we know if the first bit 1 represents the letter 'a' or if it is the first bit for the letter 'b' or 'c'?\nThis property, that no code is the prefix of another code, makes it possible to decode. And it is especially important in Huffman Coding because of the variable bit lengths.\nHuffman Coding Implementation\nThe correct word for creating Huffman code based on data or text is \"encoding\", and the opposite would be \"decoding\", when the original data or text is recreated based on the code.\nThe code example below takes a word, or any text really, and compress it using Huffman Coding.\nExample\nHuffman Coding.\nHuffman Decoding Implementation\nIn addition to encode data using Huffman coding, we should also have a way to decode it, to recreate the original information.\nThe implementation below is basically the same as the previous code example, but with an additional function for decoding the Huffman code.\nThe huffman_decoding function takes the Huffman code, and the codes Python dictionary (a hashmap) with the characters and their corresponding binary codes. The Function then reverse the mapping, and checks the Huffman code bit-by-bit to recreate the original text.\nExample\nHuffman Decoding.\nYou have now seen how a text can be compressed using Huffman coding, and how a Huffman code can be decoded to recreate the original text.\nNote: Huffman Coding can be used for lossless compression of any kind of data, not just text. Huffman Coding is also used as a component in other compression algorithms like zip, and even in lossy compressions like jpeg and mp3.",
      "examples": [
        "class Node: def __init__(self, char=None, freq=0): self.char = char self.freq = freq self.left = None self.right = None nodes = [] def calculate_frequencies(word): frequencies = {} for char in word: if char not in frequencies: freq = word.count(char) frequencies[char] = freq nodes.append(Node(char, freq)) def build_huffman_tree(): while len(nodes) > 1: nodes.sort(key=lambda x: x.freq) left = nodes.pop(0) right = nodes.pop(0) merged = Node(freq=left.freq + right.freq) merged.left = left merged.right = right nodes.append(merged) return nodes[0] def generate_huffman_codes(node, current_code, codes): if node is None: return if node.char is not None: codes[node.char] = current_code generate_huffman_codes(node.left, current_code + '0', codes) generate_huffman_codes(node.right, current_code + '1', codes) def huffman_encoding(word): global nodes nodes = [] calculate_frequencies(word) root = build_huffman_tree() codes = {} generate_huffman_codes(root, '', codes) return codes word = \"lossless\" codes = huffman_encoding(word) encoded_word = ''.join(codes[char] for char in word) print(\"Word:\", word) print(\"Huffman code:\", encoded_word) print(\"Conversion table:\", codes)",
        "class Node: def __init__(self, char=None, freq=0): self.char = char self.freq = freq self.left = None self.right = None nodes = [] def calculate_frequencies(word): frequencies = {} for char in word: if char not in frequencies: freq = word.count(char) frequencies[char] = freq nodes.append(Node(char, freq)) def build_huffman_tree(): while len(nodes) > 1: nodes.sort(key=lambda x: x.freq) left = nodes.pop(0) right = nodes.pop(0) merged = Node(freq=left.freq + right.freq) merged.left = left merged.right = right nodes.append(merged) return nodes[0] def generate_huffman_codes(node, current_code, codes): if node is None: return if node.char is not None: codes[node.char] = current_code generate_huffman_codes(node.left, current_code + '0', codes) generate_huffman_codes(node.right, current_code + '1', codes) def huffman_encoding(word): global nodes nodes = [] calculate_frequencies(word) root = build_huffman_tree() codes = {} generate_huffman_codes(root, '', codes) return codes def huffman_decoding(encoded_word, codes): current_code = '' decoded_chars = [] # Invert the codes dictionary to get the reverse mapping code_to_char = {v: k for k, v in codes.items()} for bit in encoded_word: current_code += bit if current_code in code_to_char: decoded_chars.append(code_to_char[current_code]) current_code = '' return ''.join(decoded_chars) word = \"lossless\" codes = huffman_encoding(word) encoded_word = ''.join(codes[char] for char in word) decoded_word = huffman_decoding(encoded_word, codes) print(\"Initial word:\", word) print(\"Huffman code:\", encoded_word) print(\"Conversion table:\", codes) print(\"Decoded word:\", decoded_word)",
        "01101100 01101111 01110011 01110011 01101100 01100101 01110011 01110011",
        "10 110 0 0 10 111 0 0",
        "10110001011100",
        "100110110",
        "0",
        "10",
        "11",
        "1",
        "huffman_decoding",
        "codes"
      ]
    },
    {
      "title": "DSA The Traveling Salesman Problem",
      "summary": "The Traveling Salesman Problem\nThe Traveling Salesman Problem states that you are a salesperson and you must visit a number of cities or towns.\nThe Traveling Salesman Problem\nRules\n: Visit every city only once, then return back to the city you started in.\nGoal\n: Find the shortest possible route.\nExcept for the Held-Karp algorithm (which is quite advanced and time consuming, (O(2nn2)O(2^n n^2)), and will not be described here), there is no other way to find the shortest route than to check all possible routes.\nThis means that the time complexity for solving this problem is O(n!)O(n!), which means 720 routes needs to be checked for 6 cities, 40,320 routes must be checked for 8 cities, and if you have 10 cities to visit, more than 3.6 million routes must be checked!\nNote: \"!\", or \"factorial\", is a mathematical operation used in combinatorics to find out how many possible ways something can be done. If there are 4 cities, each city is connected to every other city, and we must visit every city exactly once, there are 4!=4⋅3⋅2⋅1=244!= 4 \\cdot 3 \\cdot 2 \\cdot 1 = 24 different routes we can take to visit those cities.\nThe Traveling Salesman Problem (TSP) is a problem that is interesting to study because it is very practical, but so time consuming to solve, that it becomes nearly impossible to find the shortest route, even in a graph with just 20-30 vertices.\nIf we had an effective algorithm for solving The Traveling Salesman Problem, the consequences would be very big in many sectors, like for example chip design, vehicle routing, telecommunications, and urban planning.\nChecking All Routes to Solve The Traveling Salesman Problem\nTo find the optimal solution to The Traveling Salesman Problem, we will check all possible routes, and every time we find a shorter route, we will store it, so that in the end we will have the shortest route.\nGood: Finds the overall shortest route.\nBad: Requires an awful lot of calculation, especially for a large amount of cities, which means it is very time consuming.\nHow it works:\nCheck the length of every possible route, one route at a time.\nIs the current route shorter than the shortest route found so far? If so, store the new shortest route.\nAfter checking all routes, the stored route is the shortest one.\nSuch a way of finding the solution to a problem is called brute force.\nBrute force is not really an algorithm, it just means finding the solution by checking all possibilities, usually because of a lack of a better way to do it.\nFinding the shortest route in The Traveling Salesman Problem by checking all routes (brute force).\nProgress: 0%\nn = 6 cities\n6!=720 possible routes\nShow every route: true\nThe reason why the brute force approach of finding the shortest route (as shown above) is so time consuming is that we are checking all routes, and the number of possible routes increases really fast when the number of cities increases.\nExample\nFinding the optimal solution to the Traveling Salesman Problem by checking all possible routes (brute force):\nUsing A Greedy Algorithm to Solve The Traveling Salesman Problem\nSince checking every possible route to solve the Traveling Salesman Problem (like we did above) is so incredibly time consuming, we can instead find a short route by just going to the nearest unvisited city in each step, which is much faster.\nGood: Finds a solution to the Traveling Salesman Problem much faster than by checking all routes.\nBad: Does not find the overall shortest route, it just finds a route that is much shorter than an average random route.\nHow it works:\nVisit every city.\nThe next city to visit is always the nearest of the unvisited cities from the city you are currently in.\nAfter visiting all cities, go back to the city you started in.\nThis way of finding an approximation to the shortest route in the Traveling Salesman Problem, by just going to the nearest unvisited city in each step, is called a greedy algorithm.\nFinding an approximation to the shortest route in The Traveling Salesman Problem by always going to the nearest unvisited neighbor (greedy algorithm).\nAs you can see by running this simulation a few times, the routes that are found are not completely unreasonable. Except for a few times when the lines cross perhaps, especially towards the end of the algorithm, the resulting route is a lot shorter than we would get by choosing the next city at random.\nExample\nFinding a near-optimal solution to the Traveling Salesman Problem using the nearest-neighbor algorithm (greedy):\nOther Algorithms That Find Near-Optimal Solutions to The Traveling Salesman Problem\nIn addition to using a greedy algorithm to solve the Traveling Salesman Problem, there are also other algorithms that can find approximations to the shortest route.\nThese algorithms are popular because they are much more effective than to actually check all possible solutions, but as with the greedy algorithm above, they do not find the overall shortest route.\nAlgorithms used to find a near-optimal solution to the Traveling Salesman Problem include:\n2-opt Heuristic: An algorithm that improves the solution step-by-step, in each step removing two edges and reconnecting the two paths in a different way to reduce the total path length.\nGenetic Algorithm: This is a type of algorithm inspired by the process of natural selection and use techniques such as selection, mutation, and crossover to evolve solutions to problems, including the TSP.\nSimulated Annealing: This method is inspired by the process of annealing in metallurgy. It involves heating and then slowly cooling a material to decrease defects. In the context of TSP, it's used to find a near-optimal solution by exploring the solution space in a way that allows for occasional moves to worse solutions, which helps to avoid getting stuck in local minima.\nAnt Colony Optimization: This algorithm is inspired by the behavior of ants in finding paths from the colony to food sources. It's a more complex probabilistic technique for solving computational problems which can be mapped to finding good paths through graphs.\nTime Complexity for Solving The Traveling Salesman Problem\nTo get a near-optimal solution fast, we can use a greedy algorithm that just goes to the nearest unvisited city in each step, like in the second simulation on this page.\nSolving The Traveling Salesman Problem in a greedy way like that, means that at each step, the distances from the current city to all other unvisited cities are compared, and that gives us a time complexity of O(n2)O(n^2) .\nBut finding the shortest route of them all requires a lot more operations, and the time complexity for that is O(n!)O(n!), like mentioned earlier, which means that for 4 cities, there are 4! possible routes, which is the same as 4⋅3⋅2⋅1=244 \\cdot 3 \\cdot 2 \\cdot 1 = 24. And for just 12 cities for example, there are 12!=12⋅11⋅10⋅...⋅2⋅1=479,001,60012! = 12 \\cdot 11 \\cdot 10 \\cdot \\; ... \\; \\cdot 2 \\cdot 1 = 479,001,600 possible routes!\nSee the time complexity for the greedy algorithm O(n2)O(n^2), versus the time complexity for finding the shortest route by comparing all routes O(n!)O(n!), in the image below.\nBut there are two things we can do to reduce the number of routes we need to check.\nIn the Traveling Salesman Problem, the route starts and ends in the same place, which makes a cycle. This means that the length of the shortest route will be the same no matter which city we start in. That is why we have chosen a fixed city to start in for the simulation above, and that reduces the number of possible routes from n!n! to (n−1)!(n-1)!.\nAlso, because these routes go in cycles, a route has the same distance if we go in one direction or the other, so we actually just need to check the distance of half of the routes, because the other half will just be the same routes in the opposite direction, so the number of routes we need to check is actually (n−1)!2 \\frac{(n-1)!}{2}.\nBut even if we can reduce the number of routes we need to check to (n−1)!2 \\frac{(n-1)!}{2}, the time complexity is still O(n!) O(n!), because for very big nn, reducing nn by one and dividing by 2 does not make a significant change in how the time complexity grows when nn is increased.\nTo better understand how time complexity works, go to this page.\nActual Traveling Salesman Problems Are More Complex\nThe edge weight in a graph in this context of The Traveling Salesman Problem tells us how hard it is to go from one point to another, and it is the total edge weight of a route we want to minimize.\nSo far on this page, the edge weight has been the distance in a straight line between two points. And that makes it much easier to explain the Traveling Salesman Problem, and to display it.\nBut in the real world there are many other things that affects the edge weight:\nObstacles: When moving from one place to another, we normally try to avoid obstacles like trees, rivers, houses for example. This means it is longer and takes more time to go from A to B, and the edge weight value needs to be increased to factor that in, because it is not a straight line anymore.\nTransportation Networks: We usually follow a road or use public transport systems when traveling, and that also affects how hard it is to go (or send a package) from one place to another.\nTraffic Conditions: Travel congestion also affects the travel time, so that should also be reflected in the edge weight value.\nLegal and Political Boundaries: Crossing border for example, might make one route harder to choose than another, which means the shortest straight line route might be slower, or more costly.\nEconomic Factors: Using fuel, using the time of employees, maintaining vehicles, all these things cost money and should also be factored into the edge weights.\nAs you can see, just using the straight line distances as the edge weights, might be too simple compared to the real problem. And solving the Traveling Salesman Problem for such a simplified problem model would probably give us a solution that is not optimal in a practical sense.\nIt is not easy to visualize a Traveling Salesman Problem when the edge length is not just the straight line distance between two points anymore, but the computer handles that very well.",
      "examples": [
        "from itertools import permutations def calculate_distance(route, distances): total_distance = 0 for i in range(len(route) - 1): total_distance += distances[route[i]][route[i + 1]] total_distance += distances[route[-1]][route[0]] return total_distance def brute_force_tsp(distances): n = len(distances) cities = list(range(1, n)) shortest_route = None min_distance = float('inf') for perm in permutations(cities): current_route = [0] + list(perm) current_distance = calculate_distance(current_route, distances) if current_distance < min_distance: min_distance = current_distance shortest_route = current_route shortest_route.append(0) return shortest_route, min_distance distances = [ [0, 2, 2, 5, 9, 3], [2, 0, 4, 6, 7, 8], [2, 4, 0, 8, 6, 3], [5, 6, 8, 0, 4, 9], [9, 7, 6, 4, 0, 10], [3, 8, 3, 9, 10, 0] ] route, total_distance = brute_force_tsp(distances) print(\"Route:\", route) print(\"Total distance:\", total_distance)",
        "def nearest_neighbor_tsp(distances): n = len(distances) visited = [False] * n route = [0] visited[0] = True total_distance = 0 for _ in range(1, n): last = route[-1] nearest = None min_dist = float('inf') for i in range(n): if not visited[i] and distances[last][i] < min_dist: min_dist = distances[last][i] nearest = i route.append(nearest) visited[nearest] = True total_distance += min_dist total_distance += distances[route[-1]][0] route.append(0) return route, total_distance distances = [ [0, 2, 2, 5, 9, 3], [2, 0, 4, 6, 7, 8], [2, 4, 0, 8, 6, 3], [5, 6, 8, 0, 4, 9], [9, 7, 6, 4, 0, 10], [3, 8, 3, 9, 10, 0] ] route, total_distance = nearest_neighbor_tsp(distances) print(\"Route:\", route) print(\"Total distance:\", total_distance)"
      ]
    },
    {
      "title": "DSA The 0/1 Knapsack Problem",
      "summary": "The 0/1 Knapsack Problem\nThe 0/1 Knapsack Problem states that you have a backpack with a weight limit, and you are in a room full of treasures, each treasure with a value and a weight.\nTo solve the 0/1 Knapsack Problem you must figure out which treasures to pack to maximize the total value, and at the same time keeping below the backpack's weight limit.\nKnapsack\n$ 0\n0/10 kg\nMicroscope\n$ 300\n2 kg\nGlobe\n$ 200\n1 kg\nCup\n$ 400\n5 kg\nCrown\n$ 500\n3 kg\nAre you able to solve the 0/1 Knapsack Problem above manually? Continue reading to see different implementations that solves the 0/1 Knapsack Problem.\nSolving the 0/1 Knapsack Problem helps businesses decide which projects to fund within a budget, maximizing profit without overspending. It is also used in logistics to optimize the loading of goods into trucks and planes, ensuring the most valuable, or highest prioritized, items are included without exceeding weight limits.\nThe 0/1 Knapsack Problem\nRules\n:\nEvery item has a weight and value.\nYour knapsack has a weight limit.\nChoose which items you want to bring with you in the knapsack.\nYou can either take an item or not, you cannot take half of an item for example.\nGoal\n:\nMaximize the total value of the items in the knapsack.\nThe Brute Force Approach\nUsing brute force means to just check all possibilities, looking for the best result. This is usually the most straight forward way of solving a problem, but it also requires the most calculations.\nTo solve the 0/1 Knapsack Problem using brute force means to:\nCalculate the value of every possible combination of items in the knapsack.\nDiscard the combinations that are heavier than the knapsack weight limit.\nChoose the combination of items with the highest total value.\nHow it works:\nConsider each item one at a time.\nIf there is capacity left for the current item, add it by adding its value and reducing the remaining capacity with its weight. Then call the function on itself for the next item.\nAlso, try not adding the current item before calling the function on itself for the next item.\nIf there is capacity left for the current item, add it by adding its value and reducing the remaining capacity with its weight. Then call the function on itself for the next item.\nAlso, try not adding the current item before calling the function on itself for the next item.\nReturn the maximum value from the two scenarios above (adding the current item, or not adding it).\nThis brute force approach to the 0/1 Knapsack problem can be implemented like this:\nExample\nSolving the 0/1 Knapsack Problem using recursion and brute force:\nRunning the code above means that the knapsack_brute_force function is called many times recursively. You can see that from all the printouts.\nEvery time the function is called, it will either include the current item n-1 or not.\nLine 2: This print statement shows us each time the function is called.\nLine 3-4: If we run out of items to check (n==0), or we run out of capacity (capacity==0), we do not do any more recursive calls because no more items can be added to the knapsack at this point.\nLine 6-7: If the current item is heavier than the capacity (weights[n-1] > capacity), forget the current item and go to the next item.\nLine 10-12: If the current item can be added to the knapsack, see what gives you the highest value: adding the current item, or not adding the current item.\nRunning the code example creates a recursion tree that looks like this, each gray box represents a function call:\nNote: In the recursion tree above, writing the real function name knapsack_brute_force(7,3) would make the drawing too wide, so \"ks(7,3)\" or \"knapsack(7,3)\" is written instead.\nFrom the recursion tree above, it is possible to see that for example taking the crown, the cup, and the globe, means that there is no space left for the microscope (2 kg), and that gives us a total value of 200+400+500=1100.\nWe can also see that only taking the microscope gives us a total value of 300 (right bottom gray box).\nAs you can see in the recursion tree above, and by running the example code, the function is sometimes called with the same arguments, like knapsack_brute_force(2,0) is for example called two times. We avoid this by using memoization.\nThe Memoization Approach (top-down)\nThe memoization technique stores the previous function call results in an array, so that previous results can be fetched from that array and does not have to be calculated again.\nRead more about memoization here.\nMemoization is a 'top-down' approach because it starts solving the problem by working its way down to smaller and smaller subproblems.\nIn the brute force example above, the same function calls happen only a few times, so the effect of using memoization is not so big. But in other examples with far more items to choose from, the memoization technique would be more helpful.\nHow it works:\nIn addition to the initial brute force code above, create an array memo to store previous results.\nFor every function call with arguments for capacity c and item number i, store the result in memo[c,i].\nTo avoid doing the same calculation more than once, every time the function is called with arguments c and i, check first if the result is already stored in memo[c,i].\nAfter improving the brute force implementation with the use of memoization, the code now looks like this:\nExample\nImproved solution to the 0/1 Knapsack Problem using memoization:\nThe highlighted lines in the code above show the memoization technique used to improve the previous brute force implementation.\nLine 24: Create an array memo where previous results are stored.\nLine 3-5: At the start of the function, before doing any calculations or recursive calls, check if the result has already been found and stored in the memo array.\nLine 16: Store the result for later.\nThe Tabulation Approach (bottom-up)\nAnother technique to solve the 0/1 Knapsack problem is to use something called tabulation. This approach is also called the iterative approach, and is a technique used in Dynamic Programming.\nTabulation solves the problem in a bottom-up manner by filling up a table with the results from the most basic subproblems first. The next table values are filled in using the previous results.\nHow it works:\nConsider one item at a time, and increase the knapsack capacity from 0 to the knapsack limit.\nIf the current item is not too heavy, check what gives the highest value: adding it, or not adding it. Store the maximum of these two values in the table.\nIn case the current item is too heavy to be added, just use the previously calculated value at the current capacity where the current item was not considered.\nUse the animation below to see how the table is filled cell by cell using previously calculated values until arriving at the final result.\nClick \"Run\" to fill the table.\nAfter the table is filled, click a cell value to see the calculation.\nWeights (kg)\nKnapsack capacities (kg)\nValues ($)\nMaximum Value in Knapsack:\nSpeed:\nThe tabulation approach works by considering one item at a time, for increasing knapsack capacities. In this way the solution is built up by solving the most basic subproblems first.\nOn each row an item is considered to be added to knapsack, for increasing capacities.\nExample\nImproved solution to the 0/1 Knapsack Problem using tabulation:\nLine 7-10: If the item weight is lower than the capacity it means it can be added. Check if adding it gives a higher total value than the result calculated in the previous row, which represents not adding the item. Use the highest (max) of these two values. In other words: Choose to take, or not to take, the current item.\nLine 8: This line might be the hardest to understand. To find the value that corresponds to adding the current item, we must use the current item's value from the values array. But in addition, we must reduce the capacity with the current item's weight, to see if the remaining capacity can give us any additional value. This is similar to check if other items can be added in addition to the current item, and adding the value of those items.\nLine 12: In case the current item is heavier than the capacity (too heavy), just fill in the value from the previous line, which represents not adding the current item.\nManual Run Through\nHere is a list of explanations to how a few of the table values are calculated. You can click the corresponding table cell in the animation above to get a better understanding as you read.\nMicroscope, capacity 1 kg: For the first value calculated, it is checked whether the microscope can be put in the bag if the weight limit is 1 kg. The microscope weighs 2 kg, it is too heavy, and so the value 0 is just copied from the cell above which corresponds to having no items in the knapsack. Only considering the microscope for a bag with weight limit 1 kg, means we cannot bring any items and we must leave empty handed with a total value of $ 0.\nMicroscope, capacity 2 kg: For the second value calculated, we are able to fit the microscope in the bag for a weight limit of 2 kg, so we can bring it, and the total value in the bag is $ 300 (the value of the microscope). And for higher knapsack capacities, only considering the microscope, means we can bring it, and so all other values in that row is $ 300.\nGlobe, capacity 1 kg: Considering the globe at 1 kg and a knapsack capacity at 1 kg means that we can bring the globe, so the value is $ 200. The code finds the maximum between bringing the globe which gives us $ 200, and the previously calculated value at 1 kg capacity, which is $ 0, from the cell above. In this case it is obvious that we should bring the globe because that is the only item with such a low weight, but in other cases the previously calculated value at the same capacity might be higher.\nGlobe, capacity 2 kg: At capacity 2 kg, the code sees that the globe can fit, which gives us a value of $ 200, but then the microscope cannot fit. And adding the microscope for a capacity of 2 kg gives us a value of $ 300, which is higher, so taking the microscope (value from the cell above) is the choice to maximize knapsack value for this table cell.\nGlobe, capacity 3 kg: Considering the globe with a capacity of 3 kg, means that we can take the globe, but with the remaining capacity of 2 kg we can also take the microscope. In this cell, taking both the globe and the microscope gives us a higher value 200+300=500 than taking just the microscope (as calculated on the previous line), so both items are taken and the cell value is 500.\nWhich Items Gives Us The Highest Value?\nAfter filling out the table and finding the maximum value the knapsack can have, it is not obvious which items we need to pack with us to get that value.\nTo find the included items, we use the table we have created, and we start with the bottom right cell with the highest value, in our case the cell with value 1200 in it.\nSteps to find the included items:\nStart with bottom right cell (the cell with the highest value).\nIf the cell above has the same value, it means that this row's item is not included, and we go to the cell above.\nIf the cell above has a different value, it means that the current row's item is included, and we move to the row above, and we move to the left as many times as the weight of the included item.\nContinue to do steps 2 and 3 until a cell with value 0 is found.\nHere is a drawing of how the included items are found, using the step-by-step method:\nWeights (kg)\nKnapsack capacities (kg)\nValues ($)\nThis is how the included items are found:\nThe bottom right value is 1200, and the cell above is 900. The values are different, which means the crown is included.\nThe next cell we go to is on the row above, and we move left as many times as the crown is heavy, so 3 places left to the cell with value 700.\nThe cell we are in now has value 700, and the cell above has value 500. The values are different, which means the item on the current row is included: the cup.\nThe cup weighs 5 kg, so the next cell we go to is on the row above, and 5 places to the left, to the cell with value 300, on the row were the globe is considered.\nThe cell above has the same value 300, which means the globe is not included, and the next cell we go to is the cell right above with value 300 where the microscope is considered.\nSince the cell above is different than the current cell with value 300, it means the microscope is included.\nThe next cell we go to is on the line above, and two places to the left because the microscope is 2 kg.\nWe arrive at the upper leftmost cell. Since the value is 0, it means we are finished.\nOur 0/1 Knapsack problem has maximum value when these items are included: the crown, the cup, and the microscope.\nThe same steps are added to the code below, to find the items that make up the solution to the 0/1 Knapsack problem.\nExample\nExtended solution to the 0/1 Knapsack Problem to find the included items:\nTime Complexity\nThe three approaches to solving the 0/1 Knapsack Problem run differently, and with different time complexities.\nBrute Force Approach: This is the slowest of the three approaches. The possibilities are checked recursively, with the time complexity O(2n)O(2^n), where nn is the number of potential items we can pack. This means the number of computations double for each extra item that needs to be considered.\nMemoization Approach: Saves computations by remembering previous results, which results in a better time complexity O(n⋅C)O(n \\cdot C), where nn is the number of items, and CC is the knapsack capacity. This approach runs otherwise in the same recursive way as the brute force approach.\nTabulation Approach: Has the same time complexity as the memoization approach O(n⋅C)O(n \\cdot C), where nn is the number of items, and CC is the knapsack capacity, but memory usage and the way it runs is more predictable, which normally makes the tabulation approach the most favorable.\nNote: Memoization and tabulation are used in something called dynamic programming, which is a powerful technique used in computer science to solve problems. To use dynamic programming to solve a problem, the problem must consist of overlapping subproblems, and that is why it can be used to solve the 0/1 Knapsack Problem, as you can see above in the memoization and tabulation approaches.",
      "examples": [
        "def knapsack_brute_force(capacity, n): print(f\"knapsack_brute_force({capacity},{n})\") if n == 0 or capacity == 0: return 0 elif weights[n-1] > capacity: return knapsack_brute_force(capacity, n-1) else: include_item = values[n-1] + knapsack_brute_force(capacity-weights[n-1], n-1) exclude_item = knapsack_brute_force(capacity, n-1) return max(include_item, exclude_item) values = [300, 200, 400, 500] weights = [2, 1, 5, 3] capacity = 10 n = len(values) print(\"\\nMaximum value in Knapsack =\", knapsack_brute_force(capacity, n))",
        "def knapsack_memoization(capacity, n): print(f\"knapsack_memoization({n}, {capacity})\") if memo[n][capacity] is not None: print(f\"Using memo for ({n}, {capacity})\") return memo[n][capacity] if n == 0 or capacity == 0: result = 0 elif weights[n-1] > capacity: result = knapsack_memoization(capacity, n-1) else: include_item = values[n-1] + knapsack_memoization(capacity-weights[n-1], n-1) exclude_item = knapsack_memoization(capacity, n-1) result = max(include_item, exclude_item) memo[n][capacity] = result return result values = [300, 200, 400, 500] weights = [2, 1, 5, 3] capacity = 10 n = len(values) memo = [[None]*(capacity + 1) for _ in range(n + 1)] print(\"\\nMaximum value in Knapsack =\", knapsack_memoization(capacity, n))",
        "def knapsack_tabulation(): n = len(values) tab = [[0]*(capacity + 1) for y in range(n + 1)] for i in range(1, n+1): for w in range(1, capacity+1): if weights[i-1] <= w: include_item = values[i-1] + tab[i-1][w-weights[i-1]] exclude_item = tab[i-1][w] tab[i][w] = max(include_item, exclude_item) else: tab[i][w] = tab[i-1][w] for row in tab: print(row) return tab[n][capacity] values = [300, 200, 400, 500] weights = [2, 1, 5, 3] capacity = 10 print(\"\\nMaximum value in Knapsack =\", knapsack_tabulation())",
        "def knapsack_tabulation(): n = len(values) tab = [[0] * (capacity + 1) for _ in range(n + 1)] for i in range(1, n + 1): for w in range(1, capacity + 1): if weights[i-1] <= w: include_item = values[i-1] + tab[i-1][w - weights[i-1]] exclude_item = tab[i-1][w] tab[i][w] = max(include_item, exclude_item) else: tab[i][w] = tab[i-1][w] for row in tab: print(row) items_included = [] w = capacity for i in range(n, 0, -1): if tab[i][w] != tab[i-1][w]: items_included.append(i-1) w -= weights[i-1] print(\"\\nItems included:\", items_included) return tab[n][capacity] values = [300, 200, 400, 500] weights = [2, 1, 5, 3] capacity = 10 print(\"\\nMaximum value in Knapsack =\", knapsack_tabulation())"
      ]
    },
    {
      "title": "Memoization",
      "summary": "Memoization\nMemoization is a technique where results are stored to avoid doing the same computations many times.\nWhen Memoization is used to improve recursive algorithms, it is called a \"top-down\" approach because of how it starts with the main problem and breaks it down into smaller subproblems.\nMemoization is used in Dynamic Programming.\nUsing Memoization To Find The nnth Fibonacci Number\nThe nnth Fibonacci number can be found using recursion. Read more about how that is done on this page.\nThe problem with this implementation is that the number of computations and recursive calls \"explodes\" when trying to find a higher Fibonacci number, because the same computations are done over and over again.\nExample\nFind the 6th Fibonacci number with recursion:\nAs you can see from running the example above, there are 25 computations, with the same computations done many times, even for just finding the 6th Fibonacci number.\nBut using memoization can help finding the nnth Fibonacci number using recursion much more effectively.\nWe use memoization by creating an array memo to hold the Fibonacci numbers, so that Fibonacci number n can be found as element memo[n]. And we only compute the Fibonacci number if it does not already exist in the memo array.\nExample\nFind the 6th Fibonacci number with recursion, but using memoization to avoid unnecessary recursive calls:\nAs you can see by running the examples above, memoization is very helpful to reduce the number of computations.\nThe number of computations is reduced from 25 in the initial code, to just 7 in the last example using memoization, and the benefit of using memoization increases really fast by how high the Fibonacci number we want to find is.\nFinding the 30th Fibonacci number requires 2,692,537 computations in the initial code, but it just requires 31 computations in the algorithm implemented using memoization!\nYou get this result by running the code below.\nExample\nSee the difference in calculations for finding the 30th Fibonacci number, with and without memoization:\nMemoization in AVL Trees\nFor more details about what an AVL Tree is, please see this page.\nAn AVL tree is a type of Binary Tree that is self-balancing.\nEvery time a node is inserted or deleted from an AVL tree, the balancing factor must be calculated for all ancestors, using the height of the left and right subtrees to find out if a rotation is needed to restore balance.\nTo avoid calculating the height of each node (going all the way down to the leaf nodes) to calculate the balancing factors, each node has its subtree height stored.\nExample\nThis means that to find the balance factor for a node, the already stored left child's height is subtracted from the already stored right child's height, no other calculations needed.\nStoring height in AVL trees is a form of memoization, because values are stored to avoid re-calculating them. In AVL trees, when the height is stored like this, it is called an augmented property.\nAn augmented property is a property of an element that does not have to be stored, but is stored to make operations more efficient.\nThe node heights must be calculated at some point of course, but that is only done when strictly needed, using retracing.",
      "examples": [
        "def F(n): print('Computing F('+str(n)+')') if n <= 1: return n else: return F(n - 1) + F(n - 2) print('F(6) = ',F(6))",
        "def F(n): if memo[n] != None: # Already computed return memo[n] else: # Computation needed print('Computing F('+str(n)+')') if n <= 1: memo[n] = n else: memo[n] = F(n - 1) + F(n - 2) return memo[n] memo = [None]*7 print('F(6) = ',F(6)) print('memo = ',memo)",
        "computation_count = 0 def F(n): global computation_count computation_count += 1 if n <= 1: return n else: return F(n - 1) + F(n - 2) computation_count_mem = 0 def F_mem(n): if memo[n] != None: # Already computed return memo[n] else: # Computation needed global computation_count_mem computation_count_mem += 1 if n <= 1: memo[n] = n else: memo[n] = F_mem(n - 1) + F_mem(n - 2) return memo[n] print('F(30) = ',F(30)) print(f'Number of computations: {computation_count}') print('\\nUsing memoization:') memo = [None]*31 print('F(30) = ',F_mem(30)) print(f'Number of computations with memoiztion: {computation_count_mem}')",
        "class TreeNode: def __init__(self, data): self.data = data self.left = None self.right = None self.height = 1",
        "memo",
        "n",
        "memo[n]"
      ]
    },
    {
      "title": "Tabulation",
      "summary": "Tabulation\nTabulation is a technique used to solve problems.\nTabulation uses a table where the results to the most basic subproblems are stored first. The table then gets filled with more and more subproblem results until we find the result to the complete problem that we are looking for.\nThe tabulation technique is said to solve problems \"bottom-up\" because of how it solves the most basic subproblems first.\nTabulation is a technique used in Dynamic Programming, which means that to use tabulation, the problem we are trying to solve must consist of overlapping subproblems.\nUsing Tabulation To Find The nnth Fibonacci Number\nThe Fibonacci numbers are great for demonstrating different programming techniques, also when demonstrating how tabulation works.\nTabulation uses a table that is filled with the lowest Fibonacci numbers F(0)=0F(0)=0 and F(1)=1F(1)=1 first (bottom-up). The next Fibonacci number to be stored in the table is F(2)=F(1)+F(0)F(2)=F(1)+F(0).\nThe next Fibonacci number is always the sum of the two previous numbers:\nF(n)=F(n−1)+F(n−2) F(n)=F(n-1)+F(n-2)\nIn this way, the table continues to get filled with next Fibonacci numbers until we find the nnth Fibonacci number that we are looking for.\nExample\nFinding the 10th Fibonacci number using tabulation:\nOther ways to find the nnth Fibonacci number include recursion, or the improved version of it using memoization.\nTabulation Is A Bottom Up Approach\nSee the drawings below to get a better idea of why tabulation is called a \"bottom up\" approach.\nAs a reference to compare with, see the drawing of the \"top-down\" recursion approach to finding the nnth Fibonacci number.\nThe tabulation approach starts building the table bottom up to find the 10th Fibonacci number, starting with F(0)F(0) and F(1)F(1).\nThe recursive approach start by trying to find F(10)F(10), but to find that it must call F(9)F(9) and F(8)F(8), and so it goes all the way down to F(0)F(0) and F(1)F(1) before the function calls start returning values that can be put together to the final answer.\nOther Problems That Are Solved with Tabulation\nJust like finding the nnth Fibonacci number, tabulation can also be used to find the solution to other problems:\nThe 0/1 Knapsack Problem is about having a set of items we can pack in a knapsack (a simple backpack), each item with a different value. To solve the problem we need to find the items that will maximize the total value of items we pack, but we cannot bring all the items we want because the knapsack has a weight limit.\nThe Shortest Path Problem can be solved using the Bellman-Ford algorithm, which also uses tabulation to find the shortest paths in a graph. More specifically, the tabulation approach of the Bellman-Ford algorithm is in how the values in the \"distances\" array gets updated.\nThe Traveling Salesman Problem can be solved precisely using the Held-Karp algorithm, which also uses tabulation. This algorithm is not described in this tutorial as it is although better than brute force O(n!)O(n!), still not very effective O(2nn2)O(2^n n^2), and quite advanced.\nTabulation in Dynamic Programming\nAs mentioned in the top, tabulation (just like memoization) is a technique used in something called Dynamic Programming.\nDynamic Programming is a way of designing algorithms to solve problems.\nFor Dynamic Programming to work, the problem we want to solve must have these two properties:\nThe problem must be built up by smaller, overlapping subproblems. For example, the solution to Fibonacci number F(3)F(3) overlaps with the solutions to Fibonacci numbers F(2)F(2) and F(1)F(1), because we get F(3)F(3) by combining F(2)F(2) and F(1)F(1).\nThe problem must also have an optimal substructure, meaning that the solution to the problem can be constructed from the solutions to its subproblems. When finding the nnth Fibonacci number, F(n)F(n) can be found by adding F(n−1)F(n-1) and F(n−2)F(n-2). So knowing the two previous numbers is not enough to find F(n)F(n), we must also know the structure so that we know how to put them together.\nRead more about Dynamic Programming on the next page.",
      "examples": [
        "def fibonacci_tabulation(n): if n == 0: return 0 elif n == 1: return 1 F = [0] * (n + 1) F[0] = 0 F[1] = 1 for i in range(2, n + 1): F[i] = F[i - 1] + F[i - 2] print(F) return F[n] n = 10 result = fibonacci_tabulation(n) print(f\"\\nThe {n}th Fibonacci number is {result}\")"
      ]
    },
    {
      "title": "Dynamic Programming",
      "summary": "Dynamic Programming\nDynamic Programming is a method for designing algorithms.\nAn algorithm designed with Dynamic Programming divides the problem into subproblems, finds solutions to the subproblems, and puts them together to form a complete solution to the problem we want to solve.\nTo design an algorithm for a problem using Dynamic Programming, the problem we want to solve must have these two properties:\nOverlapping Subproblems: Means that the problem can be broken down into smaller subproblems, where the solutions to the subproblems are overlapping. Having subproblems that are overlapping means that the solution to one subproblem is part of the solution to another subproblem.\nOptimal Substructure: Means that the complete solution to a problem can be constructed from the solutions of its smaller subproblems. So not only must the problem have overlapping subproblems, the substructure must also be optimal so that there is a way to piece the solutions to the subproblems together to form the complete solution.\nWe have already seen Dynamic Programming in this tutorial, in the memoization and tabulation techniques, and for solving problems like the 0/1 Knapsack Problem, or to find the shortest path with the Bellman-Ford algorithm.\nNote: Another way of designing an algorithm is using a greedy approach.\nUsing Dynamic Programming To Find The nnth Fibonacci Number\nLet's say we want an algorithm that finds the nnth Fibonacci number. We don't know how to find the nnth Fibonacci number yet, except that we want to use Dynamic Programming to design the algorithm.\nThe Fibonacci numbers is a sequence of numbers starting with 00 and 11, and the next numbers are created by adding the two previous numbers.\nThe 8 first Fibonacci numbers are: 0,1,1,2,3,5,8,130,\\; 1,\\; 1,\\; 2,\\; 3,\\; 5,\\; 8,\\; 13.\nAnd counting from 0, the 44th Fibonacci number F(4)F(4) is 33.\nIn general, this is how a Fibonacci number is created based on the two previous:\nF(n)=F(n−1)+F(n−2) F(n)=F(n-1)+F(n-2)\nSo how can we use Dynamic Programming to design an algorithm that finds the nnth Fibonacci number?\nThere is no exact rule for how to design an algorithm using Dynamic Programming, but here is a suggestion that should work in most cases:\nCheck if the the problem has \"overlapping subproblems\" and an \"optimal substructure\".\nSolve the most basic subproblems.\nFind a way to put the subproblem solutions together to form solutions to new subproblems.\nWrite the algorithm (the step-by-step procedure).\nImplement the algorithm (test if it works).\nLet's do it.\nStep 1: Check if the problem has \"overlapping subproblems\" and an \"optimal substructure\".\nBefore trying to find an algorithm using Dynimaic Programming, we must first check if the problem has the two properties \"overlapping subproblems\" and \"optimal substructure\".\nOverlapping subproblems?\nYes. The 66th Fibonacci number is a combination of the 55th and 44th Fibonacci number: 8=5+38=5+3. And this rule holds for all other Fibonacci numbers as well. This shows that the problem of finding the nnth Fibonacci number can be broken into subproblems.\nAlso, the subproblems overlap because F(5)F(5) is based on F(4)F(4) and F(3)F(3), and F(6)F(6) is based on F(5)F(5) and F(4)F(4).\nF(5)=F(4)_+F(3)5=3_+2andF(6)=F(5)+F(4)_8=5+3_ \\begin{equation} \\begin{aligned} F(5) {} & =\\underline{F(4)}+F(3) \\\\ 5 & =\\underline{3}+2 \\\\\\\\ & and \\\\\\\\ F(6) & =F(5)+\\underline{F(4)} \\\\ 8 & =5+\\underline{3} \\end{aligned} \\end{equation}\nYou see? Both solutions to subproblems F(5)F(5) and F(6)F(6) are created using the solution to F(4)F(4), and there are many cases like that, so the subproblems overlap as well.\nOptimal substructure?\nYes, the Fibonacci number sequence has a very clear structure, because the two previous numbers are added to create the next Fibonacci number, and this holds for all Fibonacci numbers except for the two first. This means we know how to put together a solution by combining the solutions to the subproblems.\nWe can conclude that the problem of finding the nnth Fibonacci number satisfies the two requirements, which means that we can use Dynamic Programming to find an algorithm that solves the problem.\nStep 2: Solve the most basic subproblems.\nWe can now start trying to find an algorithm using Dynamic Programming.\nSolving the most basic subproblems first is a good place to start to get an idea of how the algorithm should run.\nIn our problem of finding the nnth Fibonacci number, finding the most basic subproblems is not that hard, because we already know that\nF(0)=0F(1)=1F(2)=1F(3)=2F(4)=3F(5)=5F(6)=8... F(0)=0 \\\\ F(1)=1 \\\\ F(2)=1 \\\\ F(3)=2 \\\\ F(4)=3 \\\\ F(5)=5 \\\\ F(6)=8 \\\\ ...\nStep 3: Find a way to put the subproblem solutions together to form solutions to new subproblems.\nIn this step, for our problem, how the subproblems are put together is quite straightforward, we just need to add the two previous Fibonacci numbers to find the next one.\nSo for example, the 22nd Fibonacci number is created by adding the two previous numbers F(2)=F(1)+F(0)F(2)=F(1)+F(0), and that is the general rule as well, like mentioned earlier: F(n)=F(n−1)+F(n−2)F(n)=F(n-1)+F(n-2).\nNote: In other problems, combining solutions to subproblems to form new solutions usually involves making decisions like \"should we choose this way, or this way?\", or \"should we include this item, or not?\".\nStep 4: Write the algorithm (the step-by-step procedure).\nInstead of writing the text for the algorithm straight away, it might be wise to try to write a procedure to solve a specific problem first, like finding the 66th Fibonacci number.\nFor reference, the 8 first Fibonacci numbers are: 0,1,1,2,3,5,8_,130,\\; 1,\\; 1,\\; 2,\\; 3,\\; 5,\\; \\underline{8},\\; 13.\nFinding the 66th Fibonacci number, we could start with the two first numbers 00 and 11, which appear in place 0 and 1 in the sequence, and put them in an array, at index 0 and 1. Then we could add the two first numbers in the array to generate the next number, and push that new number as a new element to the array. If we continue like this until the array is 7 elements long we would stop and return F[6]. That would work, right?\nAfter solving the specific problem above, it is now easier to write the actual algorithm.\nThe algorithm for finding the nnth Fibonacci number, using Dynamic Programming as a design method, can be described like this:\nHow it works:\nCreate an array F, with n+1n+1 elements.\nStore the two first Fibonacci numbers F[0]=0 and F[1]=1.\nStore the next element F[2]=F[1]+F[0], and continue to create new Fibonacci numbers like that until the value in F[n] is created.\nReturn F[n].\nStep 5: Implement the algorithm (test if it works).\nTo implement the algorithm above, we assume that the argument n to the function is a positive number (the nnth Fibonacci number), we use a for loop to create new Fibonacci numbers, and we return the base cases F[0] and F[1] straight away if the function is called with 0 or 1 as an argument.\nImplementing the algorithm also means that we can check if it works.\nExample\nFinding the 6th Fibonacci number with our new algorithm:\nThere it is!\nWe have used Dynamic Programming as a design method to create an algorithm that finds the nnth Fibonacci number.\nWe have also implemented the algorithm to demonstrate that it works, and in doing so we have unintentionally used a well established technique within Dynamic Programming called tabulation, where the solution is found by solving subproblems bottom-up, using some kind of table.\nFurthermore, we have avoided calculating the same overlapping subproblems many times, like F[3] for example, that we could potentially have ended up doing otherwise, with a brute force recursive approach for example.\nAnother technique used in Dynamic Programming is called memoization. In this case, using memoization essentially solves the problem recursively with brute force, but stores the subproblem solutions for later as the algorithm runs to avoid doing the same calculations more than once.\nTechniques Used in Dynamic Programming\nIt might be difficult to design an algorithm using Dynamic Programming, but the concept of Dynamic Programming is actually not that hard: Solve the problem, but since the subproblems are overlapping, do it in a smart way so that a specific subproblem only needs to be solved once.\nTo be able to use solutions to previously solved subproblems in Dynamic Programming, the previously found solutions must be stored somehow, and that can be done using memoization or tabulation.\nMemoization is a technique used in Dynamic Programming, where the solution is found recursively. As the algorithm runs, solutions to subproblems are stored, and before trying to compute the solution to a subproblem, it checks first to see if that solution has already been computed, to avoid doing the same computation more than once.\nThe memoization technique is called \"top-down\" because the initial function call is for the main problem, and it results in new function calls for solving smaller and smaller subproblems.\nTabulation is a technique used in Dynamic Programming, where solutions to the overlapping subproblems are stored in a table (array), starting with the most basic subproblems.\nThe tabulation technique is not recursive, and it is called \"bottom-up\" because of the way the final solution is built up by solving the most basic subproblems first. Since the most basic subproblem solutions are stored in the table first, when solving a subproblem later that relies on previous subproblems, the algorithm can just pick these solutions right from the table, no need to compute them again.\nTo get a better sense of how memoization works, and is considered \"top-down\", and how tabulation works, and is \"bottom-up\", take a look at the two images below.\nAs you can see in the images above, the tabulation approach starts at the bottom by solving F(0) first, while the memoization approach start at the top with F(10) and breaking it into smaller and smaller subproblems from there.",
      "examples": [
        "def nth_fibo(n): if n==0: return 0 if n==1: return 1 F = [None] * (n + 1) F[0] = 0 F[1] = 1 for i in range(2, n + 1): F[i] = F[i - 1] + F[i - 2] return F[n] n = 6 result = nth_fibo(n) print(f\"The {n}th Fibonacci number is {result}\")",
        "F[6]",
        "F",
        "F[0]=0",
        "F[1]=1",
        "F[2]=F[1]+F[0]",
        "F[n]",
        "n",
        "for",
        "F[0]",
        "F[1]",
        "0",
        "1",
        "F[3]"
      ]
    },
    {
      "title": "DSA Greedy Algorithms",
      "summary": "Greedy Algorithms\nA greedy algorithm decides what to do in each step, only based on the current situation, without a thought of how the total problem looks like.\nIn other words, a greedy algorithm makes the locally optimal choice in each step, hoping to find the global optimum solution in the end.\nIn Dijkstra's algorithm for example, the next vertex to be visited is always the next unvisited vertex with the currently shortest distance from the source, as seen from the current group of visited vertices.\nSo Dijkstra's algorithm is greedy because the choice of which vertex to visit next is only based on the currently available information, without considering the overall problem or how this choice might affect future decisions or the shortest paths in the end.\nChoosing a greedy algorithm is a design choice, just like Dynamic Programming is another algorithm design choice.\nTwo properties must be true for a problem for a greedy algorithm to work:\nGreedy Choice Property: Means that the problem is so that the solution (the global optimum) can be reached by making greedy choices in each step (locally optimal choices).\nOptimal Substructure: Means that the optimal solution to a problem, is a collection of optimal solutions to sub-problems. So solving smaller parts of the problem locally (by making greedy choices) contributes to the overall solution.\nMost of the problems in this tutorial, like sorting an array, or finding the shortest paths in a graph, have these properties, and those problems can therefore be solved by greedy algorithms like Selection sort or Dijkstra's algorithm.\nBut problems like The Traveling Salesman, or the 0/1 Knapsack Problem, do not have these properties, and so a greedy algorithm can not be used to solve them. These problems are discussed further down.\nIn addition, even if a problem can be solved by a greedy algorithm, it can also be solved by non-greedy algorithms.\nAlgorithms That Are Not Greedy\nBelow are algorithms that are not greedy, meaning they do not only rely on doing locally optimal choices in each step:\nMerge Sort: Splits the array in halves over and over again, and then merges the array parts together again in a way that results in a sorted array. These operations are not a series of locally optimal choices like greedy algorithms are.\nQuick Sort: The choice of pivot element, the arranging of elements around the pivot element, and the recursive calls to do the same with the left and right side of the pivot element — those actions do not rely on making greedy choices.\nBFS and DFS Traversal: These algorithms traverse a graph without making a choice locally in each step on how to continue with the traversal, and so they are not greedy algorithms.\nFinding the nth Fibonacci number using memoization: This algorithm belongs to a way of solving problems called Dynamic Programming, which solves overlapping sub-problems, and then pieces them back together. Memoization is used in each step to optimize the overall algorithm, which means that at each step, this algorithm does not only consider what is the locally optimal solution, but it also takes into account that a result computed in this step, might be used in later steps.\nThe 0/1 Knapsack Problem\nThe 0/1 Knapsack Problem cannot be solved by a greedy algorithm because it does not fulfill the greedy choice property, and the optimal substructure property, as mentioned earlier.\nThe 0/1 Knapsack Problem\nRules\n:\nEvery item has a weight and value.\nYour knapsack has a weight limit.\nChoose which items you want to bring with you in the knapsack.\nYou can either take an item or not, you cannot take half of an item for example.\nGoal\n:\nMaximize the total value of the items in the knapsack.\nThis problem cannot be solved by a greedy algorithm, because choosing the item with the highest value, the lowest weight, or the highest value to weight ratio, in each step (local optimal solution, greedy), does not guarantee the optimal solution (global optimum).\nLet's say your backpack's limit is 10 kg, and you have these three treasures in front of you:\nMaking the greedy choice by taking the most valuable thing first, the horse figure with value $600, means that you can not bring any of the other things without breaking the weight limit.\nSo by trying to solve this problem in a greedy way you end up with a metal horse with value $600.\nBut the best solution in this case is taking the shield and the pot, maximizing the total value in the backpack to be $800, while still being under the weight limit.\nWhat about always taking the treasure with the lowest weight? Or always taking the treasure with the highest value to weight ratio?\nWhile following those principles would actually lead us to the best solution in this specific case, we could not guarantee that those principles would work if the values and weights in this example were changed.\nThis means that The 0/1 Knapsack Problem cannot be solved with a greedy algorithm.\nRead more about The 0/1 Knapsack Problem here.\nThe Traveling Salesman\nThe Traveling Salesman Problem is a famous problem that also cannot be solved by a greedy algorithm, because it does not fulfill the greedy choice property, and the optimal substructure property, as mentioned earlier.\nThe Traveling Salesman Problem states that you are a salesperson with a number of cities or towns you must visit to sell your things.\nThe Traveling Salesman Problem\nRules\n: Visit every city only once, then return back to the city you started in.\nGoal\n: Find the shortest possible route.\nUsing a greedy algorithm here, you would always go to the next unvisited city that is closest to the city you are currently in. But this would in most cases not lead you to the optimal solution with the shortest total path.\nThe simulation below shows how it looks like when a greedy algorithm tries to solve The Traveling Salesman Problem.\nRunning the simulation, it might not always be obvious that the algorithm is flawed, but you might notice how sometimes the lines are crossing, creating a longer total distance, when that is clearly not necessary.\nA greedy algorithm trying to solve The Traveling Salesman Problem.\nWhile running a greedy approach to The Traveling Salesman Problem sometimes gives us a pretty good approximation to the shortest possible route, a greedy algorithm will not be able to solve The Traveling Salesman Problem in general.\nThe Traveling Salesman Problem does not fulfill the properties needed so that it can be solved by a greedy algorithm.\nNote: There is actually no algorithm that finds the shortest route in The Traveling Salesman Problem efficiently. We just have to check all possible routes! This gives us a time complexity of O(n!)O(n!), which means the number of calculations explodes when the number of cities (nn) is increased.\nRead more about the Traveling Salesman Problem here.",
      "examples": []
    },
    {
      "title": "DSA Examples",
      "summary": "A Simple Algorithm\nArrays\nBubble Sort\nSelection Sort\nInsertion Sort\nQuick Sort\nCounting Sort\nRadix Sort\nMerge Sort\nLinear Search\nBinary Search\nLinked Lists in Memory\nLinked Lists Types\nLinked Lists Operations\nStacks\nQueues\nHash Tables\nHash Sets\nHash Maps\nBinary Trees\nPre-order Traversal\nIn-order Traversal\nPost-order Traversal\nArray Implementation of Binary Trees\nBinary Search Trees\nAVL Trees\nGraphs Implementation\nGraphs Traversal\nCycle Detection in Graphs\nDijkstra's Algorithm\nThe Bellman-Ford Algorithm\nPrim's Algorithm\nKruskal's Algorithm\nThe Ford-Fulkerson Algorithm\nThe Edmonds-Karp Algorithm\nThe Eucledian Algorithm\nHuffman Coding\nThe Traveling Salesman Problem\nThe 0/1 Knapsack Problem\nMemoization\nTabulation\nDynamic Programming",
      "examples": []
    },
    {
      "title": "DSA Exercises",
      "summary": "You can test your Data Structures and Algorithms skills with W3Schools' Exercises.\nExercises\nWe have gathered a variety of DSA exercises (with answers) for each DSA Chapter.\nTry to solve an exercise by filling in the blanks, or show the answer to see what you've done wrong.\nCount Your Score\nYou will get 1 point for each correct answer. Your score and total score will always be displayed.\nStart DSA Exercises\nGood luck!\nStart DSA Exercises ❯\nIf you don't know much about Data Structures and Algorithms, we suggest that you read our DSA Tutorial from scratch.",
      "examples": []
    },
    {
      "title": "DSA Quiz",
      "summary": "You can test your skills in Data Structures and Algorithms with W3Schools' Quiz.\nThe Test\nThe test contains 25 questions and there is no time limit.\nThe test is not official, it's just a nice way to see how much you know about DSA.\nCount Your Score\nYou will get 1 point for each correct answer. At the end of the Quiz, your total score will be displayed. Maximum score is 25 points.\nStart the Quiz\nGood luck!\nStart the DSA Quiz ❯\nIf you do not know DSA, we suggest that you read our DSA Tutorial from scratch.",
      "examples": []
    },
    {
      "title": "DSA Syllabus",
      "summary": "Introduction\nThe W3Schools Data Structures and Algorithms Tutorial is comprehensive and beginner-friendly.\nIt will give you a fundamental knowledge of data structures and algorithms.\nThis tutorial is designed for beginners and requires only basic programming knowledge.\nThe content has been carefully made to be bite-sized, simple, and easy to understand.\nThe content has been proven by millions of users over the years. It is updated and improved frequently.\nThe syllabus outline and its sequence are structured so you can learn DSA step by step, from basic data structures to advanced algorithms.\nGet Started With DSA »\nLearning Outcomes\nUnderstand basic data structures like arrays, linked lists, trees, and graphs.\nLearn how to sort and search data efficiently.\nAnalyze the performance of algorithms using time complexity.\nImprove program efficiency by selecting appropriate data structures.\nSolve real-world problems with effective algorithms.\nNote: Are you a teacher teaching DSA? W3Schools Academy is a toolbox of features that can help you teach. It offers classroom features such as pre-built study plans, classroom administration and much more. Read more about Academy here.\nWhich Subjects Are DSA Relevant For?\nSoftware Development:\nDSA is essential for efficient programming.\nProblem Solving:\nDSA provides tools for solving complex problems.\nSystem Design:\nDSA helps in designing scalable systems.\nCompetitive Programming:\nDSA is crucial for coding competitions.\nTechnical Interviews:\nDSA is a key topic in job interviews.\nPerformance Optimization:\nDSA enables writing efficient code.\nData Science:\nDSA helps in handling large datasets efficiently.\nGet Started\nActivities\nIn this tutorial we offer different activities for you to learn DSA for free:\nLessons\nExercises\nQuizzes\nSign in to Track Progress\nYou can also create a free account to track your progress.\nAs a signed-in user, you get access to features such as:\nLearning paths\nSandbox and lab environments\nAchievements\nAnd much more!\nSign Up - It's free\nOverview of the Modules\nDSA HOME\nDSA Intro\nDSA Simple Algorithm\nDSA Arrays\nDSA Bubble Sort\nDSA Selection Sort\nDSA Insertion Sort\nDSA Quick Sort\nDSA Counting Sort\nDSA Radix Sort\nDSA Merge Sort\nDSA Linear Search\nDSA Binary Search\nDSA Linked Lists\nDSA Linked Lists in Memory\nDSA Linked Lists Types\nLinked Lists Operations\nDSA Stacks\nDSA Queues\nDSA Hash Tables\nDSA Hash Sets\nDSA Hash Maps\nDSA Trees\nDSA Binary Trees\nDSA Pre-order Traversal\nDSA In-order Traversal\nDSA Post-order Traversal\nDSA Array Implementation\nDSA Binary Search Trees\nDSA AVL Trees\nDSA Graphs\nGraphs Implementation\nDSA Graphs Traversal\nDSA Cycle Detection\nDSA Shortest Path\nDSA Dijkstra's\nDSA Bellman-Ford\nMinimum Spanning Tree\nDSA Prim's\nDSA Kruskal's\nDSA Maximum Flow\nDSA Ford-Fulkerson\nDSA Edmonds-Karp\nDSA Time Complexity\nDSA Bubble Sort Time Complexity\nDSA Selection Sort Time Complexity\nDSA Insertion Sort Time Complexity\nDSA Time Complexity for Specific Algorithms\nDSA Counting Sort Time Complexity\nDSA Radix Sort Time Complexity\nDSA Merge Sort Time Complexity\nDSA Linear Search Time Complexity\nDSA Binary Search Time Complexity\nGet Started\nSandbox and Lab Environment\nDSA, like any programming topic, is best learned through hands-on practice.\nTry this example using our editor:\nExample\nIf you want to explore more and host your project, we have a feature called Spaces that allows you to practice DSA implementations for free.\nHere you get a secure sandbox environment called Spaces, where you can practice algorithms and test data structures in real-time.\nSpaces allow you to test, implement, and analyze code. This includes a W3Schools subdomain, hosting, and secure SSL certificates.\nSpaces require no installation and run directly in the browser.\nFeatures include:\nCollaboration\nFile navigator\nTerminal & log\nPackage manager\nDatabase\nEnvironment manager\nAnalytics\nCreate a Spaces Account\nDSA Certification\nW3Schools offers an end-of-pathway certification program.\nHere you can take exams to get certified.\nThe DSA exam is a test that summarizes the W3Schools DSA syllabus.\nAfter passing the exam you get the \"Certified DSA Developer\" Certification.\nThere are two different types of certifications:\nNon-adaptive\nAdaptive\nThe non-adaptive is pass or no pass.\nThe adaptive certification is adaptive and graded; students will get a grade from intermediate, advanced to professional.\nBuy Certificate »\nAre You a Teacher?\nAre you interested in learning how you can use W3Schools Academy to Teach DSA?\nWatch a demo of W3Schools Academy. You'll see how it works, and discover how it can make teaching programming easier and more engaging.\nWatch Demo »",
      "examples": [
        "my_array = [7, 12, 9, 4, 11] minVal = my_array[0] for i in my_array: if i < minVal: minVal = i print('Lowest value:',minVal)"
      ]
    },
    {
      "title": "DSA Study Plan",
      "summary": "Introduction\nThe DSA study plan helps you teach your students DSA step-by-step.\nCreating a study plan for DSA is easy.\nYou can use a pre-built study plan or customize it.\nStudents have different skill levels. The study plans can be customized to ensure that everyone is challenged.\nSave time with pre-built teacher materials and study plans. Easily organize your class with a timeline from the introduction of DSA to the final exam.\nW3Schools Academy\nThis study plan is a feature of W3Schools Academy.\nW3Schools Academy is a platform that has everything you need to teach coding, all in one place.\nIt offers you as a teacher a toolbox of features that helps you succeed with teaching in your classroom.\nYou need to have an active subscription to access the study plan feature. There are two different subscription tiers:\nEssentials ($1.99 / month per student)\nFull Access ($5.99 / month per student)\nCalculate your price and order here.\nLearn More »\nAcademy also offer other features such as:\nManaging your classroom\nTracking of student progress and reports\nLabs, assignments, and code challenges (prebuilt challenges or create your own ones)\nAuto-grading\nTeaching materials\nCertification exams\nGet a free demo »\nTeacher Materials\nW3Schools has everything you need to teach DSA.\nThe DSA training materials is available for you to include and use in your study plan:\nW3Schools DSA Tutorial\nDSA Exercises\nDSA Quiz\nDSA Challenges (Coding challenges)\nDSA Certification Exam (End of Pathway Exam)\nDSA Syllabus\nWith the Data Structures and Algorithms (DSA) Syllabus, your students will start with the basics, like understanding arrays and simple algorithms, and move to more advanced topics, like trees, graphs, and complex algorithmic techniques. Each chapter includes examples, try-it-yourself sections, exercises, and quizzes to make learning easy, interactive, and fun.\nRead more about DSA Syllabus.\nStudy Plan Overview\nThe study plan features are made to help teachers and students. They make learning easy, flexible, and fun. These features work for different types of classes, learning styles and student level.\nLearning Paths\nYou can add ready-made learning paths.\nThe learning paths are by default ordered by our recommended order.\nYou can change the order.\nYou can add custom activities with text, links, or multi-media files.\nDrag and drop or click to make changes to the path.\nYou can add ready-made learning paths.\nThe learning paths are by default ordered by our recommended order.\nYou can change the order.\nYou can add custom activities with text, links, or multi-media files.\nDrag and drop or click to make changes to the path.\nInteractive Content\nTutorials\nTry-its (test code snippets)\nExercises\nQuiz\nChallenges\nLabs\nTutorials\nTry-its (test code snippets)\nExercises\nQuiz\nChallenges\nLabs\nTimeline and Pace\nYou can set a timeline of your study plan (e.g., 4-week, 8-week, 12-week, 24-week plans).\nYou can decide the learning pace for your class.\nDifferent study plans can be assigned to different students in the same class.\nThe flexibility can help to make sure that everyone is challenged.\nYou can set a timeline of your study plan (e.g., 4-week, 8-week, 12-week, 24-week plans).\nYou can decide the learning pace for your class.\nDifferent study plans can be assigned to different students in the same class.\nThe flexibility can help to make sure that everyone is challenged.\nTrack Student Progress\nThere are tools to track student progress.\nThe analytic tools include: chapter progress, exercises results, quiz results, exam results, and much more.\nThe challenges can be auto-graded or manually graded. The results are available to you as a teacher.\nThere are tools to track student progress.\nThe analytic tools include: chapter progress, exercises results, quiz results, exam results, and much more.\nThe challenges can be auto-graded or manually graded. The results are available to you as a teacher.\nEnd of Pathway Exam\nThe DSA study plan aligns with the DSA Certification Exam.\nThe exam can be taken at the end of the study plan, at your selected date.\nThe exam summarizes the DSA Tutorial.\nYou get reports of the students' results.\nThe DSA study plan aligns with the DSA Certification Exam.\nThe exam can be taken at the end of the study plan, at your selected date.\nThe exam summarizes the DSA Tutorial.\nYou get reports of the students' results.\nAccessibility\nStudy plans and learning materials are accessible on desktops, tablets, and smartphones.\nThis ensures students can learn anytime, anywhere.\nStudy plans and learning materials are accessible on desktops, tablets, and smartphones.\nThis ensures students can learn anytime, anywhere.\nLearn More »\nSample Study Plan\nYou choose the timeline and pace of your study plans.\nSchools have different preferences.\nSome would like more intensive pace, e.g. 5 weeks, others 12 or more weeks.\nIt is completely up to you.\nFor example, this is how a 4-week DSA study plan could look like:\nWeek 1: Introduction, Arrays, Linked Lists\nWeek 2: Stacks & Queues, Hash Tables, Trees\nWeek 3: Graphs, Shortest Path\nWeek 4: Minimum Spanning Tree, Maximum Flow, Time Complexity, DSA Certification Exam\nImage of Sample DSA study plan:\nReady to get started?\nStart with DSA Study Plans today.\nGet Started »\nAre You a Teacher?\nAre you interested in learning how you can use W3Schools Academy to Teach DSA programming?\nWatch a demo of W3Schools Academy. You'll see how it works, and discover how it can make teaching programming easier and more engaging.\nWatch Demo »",
      "examples": []
    },
    {
      "title": "W3Schools DSA Certificate",
      "summary": "W3Schools offers an Online Certification Program.\nThe perfect solution for busy professionals who need to balance work, family, and career building.\nMore than 50 000 certificates already issued!\nGet Your Certificate »\nW3Schools offers an Online Certification Program.\nThe perfect solution for busy professionals who need to balance work, family, and career building.\nMore than 50 000 certificates already issued!\nGet Your Certificate »\nWho Should Consider Getting Certified?\nAny student or professional within the digital industry.\nCertifications are valuable assets to gain trust and demonstrate knowledge to your clients, current or future employers on a ever increasing competitive market.\nW3Schools is Trusted by Top Companies\nW3Schools has over two decades of experience with teaching coding online.\nOur certificates are recognized and valued by companies looking to employ skilled developers.\nSave Time and Money\nShow the world your coding skills by getting a certification.\nThe prices is a small fraction compared to the price of traditional education.\nDocument and validate your competence by getting certified!\nExam overview\nFee: 95 USD\nAchievable certification levels:\nIntermediate (40%)\nAdvanced (75%)\nProfessional (90%)\nNumber of questions:\nAdaptive, 60 on average\nRequirement to pass:\nMinimum 40% - Intermediate level\nTime limit: 60 minutes\nNumber of attempts to pass: 3\nExam deadline: None\nCertification Expiration: None\nFormat: Online, multiple choice\nRegister now »\nAdvance Faster in Your Career\nGetting a certificate proves your commitment to upgrading your skills.\nThe certificate can be added as credentials to your CV, Resume, LinkedIn profile, and so on.\nIt gives you the credibility needed for more responsibilities, larger projects, and a higher salary.\nKnowledge is power, especially in the current job market.\nDocumentation of your skills enables you to advance your career or helps you to start a new one.\nHow Does It Work?\nStudy for free at W3Schools.com\nStudy at your own speed\nTest your skills with W3Schools online quizzes\nApply for your certificate by paying an exam fee\nTake your exam online, at any time, and from any location\nGet Your Certificate and Share It With The World\nExample certificate:\nEach certificate gets a unique link that can be shared with others.\nValidate your certification with the link or QR code.\nCheck how it looks like in this Example.\nShare your certificate on Linked in the Certifications section in just one click!\nDocument Your Skills\nGetting a certificate proves your commitment to upgrade your skills, gives you the credibility needed for more responsibilities, larger projects, and a higher salary.\nGet Your Certificate »\nLooking to add multiple users?\nAre you an educator, manager or business owner looking for courses or certifications?\nWe are working with schools, companies and organizations from all over the world.\nGet courses and/or certifications for your team here.",
      "examples": []
    }
  ],
  "glossary": [
    "arrays",
    "backend",
    "binary search",
    "bubble sort",
    "counting sort",
    "dsa arrays",
    "dsa bellman-ford",
    "dsa edmonds-karp",
    "dsa exercises",
    "dsa ford-fulkerson",
    "dsa graphs",
    "dsa history",
    "dsa home",
    "dsa intro",
    "dsa memoization",
    "dsa queues",
    "dsa quiz",
    "dsa reference",
    "dsa stacks",
    "dsa syllabus",
    "dsa tabulation",
    "dsa trees",
    "exercise",
    "exercises",
    "free",
    "graphs",
    "graphs implementation",
    "hash tables",
    "insertion sort",
    "javascript",
    "linear search",
    "linked lists",
    "maximum flow",
    "merge sort",
    "note",
    "programs",
    "quick sort",
    "radix sort",
    "report error",
    "selection sort",
    "shortest path",
    "stacks queues",
    "time complexity",
    "trees",
    "w3.css",
    "w3schools spaces"
  ]
}