{
  "course_name": "Statistics",
  "description": "Welcome to this introductory course in Statistics. This course serves as an excellent primer to Data Analytics. We have created 36 tutorial pages for you to learn more about some of the most important concepts in Statistics. In our \"Try it Yourself\" editor, you can use Python modules and R code, and modify the code to see the result. With Python use the NumPy library mean() method to find the mean of the values 4,11,7,14: Sign Up for Free Note: This is an optional feature. You can study at W3Schools without creating an account.",
  "course_summary": [
    {
      "title": "Statistics Introduction",
      "summary": "Statistics gives us methods of gaining knowledge from data.\nWhat is Statistics Used for?\nStatistics is used in all kinds of science and business applications.\nStatistics gives us more accurate knowledge which helps us make better decisions.\nStatistics can focus on making predictions about what will happen in the future. It can also focus on explaining how different things are connected.\nNote: Good statistical explanations are also useful for predictions.\nTypical Steps of Statistical Methods\nGathering data\nDescribing and visualizing data\nMaking conclusions\nIt is important to keep all three steps in mind for any questions we want more knowledge about.\nKnowing which types of data are available can tell you what kinds of questions you can answer with statistical methods.\nKnowing which questions you want to answer can help guide what sort of data you need. A lot of data might be available, and knowing what to focus on is important.\nHow is Statistics Used?\nStatistics can be used to explain things in a precise way. You can use it to understand and make conclusions about the group that you want to know more about. This group is called the population.\nA population could be many different kinds of groups. It could be:\nAll of the people in a country\nAll the businesses in an industry\nAll the customers of a business\nAll people that play football who are older than 45\nand so on - it just depends on what you want to know about.\nGathering data about the population will give you a sample. This is a part of the whole population. Statistical methods are then used on that sample.\nThe results of the statistical methods from the sample is used to make conclusions about the population.\nNote: The word 'statistic' can also refer to specific bits of knowledge; like the average value of something.\nREMOVE ADS\nImportant Concepts in Statistics\nPredictions and Explanations\nPopulations and Samples\nParameters and Sample Statistics\nSampling Methods\nData Types\nMeasurement Level\nDescriptive Statistics\nRandom Variables\nUnivariate and Multivariate Statistics\nProbability Calculation\nProbability Distributions\nStatistical Inference\nParameter Estimation\nHypothesis Testing\nCorrelation\nRegression Analysis\nCausal Inference\nWe will cover these topics step by step in this tutorial.\nStatistics and Programming\nStatistical analysis is typically done with computers. Small amounts of data can be analyzed well, without computers.\nHistorically, all data analysis was performed manually. It was time-consuming and prone to errors.\nNowadays, programming and software is typically used for data analysis.\nIn this course, we will show examples of code to do statistics with the programming languages Python and R.",
      "examples": []
    },
    {
      "title": "Statistics - Gathering Data",
      "summary": "Gathering Data\nGathering data is the first step in statistical analysis.\nSay for example that you want to know something about all the people in France.\nThe population is then all of the people in France.\nIt is too much effort to gather information about all of the members of a population (e.g. all 67+ million people living in France). It is often much easier to collect a smaller group of that population and analyze that. This is called a sample.\nA representative sample\nThe sample needs to be similar to the whole population of France. It should have the same characteristics as the population. If you only include people named Jacques living in Paris who are 48 years old, the sample will not be similar to the whole population.\nSo for a good sample, you will need people from all over France, with different ages, professions, and so on.\nIf the members of the sample have similar characteristics (like age, profession, etc.) to the whole population of France, we say that the sample is representative of the population.\nA good representative sample is crucial for statistical methods.\nNote: Data from a proper sample is often just as good data from the whole population, as long as it is representative!\nA good sample allows you to make accurate conclusions about the whole population.",
      "examples": []
    },
    {
      "title": "Statistics - Describing Data",
      "summary": "Describing data is typically the second step of statistical analysis after gathering data.\nDescriptive Statistics\nThe information (data) from your sample or population can be visualized with graphs or summarized by numbers. This will show key information in a simpler way than just looking at raw data. It can help us understand how the data is distributed.\nGraphs can visually show the data distribution.\nExamples of graphs include:\nHistograms\nPie charts\nBar graphs\nBox plots\nSome graphs have a close connection to numerical summary statistics. Calculating those gives us the basis of these graphs.\nFor example, a box plot visually shows the quartiles of a data distribution.\nQuartiles are the data split into four equal size parts, or quarters. A quartile is one type of summary statistics.\nSummary statistics\nSummary statistics take a large amount of information and sums it up in a few key values.\nNumbers are calculated from the data which also describe the shape of the distributions. These are individual 'statistics'.\nSome important examples are:\nMean, median and mode\nRange and interquartile range\nQuartiles and percentiles\nStandard deviation and variance\nNote: Descriptive statistics is often presented as a part of statistical analysis.\nDescriptive statistics is also useful for guiding further analysis, giving insight into the data, and finding what is worth investigating more closely.",
      "examples": []
    },
    {
      "title": "Statistics - Making Conclusions",
      "summary": "Using statistics to make conclusions about a population is called statistical inference.\nStatistical Inference\nStatistics from the data in the sample is used to make conclusions about the whole population. This is a type of statistical inference.\nProbability theory is used to calculate the certainty that those statistics also apply to the population.\nWhen using a sample, there will always be some uncertainty about what the data looks like for the population.\nUncertainty is often expressed as confidence intervals.\nConfidence intervals are numerical ways of showing how likely it is that the true value of this statistic is within a certain range for the population.\nHypothesis testing is a another way of checking if a statement about a population is true. More precisely, it checks how likely it is that a hypothesis is true is based on the sample data.\nSome examples of statements or questions that can be checked with hypothesis testing:\nPeople in the Netherlands taller than people in Denmark\nDo people prefer Pepsi or Coke?\nDoes a new medicine cure a disease?\nNote: Confidence intervals and hypothesis testing are closely related and describe the same things in different ways. Both are widely used in science.\nCausal Inference\nCausal inference is used to investigate if something causes another thing.\nFor example: Does rain make plants grow?\nIf we think two things are related we can investigate to see if they correlate. Statistics can be used to find out how strong this relation is.\nEven if things are correlated, finding out of something is caused by other things can be difficult. It can be done with good experimental design or other special statistical techniques.\nNote: Good experimental design is often difficult to achieve because of ethical concerns or other practical reasons.",
      "examples": []
    },
    {
      "title": "Statistics - Prediction and Explanation",
      "summary": "Some types of statistical methods are focused on predicting what will happen.\nOther types of statistical methods are focused on explaining how things are connected.\nPrediction\nSome statistical methods are not focused on explaining how things are connected. Only the accuracy of prediction is important.\nMany statistical methods are successful at predicting without giving insight into how things are connected.\nSome types of machine learning let computers do the hard work, but the way they predict is difficult to understand. These approaches can also be vulnerable to mistakes if the circumstances change, since the how they work is less clear.\nNote: Predictions about future events are called forecasts. Not all predictions are about the future.\nSome predictions can be about something else that is unknown, even if it is not in the future.\nExplanation\nDifferent statistical methods are often used for explaining how things are connected. These statistical methods may not make good predictions.\nThese statistical methods often explain only small parts of the whole situation. But, if you only want to know how a few things are connected, the rest might not matter.\nIf these methods accurately explain how all the relevant things are connected, they will also be good at prediction. But managing to explain every detail is often challenging.\nSome times we are specifically interested in figuring out if one thing causes another. This is called causal inference.\nIf we are looking at complicated situations, many things are connected. To figure out what causes what, we need to untangle every way these things are connected.\nNote: Making conclusions about causality should be done carefully.",
      "examples": []
    },
    {
      "title": "Statistics - Populations and Samples",
      "summary": "The terms 'population' and 'sample' are important in statistics and refer to key concepts that are closely related.\nPopulation and Samples\nPopulation: Everything in the group that we want to learn about.\nSample: A part of the population.\nExamples of populations and a sample from those populations:\nFor good statistical analysis, the sample needs to be as \"similar\" as possible to the population. If they are similar enough, we say that the sample is representative of the population.\nThe sample is used to make conclusions about the whole population. If the sample is not similar enough to the whole population, the conclusions could be useless.\nNote: Many words have specific meanings in statistics.\nThe word 'population' normally refers to a group of people. In statistics, it is any specific group that we are interested in learning about.",
      "examples": []
    },
    {
      "title": "Statistics - Parameters and Statistics",
      "summary": "The terms 'parameter' and (sample) 'statistic' refer to key concepts that are closely related in statistics.\nThey are also directly connected to the concepts of populations and samples.\nParameters and Statistics\nParameter: A number that describes something about the whole population.\nSample statistic: A number that describes something about the sample.\nThe parameters are the key things we want to learn about. The parameters are usually unknown.\nSample statistics gives us estimates for parameters.\nThere will always be some uncertainty about how accurate estimates are. More certainty gives us more useful knowledge.\nFor every parameter we want to learn about we can get a sample and calculate a sample statistic, which gives us an estimate of the parameter.\nSome Important Examples\nMean, median and mode are different types of averages (typical values in a population).\nFor example:\nThe typical age of people in a country\nThe typical profits of a company\nThe typical range of an electric car\nVariance and standard deviation are two types of values describing how spread out the values are.\nA single class of students in a school would usually be about the same age. The age of the students will have low variance and standard deviation.\nA whole country will have people of all kinds of different ages. The variance and standard deviation of age in the whole country would then be bigger than in a single school grade.",
      "examples": []
    },
    {
      "title": "Statistics - Study Types",
      "summary": "A statistical study can be a part of the process of gathering data.\nThere are different types of studies. Some are better than others, but they might be harder to do.\nMain Types of Statistical Studies\nThe main types of statistical studies are observational and experimental studies.\nWe are often interested in knowing if something is the cause of another thing.\nExperimental studies are generally better than observational studies for investigating this, but usually require more effort.\nAn observational study is when observe and gather data without changing anything.\nExperimental Studies\nIn an experimental study, the circumstances around the sample is changed. Usually, we compare two groups from a population and these two groups are treated differently.\nOne example can be a medical study to see if a new medicine is effective.\nOne group receives the medicine and the other does not. These are the different circumstances around those samples.\nWe can compare the health of both groups afterwards and see if the results are different.\nExperimental studies can allow us to investigate causal relationships. A well designed experimental study can be useful since it can isolate the relationship we are interested in from other effects. Then we can be more confident that we are measuring the true effect.",
      "examples": []
    },
    {
      "title": "Statistics - Sample Types",
      "summary": "A study needs participants and there are different ways of gathering them.\nSome methods are better than others, but they might be more difficult.\nDifferent Types of Sampling Methods\nRandom Sampling\nA random sample is where every member of the population has an equal chance to be chosen.\nRandom sampling is the best. But, it can be difficult, or impossible, to make sure that it is completely random.\nNote: Every other sampling method is compared to how close it is to a random sample - the closer, the better.\nConvenience Sampling\nA convenience sample is where the participants that are the easiest to reach are chosen.\nNote: Convenience sampling is the easiest to do.\nIn many cases this sample will not be similar enough to the population, and the conclusions can potentially be useless.\nSystematic Sampling\nA systematic sample is where the participants are chosen by some regular system.\nFor example:\nThe first 30 people in a queue\nEvery third on a list\nThe first 10 and the last 10\nStratified Sampling\nA stratified sample is where the population is split into smaller groups called 'strata'.\nThe 'strata' can, for example, be based on demographics, like:\nDifferent age groups\nProfessions\nStratification of a sample is the first step. Another sampling method (like random sampling) is used for the second step of choosing participants from all of the smaller groups (strata).\nClustered Sampling\nA clustered sample is where the population is split into smaller groups called 'clusters'.\nThe clusters are usually natural, like different cities in a country.\nThe clusters are chosen randomly for the sample.\nAll members of the clusters can participate in the sample, or members can be chosen randomly from the clusters in a third step.",
      "examples": []
    },
    {
      "title": "Statistics - Data Types",
      "summary": "Data can be different types, and require different types of statistical methods to analyze.\nDifferent types of data\nThere are two main types of data: Qualitative (or 'categorical') and quantitative (or 'numerical'). These main types also have different sub-types depending on their measurement level.\nQualitative Data\nInformation about something that can be sorted into different categories that can't be described directly by numbers.\nExamples:\nBrands\nNationality\nProfessions\nWith categorical data we can calculate statistics like proportions. For example, the proportion of Indian people in the world, or the percent of people who prefer one brand to another.\nQuantitative Data\nInformation about something that is described by numbers.\nExamples:\nIncome\nAge\nHeight\nWith numerical data we can calculate statistics like the average income in a country, or the range of heights of players in a football team.",
      "examples": []
    },
    {
      "title": "Statistics - Measurement Levels",
      "summary": "Different data types have different measurement levels.\nMeasurement levels are important for what types of statistics can be calculated and how to best present the data.\nMeasurement Levels\nThe main types of data are Qualitative (categories) and Quantitative (numerical). These are further split into the following measurement levels.\nThese measurement levels are also called measurement 'scales'\nNominal Level\nCategories (qualitative data) without any order.\nExamples:\nBrand names\nCountries\nColors\nOrdinal level\nCategories that can be ordered (from low to high), but the precise \"distance\" between each is not meaningful.\nExamples:\nLetter grade scales from F to A\nMilitary ranks\nLevel of satisfaction with a product\nConsider letter grades from F to A: Is the grade A precisely twice as good as a B? And, is the grade B also twice as good as C?\nExactly how much distance it is between grades is not clear and precise. If the grades are based on amounts of points on a test, you can say that there is a precise \"distance\" on the point scale, but not the grades themselves.\nInterval Level\nData that can be ordered and the distance between them is objectively meaningful. But there is no natural 0-value where the scale originates.\nExamples:\nYears in a calendar\nTemperature measured in Fahrenheit\nNote: Interval scales are usually invented by people, like degrees of temperature.\n0 degrees Celsius is 32 degrees of Fahrenheit. There is consistent distances between each degree (for every 1 extra degree of Celsius, there is 1.8 extra Fahrenheit), but they do not agree on where 0 degrees is.\nRatio Level\nData that can be ordered and there is a consistent and meaningful distance between them. And it also has a natural 0-value.\nExamples:\nMoney\nAge\nTime\nData that is on the ratio level (or \"ratio scale\") gives us the most detailed information. Crucially, we can compare precisely how big one value is compared to another. This would be the ratio between these values, like twice as big, or ten times as small.",
      "examples": []
    },
    {
      "title": "Statistics - Descriptive Statistics",
      "summary": "Descriptive statistics gives us insight into data without having to look at all of it in detail.\nKey Features to Describe about Data\nGetting a quick overview of how the data is distributed is a important step in statistical methods.\nWe calculate key numerical values about the data that tells us about the distribution of the data. We also draw graphs showing visually how the data is distributed.\nKey Features of Data:\nWhere is the center of the data? (location)\nHow much does the data vary? (scale)\nWhat is the shape of the data? (shape)\nThese can be described by summary statistics (numerical values).\nThe Center of the Data\nThe center of the data is where most of the values are concentrated.\nDifferent kinds of averages, like mean, median and mode, are measures of the center.\nNote: Measures of the center are also called location parameters, because they tell us something about where data is 'located' on a number line.\nThe Variation of the Data\nThe variation of the data is how spread out the data are around the center.\nStatistics like standard deviation, range and quartiles are measures of variation.\nNote: Measures of variation are also called scale parameters.\nThe Shape of the Data\nThe shape of the data can refer to the how the data are bunched up on either side of the center.\nStatistics like skew describe if the right or left side of the center is bigger. Skew is one type of shape parameters.\nFrequency Tables\nOne typical of presenting data is with frequency tables.\nA frequency table counts and orders data into a table. Typically, the data will need to be sorted into intervals.\nFrequency tables are often the basis for making graphs to visually present the data.\nVisualizing Data\nDifferent types of graphs are used for different kinds of data. For example:\nPie charts for qualitative data\nHistograms for quantitative data\nScatter plots for bivariate data\nGraphs often have a close connection to numerical summary statistics.\nFor example, box plots show where the quartiles are.\nQuartiles also tell us where the minimum and maximum values, range, interquartile range, and median are.",
      "examples": []
    },
    {
      "title": "Statistics - Frequency Tables",
      "summary": "A frequency table is a way to present data. The data are counted and ordered to summarize larger sets of data.\nWith a frequency table you can analyze the way the data is distributed across different values.\nFrequency Tables\nFrequency means the number of times a value appears in the data. A table can quickly show us how many times each value appears.\nIf the data has many different values, it is easier to use intervals of values to present them in a table.\nHere is the age of the 934 Nobel Prize winners up until the year 2020. In the table each row is an age interval of 10 years.\nWe can see that there is only one winner from ages 10 to 19. And that the highest number of winners are in their 60s.\nNote: The intervals for the values are also called 'bins'.\nREMOVE ADS\nRelative Frequency Tables\nRelative frequency means the number of times a value appears in the data compared to the total amount. A percentage is a relative frequency.\nHere are the relative frequencies of ages of Noble Prize winners. Now, all the frequencies are divided by the total (934) to give percentages.\nCumulative Frequency Tables\nCumulative frequency counts up to a particular value.\nHere are the cumulative frequencies of ages of Nobel Prize winners. Now, we can see how many winners have been younger than a certain age.\nCumulative frequency tables can also be made with relative frequencies (percentages).",
      "examples": []
    },
    {
      "title": "Statistics - Histograms",
      "summary": "A histogram visually presents quantitative data.\nHistograms\nA histogram is a widely used graph to show the distribution of quantitative (numerical) data.\nIt shows the frequency of values in the data, usually in intervals of values. Frequency is the amount of times that value appeared in the data.\nEach interval is represented with a bar, placed next to the other intervals on a number line.\nThe height of the bar represents the frequency of values in that interval.\nHere is a histogram of the age of all 934 Nobel Prize winners up to the year 2020:\nThis histogram uses age intervals from 10 to 19, 20 to 29, and so on.\nNote: Histograms are similar to bar graphs, which are used for qualitative data.\nREMOVE ADS\nBin Width\nThe intervals of values are often called 'bins'. And the length of an interval is called 'bin width'.\nWe can choose any width. It is best with a bin width that shows enough detail without being confusing.\nHere is a histogram of the same Nobel Prize winner data, but with bin widths of 5 instead of 10:\nThis histogram uses age intervals from from 15 to 19, 20 to 24, 25 to 29, and so on.\nSmaller intervals gives a more detailed look at the distribution of the age values in the data.",
      "examples": []
    },
    {
      "title": "Statistics - Bar Graphs",
      "summary": "A bar graph visually presents qualitative data.\nBar Graphs\nBar graphs are used show the distribution of qualitative (categorical) data.\nIt shows the frequency of values in the data. Frequency is the amount of times that value appeared in the data.\nEach category is represented with a bar. The height of the bar represents the frequency of values from that category in the data.\nHere is a bar graph of the number of people who have won a Nobel Prize in each category up to the year 2020:\nSome of the categories have existed longer than others. Multiple winners are also more common in some categories. So there is a different number of winners in each category.\nNote: Bar graphs are similar to histograms, which are used for quantitative data.",
      "examples": []
    },
    {
      "title": "Statistics - Pie Charts",
      "summary": "A pie chart visually presents qualitative data.\nPie Charts\nPie graphs are used to show the distribution of qualitative (categorical) data.\nIt shows the frequency or relative frequency of values in the data.\nFrequency is the amount of times that value appeared in the data. Relative frequency is the percentage of the total.\nEach category is represented with a slice in the 'pie' (circle). The size of each slice represents the frequency of values from that category in the data.\nHere is a pie chart of the number of people who have won a Nobel Prize in each category up to the year 2020:\nThis pie chart shows relative frequency. So each slice is sized by the percentage for each category.\nSome of the categories have existed longer than others. Multiple winners are also more common in some categories. So there is a different number of winners in each category.",
      "examples": []
    },
    {
      "title": "Statistics - Box Plots",
      "summary": "A box plot is a graph used to show key features of quantitative data.\nBox Plots\nA box plot is a good way to show many important features of quantitative (numerical) data.\nIt shows the median of the data. This is the middle value of the data and one type of an average value.\nIt also shows the range and the quartiles of the data. This tells us something about how spread out the data is.\nHere is a box plot of the age of all the Nobel Prize winners up to the year 2020:\nThe median is the red line through the middle of the 'box'. We can see that this is just above the number 60 on the number line below. So the middle value of age is 60 years.\nThe left side of the box is the 1st quartile. This is the value that separates the first quarter, or 25% of the data, from the rest. Here, this is 51 years.\nThe right side of the box is the 3rd quartile. This is the value that separates the first three quarters, or 75% of the data, from the rest. Here, this is 69 years.\nThe distance between the sides of the box is called the inter-quartile range (IQR). This tells us where the 'middle half' of the values are. Here, half of the winners were between 51 and 69 years.\nThe ends of the lines from the box at the left and the right are the minimum and maximum values in the data. The distance between these is called the range.\nThe youngest winner was 17 years old, and the oldest was 97 years old. So the range of the age of winners was 80 years.\nNote: Box plots are also called 'box and whiskers plots'.",
      "examples": []
    },
    {
      "title": "Statistics - Average",
      "summary": "An average is a measure of where most of the values in the data are located.\nThe Center of the Data\nThe center of the data is where most of the values in the data are located. Averages are measures of the location of the center.\nThere are different types of averages. The most commonly used are:\nMean\nMedian\nMode\nNote: In statistics, averages are often referred to as 'measures of central tendency'.\nFor example, using the values:\n40, 21, 55, 21, 48, 13, 72\nMean\nThe mean is usually referred to as 'the average'.\nThe mean is the sum of all the values in the data divided by the total number of values in the data:\n(40 + 21 + 55 + 31 + 48 + 13 + 72)/7 =\n38.57\nNote: There are are multiple types of mean values. The most common type of mean is the arithmetic mean.\nIn this tutorial, 'mean' refers to the arithmetic mean.\nREMOVE ADS\nMedian\nThe median is the 'middle value' of the data.\nThe median is found by ordering all the values in the data and picking the middle value:\n13, 21, 21,\n40\n, 48, 55, 72\nThe median is less influenced by extreme values in the data than the mean.\nChanging the last value to 356 does not change the median:\n13, 21, 21,\n40\n, 48, 55, 356\nThe median is still 40.\nChanging the last value to 356 changes the mean a lot:\n(13 + 21 + 21 + 40 + 48 + 55 + 72)/7 =\n38.57\n(13 + 21 + 21 + 40 + 48 + 55 + 356)/7 =\n79.14\nNote: Extreme values are values in the data that are much smaller or larger than the average values in the data.\nMode\nThe mode is the value(s) that appears most often in the data:\n40,\n21\n, 55,\n21\n, 48, 13, 72\nHere, 21 appears two times, and the other values only once. The mode of this data is 21.\nThe mode is also used for categorical data, unlike the median and mean. Categorical data can't be described directly with numbers, like names:\nAlice,\nJohn\n, Bob, Maria,\nJohn\n, Julia, Carol\nHere, John appears two times, and the other values only once. The mode of this data is John.\nNote: There can be more than one mode if multiple values appear the same number of times in the data.",
      "examples": [
        "40, 21, 55, 21, 48, 13, 72",
        "(40 + 21 + 55 + 31 + 48 + 13 + 72)/7 =\n38.57",
        "13, 21, 21,\n40\n, 48, 55, 72",
        "13, 21, 21,\n40\n, 48, 55, 356",
        "(13 + 21 + 21 + 40 + 48 + 55 + 72)/7 =\n38.57\n(13 + 21 + 21 + 40 + 48 + 55 + 356)/7 =\n79.14",
        "40,\n21\n, 55,\n21\n, 48, 13, 72",
        "Alice,\nJohn\n, Bob, Maria,\nJohn\n, Julia, Carol"
      ]
    },
    {
      "title": "Statistics - Mean",
      "summary": "The mean is a type of average value, which describes where center of the data is located.\nMean\nThe mean is usually referred to as 'the average'.\nThe mean is the sum of all the values in the data divided by the total number of values in the data.\nThe mean is calculated for numerical variables. A variable is something in the data that can vary, like:\nAge\nHeight\nIncome\nNote: There are are multiple types of mean values. The most common type of mean is the arithmetic mean.\nIn this tutorial 'mean' refers to the arithmetic mean.\nCalculating the Mean\nYou can calculate the mean for both the population and the sample.\nThe formulas are the same and uses different symbols to refer to the population mean (\nμ\n) and sample mean (\nx\n¯\n).\nCalculating the population mean (\nμ\n) is done with this formula:\nμ\n=\n∑\nx\ni\nn\nCalculating the sample mean (\nx\n¯\n) is done with this formula:\nx\n¯\n=\n∑\nx\ni\nn\nThe bottom part of the fraction (\nn\n) is the total number of observations.\n∑\nis the symbol for adding together a list of numbers.\nx\ni\nis the list of values in the data:\nx\n1\n,\nx\n2\n,\nx\n3\n,\n…\nThe top part of the fraction (\n∑\nx\ni\n) is the sum of\nx\n1\n,\nx\n2\n,\nx\n3\n,\n…\nadded together.\nSo, if a sample has 4 observations with values: 4, 11, 7, 14 the calculation is:\nx\n¯\n=\n4\n+\n11\n+\n7\n+\n14\n4\n=\n36\n4\n=\n9\n―\nREMOVE ADS\nCalculation with Programming\nThe mean can easily be calculated with many programming languages.\nUsing software and programming to calculate statistics is more common for bigger sets of data, as calculating by hand becomes difficult.\nExampleGet your own Python Server\nWith Python use the NumPy library mean() method to find the mean of the values 4,11,7,14:\nExample\nUse the R mean() function to find the mean of the values 4,11,7,14:\nStatistics Symbol Reference",
      "examples": [
        "import numpy\n\nvalues = [4,11,7,14]\n\nx = numpy.mean(values)\n\nprint(x)",
        "values <- c(4,7,11,14)\n\nmean(values)",
        "mean()"
      ]
    },
    {
      "title": "Statistics - Median",
      "summary": "The median is a type of average value, which describes where the center of the data is located.\nMedian\nThe median is the middle value in a data set ordered from low to high.\nFinding the Median\nThe median can only be calculated for numerical variables.\nThe formula for finding the middle value is:\nn\n+\n1\n2\nWhere\nn\nis the total number of observations.\nIf the total number of observations is an odd number, the formula gives a whole number and the value of this observation is the median.\n13, 21, 21,\n40\n, 48, 55, 72\nHere, there are 7 total observations, so the median is the 4th value:\n7\n+\n1\n2\n=\n8\n2\n=\n4\nThe 4th value in the ordered list is 40, so that is the median.\nIf the total number of observations is an even number, the formula gives a decimal number between two observations.\n13, 21, 21,\n40, 42\n, 48, 55, 72\nHere, there are 8 total observations, so the median is between the 4th and 5th values:\n8\n+\n1\n2\n=\n9\n2\n=\n4.5\nThe 4th and 5th values in the ordered list is 40 and 42, so the median is the mean of these two values. That is, the sum of those two values divided by 2:\n40\n+\n42\n2\n=\n82\n2\n=\n41\n―\nNote: It is important that the numbers are ordered before you can find the median.\nREMOVE ADS\nFinding the Median with Programming\nThe median can easily be found with many programming languages.\nUsing software and programming to calculate statistics is more common for bigger sets of data, as finding it manually becomes difficult.\nExampleGet your own Python Server\nWith Python use the NumPy library median() method to find the median of the values 13, 21, 21, 40, 42, 48, 55, 72:\nExample\nUse the R median() function to find the median of the values 13, 21, 21, 40, 42, 48, 55, 72:",
      "examples": [
        "13, 21, 21,\n40\n, 48, 55, 72",
        "13, 21, 21,\n40, 42\n, 48, 55, 72",
        "import numpy\n\nvalues = [13,21,21,40,42,48,55,72]\n\nx = numpy.median(values)\n\nprint(x)",
        "values <- c(13,21,21,40,42,48,55,72)\n\nmedian(values)",
        "median()"
      ]
    },
    {
      "title": "Statistics - Mode",
      "summary": "The mode is a type of average value, which describes where most of the data is located.\nMode\nThe mode is the value(s) that are the most common in the data.\nA dataset can have multiple values that are modes.\nA distribution of values with only one mode is called unimodal.\nA distribution of values with two modes is called bimodal. In general, a distribution with more than one mode is called multimodal.\nMode can be found for both categorical and numerical data.\nFinding the Mode\nHere is a numerical example:\n4,\n7\n, 3, 8, 11,\n7\n, 10, 19, 6, 9,\n12\n,\n12\nBoth 7 and 12 appears two times each, and the other values only once. The modes of this data is 7 and 12.\nHere is a categorical example with names:\nAlice,\nJohn\n, Bob, Maria,\nJohn\n, Julia, Carol\nJohn appears two times, and the other values only once. The mode of this data is John.\nFinding the Mode with Programming\nThe mode can easily be found with many programming languages.\nUsing software and programming to calculate statistics is more common for bigger sets of data, as calculating manually becomes difficult.\nExampleGet your own Python Server\nWith Python use the statistics library multimode() method to find the modes of the values 4,7,3,8,11,7,10,19,6,9,12,12:\nExample\nUsing R with a user-defined function to find the modes of the values 4,7,3,8,11,7,10,19,6,9,12,12:\nNote: R has no built-in function to find the mode.",
      "examples": [
        "4,\n7\n, 3, 8, 11,\n7\n, 10, 19, 6, 9,\n12\n,\n12",
        "Alice,\nJohn\n, Bob, Maria,\nJohn\n, Julia, Carol",
        "from statistics import multimode\n\nvalues = [4,7,3,8,11,7,10,19,6,9,12,12]\n\nx = multimode(values)\n\nprint(x)",
        "mode <- function(x) {\nunique_values <- unique(x)\ntable <- tabulate(match(x, unique_values))\nunique_values[table == max(table)]\n}\n\nvalues <- c(4,7,3,8,11,7,10,19,6,9,12,12)\n\nmode(values)",
        "multimode()"
      ]
    },
    {
      "title": "Statistics - Variation",
      "summary": "Variation is a measure of how spread out the data is around the center of the data.\nThe Variation of the Data\nMeasures of variation are statistics of how far away the values in the observations (data points) are from each other.\nThere are different measures of variation. The most commonly used are:\nRange\nQuartiles and Percentiles\nInterquartile Range\nStandard Deviation\nMeasures of variation combined with an average (measure of center) gives a good picture of the distribution of the data.\nNote: These measures of variation can only be calculated for numerical data.\nRange\nThe range is the difference between the smallest and the largest value of the data.\nRange is the simplest measure of variation.\nHere is a histogram of the age of all 934 Nobel Prize winners up to the year 2020, showing the range:\nThe youngest winner was 17 years and the oldest was 97 years. The range of ages for Nobel Prize winners is then 80 years.\nQuartiles and Percentiles\nQuartiles and percentiles are ways of separating equal numbers of values in the data into parts.\nQuartiles are values that separate the data into four equal parts.\nPercentiles are values that separate the data into 100 equal parts.\nHere is a histogram of the age of all 934 Nobel Prize winners up to the year 2020, showing the quartiles:\nThe quartiles (Q0,Q1,Q2,Q3,Q4) are the values that separate each quarter.\nBetween Q0 and Q1 are the 25% lowest values in the data. Between Q1 and Q2 are the next 25%. And so on.\nQ0 is the smallest value in the data.\nQ2 is the middle value (median).\nQ4 is the largest value in the data.\nREMOVE ADS\nInterquartile Range\nInterquartile range is the difference between the first and third quartiles (Q1 and Q3).\nThe 'middle half' of the data is between the first and third quartile.\nHere is a histogram of the age of all 934 Nobel Prize winners up to the year 2020, showing the interquartile range (IQR):\nHere, the middle half of is between 51 and 69 years. The interquartile range for Nobel Prize winners is then 18 years.\nStandard Deviation\nStandard deviation is the most used measure of variation.\nStandard deviation (σ) measures how far a 'typical' observation is from the average of the data (μ).\nStandard deviation is important for many statistical methods.\nHere is a histogram of the age of all 934 Nobel Prize winners up to the year 2020, showing standard deviations:\nNote: Values within one standard deviation (σ) are considered to be typical.\nValues outside three standard deviations are considered to be outliers.",
      "examples": []
    },
    {
      "title": "Statistics - Range",
      "summary": "The range is a measure of variation, which describes how spread out the data is.\nRange\nThe range is the difference between the smallest and the largest value of the data.\nRange is the simplest measure of variation.\nHere is a histogram of the age of all 934 Nobel Prize winners up to the year 2020, showing the range:\nThe youngest winner was 17 years and the oldest was 97 years. The range of ages for Nobel Prize winners is then 80 years.\nCalculating the Range\nThe range can only be calculated for numerical data.\nFirst, find the smallest and largest values of this example:\n13\n, 21, 21, 40, 48, 55,\n72\nCalculate the difference by subtracting the smallest from the largest:\n72 - 13 =\n59\nREMOVE ADS\nCalculating the Range with Programming\nThe range can easily be found with many programming languages.\nUsing software and programming to calculate statistics is more common for bigger sets of data, as finding it manually becomes difficult.\nExampleGet your own Python Server\nWith Python use the NumPy library ptp() method to find the range of the values 13, 21, 21, 40, 48, 55, 72:\nExample\nUse the R min() and max() functions to find the range of the values 13, 21, 21, 40, 48, 55, 72:\nNote: The range() function in R returns the smallest and largest values.",
      "examples": [
        "13\n, 21, 21, 40, 48, 55,\n72",
        "import numpy\n\nvalues = [13,21,21,40,48,55,72]\n\nx = numpy.ptp(values)\n\nprint(x)",
        "values <- c(13,21,21,40,48,55,72)\n\nmax(values) - min(values)",
        "ptp()",
        "min()",
        "max()",
        "range()"
      ]
    },
    {
      "title": "Statistics - Quartiles and Percentiles",
      "summary": "Quartiles and percentiles are measures of variation, which describes how spread out the data is.\nQuartiles and percentiles are both types of quantiles.\nQuartiles\nQuartiles are values that separate the data into four equal parts.\nHere is a histogram of the age of all 934 Nobel Prize winners up to the year 2020, showing the quartiles:\nThe quartiles (Q0,Q1,Q2,Q3,Q4) are the values that separate each quarter.\nBetween Q0 and Q1 are the 25% lowest values in the data. Between Q1 and Q2 are the next 25%. And so on.\nQ0 is the smallest value in the data.\nQ1 is the value separating the first quarter from the second quarter of the data.\nQ2 is the middle value (median), separating the bottom from the top half.\nQ3 is the value separating the third quarter from the fourth quarter\nQ4 is the largest value in the data.\nCalculating Quartiles with Programming\nQuartiles can easily be found with many programming languages.\nUsing software and programming to calculate statistics is more common for bigger sets of data, as finding it manually becomes difficult.\nExampleGet your own Python Server\nWith Python use the NumPy library quantile() method to find the quartiles of the values 13, 21, 21, 40, 42, 48, 55, 72:\nExample\nUse the R quantile() function to find the quantiles of the values 13, 21, 21, 40, 42, 48, 55, 72:\nPercentiles\nPercentiles are values that separate the data into 100 equal parts.\nFor example, The 95th percentile separates the lowest 95% of the values from the top 5%\nThe 25th percentile (P25%) is the same as the first quartile (Q1).\nThe 50th percentile (P50%) is the same as the second quartile (Q2) and the median.\nThe 75th percentile (P75%) is the same as the third quartile (Q3)\nREMOVE ADS\nCalculating Percentiles with Programming\nPercentiles can easily be found with many programming languages.\nUsing software and programming to calculate statistics is more common for bigger sets of data, as finding it manually becomes difficult.\nExample\nWith Python use the NumPy library percentile() method to find the 65th percentile of the values 13, 21, 21, 40, 42, 48, 55, 72:\nExample\nUse the R quantile() function to find the 65th percentile (0.65) of the values 13, 21, 21, 40, 42, 48, 55, 72:",
      "examples": [
        "import numpy\n\nvalues = [13,21,21,40,42,48,55,72]\n\nx = numpy.quantile(values, [0,0.25,0.5,0.75,1])\n\nprint(x)",
        "values <- c(13,21,21,40,42,48,55,72)\n\nquantile(values)",
        "import numpy\n\nvalues = [13,21,21,40,42,48,55,72]\n\nx = numpy.percentile(values, 65)\n\nprint(x)",
        "values <- c(13,21,21,40,42,48,55,72)\n\nquantile(values, 0.65)",
        "quantile()",
        "percentile()",
        "65",
        "0.65"
      ]
    },
    {
      "title": "Statistics - Interquartile Range",
      "summary": "Interquartile range is a measure of variation, which describes how spread out the data is.\nInterquartile Range\nInterquartile range is the difference between the first and third quartiles (Q1 and Q3).\nThe 'middle half' of the data is between the first and third quartile.\nThe first quartile is the value in the data that separates the bottom 25% of values from the top 75%.\nThe third quartile is the value in the data that separates the bottom 75% of the values from the top 25%\nHere is a histogram of the age of all 934 Nobel Prize winners up to the year 2020, showing the interquartile range (IQR):\nHere, the middle half of is between 51 and 69 years. The interquartile range for Nobel Prize winners is then 18 years.\nREMOVE ADS\nCalculating the Interquartile Range with Programming\nThe interquartile range can easily be found with many programming languages.\nUsing software and programming to calculate statistics is more common for bigger sets of data, as finding it manually becomes difficult.\nExampleGet your own Python Server\nWith Python use the SciPy library iqr() method to find the interquartile range of the values 13, 21, 21, 40, 42, 48, 55, 72:\nExample\nUse the R IQR() function to find the interquartile range of the values 13, 21, 21, 40, 42, 48, 55, 72:",
      "examples": [
        "from scipy import stats\n\nvalues = [13,21,21,40,42,48,55,72]\n\nx = stats.iqr(values)\n\nprint(x)",
        "values <- c(13,21,21,40,42,48,55,72)\n\nIQR(values)",
        "iqr()",
        "IQR()"
      ]
    },
    {
      "title": "Statistics - Standard Deviation",
      "summary": "Standard deviation is the most commonly used measure of variation, which describes how spread out the data is.\nStandard Deviation\nStandard deviation (σ) measures how far a 'typical' observation is from the average of the data (μ).\nStandard deviation is important for many statistical methods.\nHere is a histogram of the age of all 934 Nobel Prize winners up to the year 2020, showing standard deviations:\nEach dotted line in the histogram shows a shift of one extra standard deviation.\nIf the data is normally distributed:\nRoughly 68.3% of the data is within 1 standard deviation of the average (from μ-1σ to μ+1σ)\nRoughly 95.5% of the data is within 2 standard deviations of the average (from μ-2σ to μ+2σ)\nRoughly 99.7% of the data is within 3 standard deviations of the average (from μ-3σ to μ+3σ)\nNote: A normal distribution has a \"bell\" shape and spreads out equally on both sides.\nCalculating the Standard Deviation\nYou can calculate the standard deviation for both the population and the sample.\nThe formulas are almost the same and uses different symbols to refer to the standard deviation (\nσ\n) and sample standard deviation (\ns\n).\nCalculating the standard deviation (\nσ\n) is done with this formula:\nσ\n=\n∑\n(\nx\ni\n−\nμ\n)\n2\nn\nCalculating the sample standard deviation (\ns\n) is done with this formula:\ns\n=\n∑\n(\nx\ni\n−\nx\n¯\n)\n2\nn\n−\n1\nn\nis the total number of observations.\n∑\nis the symbol for adding together a list of numbers.\nx\ni\nis the list of values in the data:\nx\n1\n,\nx\n2\n,\nx\n3\n,\n…\nμ\nis the population mean and\nx\n¯\nis the sample mean (average value).\n(\nx\ni\n−\nμ\n)\nand\n(\nx\ni\n−\nx\n¯\n)\nare the differences between the values of the observations (\nx\ni\n) and the mean.\nEach difference is squared and added together.\nThen the sum is divided by\nn\nor (\nn\n−\n1\n) and then we find the square root.\nUsing these 4 example values for calculating the population standard deviation:\n4, 11, 7, 14\nWe must first find the mean:\nμ\n=\n∑\nx\ni\nn\n=\n4\n+\n11\n+\n7\n+\n14\n4\n=\n36\n4\n=\n9\n―\nThen we find the difference between each value and the mean\n(\nx\ni\n−\nμ\n)\n:\n4\n−\n9\n=\n−\n5\n11\n−\n9\n=\n2\n7\n−\n9\n=\n−\n2\n14\n−\n9\n=\n5\nEach value is then squared, or multiplied with itself\n(\nx\ni\n−\nμ\n)\n2\n:\n(\n−\n5\n)\n2\n=\n(\n−\n5\n)\n(\n−\n5\n)\n=\n25\n2\n2\n=\n2\n∗\n2\n=\n4\n(\n−\n2\n)\n2\n=\n(\n−\n2\n)\n(\n−\n2\n)\n=\n4\n5\n2\n=\n5\n∗\n5\n=\n25\nAll of the squared differences are then added together\n∑\n(\nx\ni\n−\nμ\n)\n2\n:\n25\n+\n4\n+\n4\n+\n25\n=\n58\nThen the sum is divided by the total number of observations,\nn\n:\n58\n4\n=\n14.5\nFinally, we take the square root of this number:\n14.5\n≈\n3.81\n―\nSo, the standard deviation of the example values is roughly:\n3.81\nREMOVE ADS\nCalculating the Standard Deviation with Programming\nThe standard deviation can easily be calculated with many programming languages.\nUsing software and programming to calculate statistics is more common for bigger sets of data, as calculating by hand becomes difficult.\nPopulation Standard Deviation\nExampleGet your own Python Server\nWith Python use the NumPy library std() method to find the standard deviation of the values 4,11,7,14:\nExample\nUse an R formula to find the standard deviation of the values 4,11,7,14:\nSample Standard Deviation\nExample\nWith Python use the NumPy library std() method to find the sample standard deviation of the values 4,11,7,14:\nExample\nUse the R sd() function to find the sample standard deviation of the values 4,11,7,14:\nStatistics Symbol Reference",
      "examples": [
        "4, 11, 7, 14",
        "import numpy\n\nvalues = [4,11,7,14]\n\nx = numpy.std(values)\n\nprint(x)",
        "values <- c(4,7,11,14)\n\nsqrt(mean((values-mean(values))^2))",
        "import numpy\n\nvalues = [4,11,7,14]\n\nx = numpy.std(values, ddof=1)\n\nprint(x)",
        "values <- c(4,7,11,14)\n\nsd(values)",
        "std()",
        "sd()"
      ]
    },
    {
      "title": "Statistics - Statistical Inference",
      "summary": "Statistical Inference\nUsing data analysis and statistics to make conclusions about a population is called statistical inference.\nThe main types of statistical inference are:\nEstimation\nHypothesis testing\nEstimation\nStatistics from a sample are used to estimate population parameters.\nThe most likely value is called a point estimate.\nThere is always uncertainty when estimating.\nThe uncertainty is often expressed as confidence intervals defined by a likely lowest and highest value for the parameter.\nAn example could be a confidence interval for the number of bicycles a Dutch person owns:\n\"The average number of bikes a Dutch person owns is between 3.5 and 6.\"\nHypothesis Testing\nHypothesis testing is a method to check if a claim about a population is true. More precisely, it checks how likely it is that a hypothesis is true is based on the sample data.\nThere are different types of hypothesis testing.\nThe steps of the test depends on:\nType of data (categorical or numerical)\nIf you are looking at:\nA single group\nComparing one group to another\nComparing the same group before and after a change\nA single group\nComparing one group to another\nComparing the same group before and after a change\nSome examples of claims or questions that can be checked with hypothesis testing:\n90% of Australians are left handed\nIs the average weight of dogs more than 40kg?\nDo doctors make more money than lawyers?\nProbability Distributions\nStatistical inference methods rely on probability calculation and probability distributions.\nYou will learn about the most important probability distributions in the next pages.",
      "examples": [
        "\"The average number of bikes a Dutch person owns is between 3.5 and 6.\"",
        "90% of Australians are left handed\nIs the average weight of dogs more than 40kg?\nDo doctors make more money than lawyers?"
      ]
    },
    {
      "title": "Statistics - Normal Distribution",
      "summary": "The normal distribution is an important probability distribution used in statistics.\nMany real world examples of data are normally distributed.\nNormal Distribution\nThe normal distribution is described by the mean (\nμ\n) and the standard deviation (\nσ\n).\nThe normal distribution is often referred to as a 'bell curve' because of it's shape:\nMost of the values are around the center (\nμ\n)\nThe median and mean are equal\nIt has only one mode\nIt is symmetric, meaning it decreases the same amount on the left and the right of the center\nThe area under the curve of the normal distribution represents probabilities for the data.\nThe area under the whole curve is equal to 1, or 100%\nHere is a graph of a normal distribution with probabilities between standard deviations (\nσ\n):\nRoughly 68.3% of the data is within 1 standard deviation of the average (from μ-1σ to μ+1σ)\nRoughly 95.5% of the data is within 2 standard deviations of the average (from μ-2σ to μ+2σ)\nRoughly 99.7% of the data is within 3 standard deviations of the average (from μ-3σ to μ+3σ)\nNote: Probabilities of the normal distribution can only be calculated for intervals (between two values).\nDifferent Mean and Standard Deviations\nThe mean describes where the center of the normal distribution is.\nHere is a graph showing three different normal distributions with the same standard deviation but different means.\nThe standard deviation describes how spread out the normal distribution is.\nHere is a graph showing three different normal distributions with the same mean but different standard deviations.\nThe purple curve has the biggest standard deviation and the black curve has the smallest standard deviation.\nThe area under each of the curves is still 1, or 100%.\nA Real Data Example of Normally Distributed Data\nReal world data is often normally distributed.\nHere is a histogram of the age of Nobel Prize winners when they won the prize:\nThe normal distribution drawn on top of the histogram is based on the population mean (\nμ\n) and standard deviation (\nσ\n) of the real data.\nWe can see that the histogram close to a normal distribution.\nExamples of real world variables that can be normally distributed:\nTest scores\nHeight\nBirth weight\nREMOVE ADS\nProbability Distributions\nProbability distributions are functions that calculates the probabilities of the outcomes of random variables.\nTypical examples of random variables are coin tosses and dice rolls.\nHere is an graph showing the results of a growing number of coin tosses and the expected values of the results (heads or tails).\nThe expected values of the coin toss is the probability distribution of the coin toss.\nNotice how the result of random coin tosses gets closer to the expected values (50%) as the number of tosses increases.\nSimilarly, here is a graph showing the results of a growing number of dice rolls and the expected values of the results (from 1 to 6).\nNotice again how the result of random dice rolls gets closer to the expected values (1/6, or 16.666%) as the number of rolls increases.\nWhen the random variable is a sum of dice rolls the results and expected values take a different shape.\nThe different shape comes from there being more ways of getting a sum of near the middle, than a small or large sum.\nAs we keep increasing the number of dice for a sum the shape of the results and expected values look more and more like a normal distribution.\nMany real world variables follow a similar pattern and naturally form normal distributions.\nNormally distributed variables can be analyzed with well-known techniques.\nYou will learn about some of the most common and useful techniques in the following pages.",
      "examples": []
    },
    {
      "title": "Statistics - Standard Normal Distribution",
      "summary": "The standard normal distribution is a normal distribution where the mean is 0 and the standard deviation is 1.\nStandard Normal Distribution\nNormally distributed data can be transformed into a standard normal distribution.\nStandardizing normally distributed data makes it easier to compare different sets of data.\nThe standard normal distribution is used for:\nCalculating confidence intervals\nHypothesis tests\nHere is a graph of the standard normal distribution with probability values (p-values) between the standard deviations:\nStandardizing makes it easier to calculate probabilities.\nThe functions for calculating probabilities are complex and difficult to calculate by hand.\nTypically, probabilities are found by looking up tables of pre-calculated values, or by using software and programming.\nThe standard normal distribution is also called the 'Z-distribution' and the values are called 'Z-values' (or Z-scores).\nZ-Values\nZ-values express how many standard deviations from the mean a value is.\nThe formula for calculating a Z-value is:\nZ\n=\nx\n−\nμ\nσ\nx\nis the value we are standardizing,\nμ\nis the mean, and\nσ\nis the standard deviation.\nFor example, if we know that:\nThe mean height of people in Germany is 170 cm (\nμ\n)\nThe standard deviation of the height of people in Germany is 10 cm (\nσ\n)\nBob is 200 cm tall (\nx\n)\nBob is 30 cm taller than the average person in Germany.\n30 cm is 3 times 10 cm. So Bob's height is 3 standard deviations larger than mean height in Germany.\nUsing the formula:\nZ\n=\nx\n−\nμ\nσ\n=\n200\n−\n170\n10\n=\n30\n10\n=\n3\n―\nThe Z-value of Bob's height (200 cm) is 3.\nREMOVE ADS\nFinding the P-value of a Z-Value\nUsing a Z-table or programming we can calculate how many people Germany are shorter than Bob and how many are taller.\nExampleGet your own Python Server\nWith Python use the Scipy Stats library norm.cdf() function find the probability of getting less than a Z-value of 3:\nExample\nWith R use the built-in pnorm() function find the probability of getting less than a Z-value of 3:\nUsing either method we can find that the probability is\n≈\n0.9987\n, or\n99.87\n%\nWhich means that Bob is taller than 99.87% of the people in Germany.\nHere is a graph of the standard normal distribution and a Z-value of 3 to visualize the probability:\nThese methods find the p-value up to the particular z-value we have.\nTo find the p-value above the z-value we can calculate 1 minus the probability.\nSo in Bob's example, we can calculate 1 - 0.9987 = 0.0013, or 0.13%.\nWhich means that only 0.13% of Germans are taller than Bob.\nFinding the P-Value Between Z-Values\nIf we instead want to know how many people are between 155 cm and 165 cm in Germany using the same example:\nThe mean height of people in Germany is 170 cm (\nμ\n)\nThe standard deviation of the height of people in Germany is 10 cm (\nσ\n)\nNow we need to calculate Z-values for both 155 cm and 165 cm:\nZ\n=\nx\n−\nμ\nσ\n=\n155\n−\n170\n10\n=\n−\n15\n10\n=\n−\n1.5\n―\nThe Z-value of 155 cm is -1.5\nZ\n=\nx\n−\nμ\nσ\n=\n165\n−\n170\n10\n=\n−\n5\n10\n=\n−\n0.5\n―\nThe Z-value of 165 cm is -0.5\nUsing the Z-table or programming we can find that the p-value for the two z-values:\nThe probability of a z-value smaller than -0.5 (shorter than 165 cm) is 30.85%\nThe probability of a z-value smaller than -1.5 (shorter than 155 cm) is 6.68%\nSubtract 6.68% from 30.85% to find the probability of getting a z-value between them.\n30.85% - 6.68% =\n24.17%\nHere is a set of graphs illustrating the process:\nFinding the Z-value of a P-Value\nYou can also use p-values (probability) to find z-values.\nFor example:\n\"How tall are you if you are taller than 90% of Germans?\"\nThe p-value is 0.9, or 90%.\nUsing a Z-table or programming we can calculate the z-value:\nExample\nWith Python use the Scipy Stats library norm.ppf() function find the z-value separating the top 10% from the bottom 90%:\nExample\nWith R use the built-in qnorm() function find the z-value separating the top 10% from the bottom 90%:\nUsing either method we can find that the Z-value is\n≈\n1.281\nMeaning that a person that is 1.281 standard deviations taller than the mean height of Germans is taller than 90% of Germans.\nWe then use the formula to calculate the height (\nx\n) based on a mean (\nμ\n) of 170 cm and standard deviation (\nσ\n) of 10 cm:\nZ\n=\nx\n−\nμ\nσ\n1.281\n=\nx\n−\n170\n10\n1.281\n⋅\n10\n=\nx\n−\n170\n12.81\n=\nx\n−\n170\n12.81\n+\n170\n=\nx\n182.81\n―\n=\nx\nSo we can conclude that:\n\"You have to be at least 182.81 cm tall to be taller than 90% of Germans\"",
      "examples": [
        "The mean height of people in Germany is 170 cm (\nμ\n)\nThe standard deviation of the height of people in Germany is 10 cm (\nσ\n)\nBob is 200 cm tall (\nx\n)",
        "import scipy.stats as stats\nprint(stats.norm.cdf(3))",
        "pnorm(3)",
        "The mean height of people in Germany is 170 cm (\nμ\n)\nThe standard deviation of the height of people in Germany is 10 cm (\nσ\n)",
        "\"How tall are you if you are taller than 90% of Germans?\"",
        "import scipy.stats as stats\nprint(stats.norm.ppf(0.9))",
        "qnorm(0.9)",
        "\"You have to be at least 182.81 cm tall to be taller than 90% of Germans\"",
        "norm.cdf()",
        "pnorm()",
        "norm.ppf()",
        "qnorm()"
      ]
    },
    {
      "title": "Statistics - Student's T Distribution",
      "summary": "The student's t-distribution is similar to a normal distribution and used in statistical inference to adjust for uncertainty.\nStudent's T Distribution\nThe t-distribution is used for estimation and hypothesis testing of a population mean (average).\nThe t-distribution is adjusted for the extra uncertainty of estimating the mean.\nIf the sample is small, the t-distribution is wider. If the sample is big, the t-distribution is narrower.\nThe bigger the sample size is, the closer the t-distribution gets to the standard normal distribution.\nBelow is a graph of a few different t-distributions.\nNotice how some of the curves have bigger tails.\nThis is due to the uncertainty from a smaller sample size.\nThe green curve has the smallest sample size.\nFor the t-distribution this is expressed as 'degrees of freedom' (df), which is calculated by subtracting 1 from the sample size (n).\nFor example a sample size of 30 will make 29 degrees of freedom for the t-distribution.\nThe t-distribution is used to find critical t-values and p-values (probabilities) for estimation and hypothesis testing.\nNote: Finding the critical t-values and p-values of the t-distribution is similar z-values and p-values of the standard normal distribution. But make sure to use the correct degrees of freedom.\nREMOVE ADS\nFinding the P-Value of a T-Value\nYou can find the p-values of a t-value by using a t-table or with programming.\nExampleGet your own Python Server\nWith Python use the Scipy Stats library t.cdf() function find the probability of getting less than a t-value of 2.1 with 29 degrees of freedom:\nExample\nWith R use the built-in pt() function find the probability of getting less than a t-value of 2.1 with 29 degrees of freedom:\nFinding the T-value of a P-Value\nYou can find the t-values of a p-value by using a t-table or with programming.\nExample\nWith Python use the Scipy Stats library t.ppf() function find the t-value separating the top 25% from the bottom 75% with 29 degrees of freedom:\nExample\nWith R use the built-in qt() function find the t-value separating the top 25% from the bottom 75% with 29 degrees of freedom (df):",
      "examples": [
        "import scipy.stats as stats\nprint(stats.t.cdf(2.1, 29))",
        "pt(2.1, 29)",
        "import scipy.stats as stats\nprint(stats.t.ppf(0.75, 29))",
        "qt(0.75, 29)",
        "t.cdf()",
        "pt()",
        "t.ppf()",
        "qt()"
      ]
    },
    {
      "title": "Statistics - Estimation",
      "summary": "Point estimates are the most likely value for a population parameter.\nConfidence intervals express the uncertainty of an estimated population parameter.\nThe Point Estimate\nA point estimate is calculated from a sample.\nThe point estimate depends on the type of data:\nCategorical data: the number of occurrences divided by the sample size.\nNumerical data: the mean (the average) of the sample.\nOne example could be:\nThe point estimate for the average height of people in Denmark is 180 cm.\nEstimates are always uncertain. This uncertainty can be expressed with a confidence interval.\nConfidence Intervals\nThe confidence interval is defined by a lower bound and an upper bound.\nThis gives us a range of values that the true parameter is likely to be between.\nFor example that:\nThe average height of people in Denmark is between 170 cm and 190 cm.\nHere, 170 cm is the lower bound, and 190 cm is the upper bound.\nThe lower and upper bounds of a confidence interval is based on the confidence level.\nThe Confidence Level\nConfidence levels can be expressed as percentages or decimal numbers, and the most commonly used are:\n90% (0.90)\n95% (0.95)\n99% (0.99)\nThe higher the confidence level, the bigger the interval will be.\nFor example, the confidence intervals for the average height of people in Denmark might be:\n90% confidence level: between 175 cm and 185 cm.\n95% confidence level: between 170 cm and 190 cm.\n99% confidence level: between 160 cm and 200 cm.\nWe use this confidence level together with a probability distribution to decide how large the margin of error is.\nREMOVE ADS\nThe Margin of Error\nThe margin of error is the distance between the point estimate and the lower and upper bounds.\nThe margin of error is based on the confidence level and the data we have from the sample.\nFor example, if the point estimate for the average height of people in Denmark is 180 cm:\n5 cm margin of error: between 175 cm and 185 cm.\n10 cm margin of error: between 170 cm and 190 cm.\n20 cm margin of error: between 160 cm and 200 cm.\nSteps for Calculating the Confidence Interval\nThe following steps are used to calculate a confidence interval:\nCheck the conditions\nFind the point estimate\nDecide the confidence level\nCalculate the margin of error\nCalculate the confidence interval\nOne condition is that the sample is randomly selected from the population.\nThe other conditions depends on what type of parameter you are calculate the confidence interval for.\nCommonly estimated parameters are:\nProportions (for qualitative data)\nMean values (for numerical data)\nYou will learn the steps for both types in the following pages.",
      "examples": [
        "The point estimate for the average height of people in Denmark is 180 cm.",
        "The average height of people in Denmark is between 170 cm and 190 cm.",
        "90% confidence level: between 175 cm and 185 cm.\n95% confidence level: between 170 cm and 190 cm.\n99% confidence level: between 160 cm and 200 cm.",
        "5 cm margin of error: between 175 cm and 185 cm.\n10 cm margin of error: between 170 cm and 190 cm.\n20 cm margin of error: between 160 cm and 200 cm."
      ]
    },
    {
      "title": "Statistics - Estimating Population Proportions",
      "summary": "A population proportion is the share of a population that belongs to a particular category.\nConfidence intervals are used to estimate population proportions.\nEstimating Population Proportions\nA statistic from a sample is used to estimate a parameter of the population.\nThe most likely value for a parameter is the point estimate.\nAdditionally, we can calculate a lower bound and an upper bound for the estimated parameter.\nThe margin of error is the difference between the lower and upper bounds from the point estimate.\nTogether, the lower and upper bounds define a confidence interval.\nCalculating a Confidence Interval\nThe following steps are used to calculate a confidence interval:\nCheck the conditions\nFind the point estimate\nDecide the confidence level\nCalculate the margin of error\nCalculate the confidence interval\nFor example:\nPopulation: Nobel Prize winners\nCategory: Born in the United States of America\nWe can take a sample and see how many of them were born in the US.\nThe sample data is used to make an estimation of the share of all the Nobel Prize winners born in the US.\nBy randomly selecting 30 Nobel Prize winners we could find that:\n6 out of 30 Nobel Prize winners in the sample were born in the US\nFrom this data we can calculate a confidence interval with the steps below.\n1. Checking the Conditions\nThe conditions for calculating a confidence interval for a proportion are:\nThe sample is randomly selected\nThere is only two options:\nBeing in the category\nNot being in the category\nBeing in the category\nNot being in the category\nThe sample needs at least:\n5 members in the category\n5 members not in the category\n5 members in the category\n5 members not in the category\nIn our example, we randomly selected 6 people that were born in the US.\nThe rest were not born in the US, so there are 24 in the other category.\nThe conditions are fulfilled in this case.\nNote: It is possible to calculate a confidence interval without having 5 of each category. But special adjustments need to be made.\nREMOVE ADS\n2. Finding the Point Estimate\nThe point estimate is the sample proportion (\np\n^\n).\nThe formula for calculating the sample proportion is the number of occurrences (\nx\n) divided by the sample size (\nn\n):\np\n^\n=\nx\nn\nIn our example, 6 out of 30 were born in the US:\nx\nis 6, and\nn\nis 30.\nSo the point estimate for the proportion is:\np\n^\n=\nx\nn\n=\n6\n30\n=\n0.2\n―\n=\n20\n%\nSo 20% of the sample were born in the US.\n3. Deciding the Confidence Level\nThe confidence level is expressed with a percentage or a decimal number.\nFor example, if the confidence level is 95% or 0.95:\nThe remaining probability (\nα\n) is then: 5%, or 1 - 0.95 = 0.05.\nCommonly used confidence levels are:\n90% with\nα\n= 0.1\n95% with\nα\n= 0.05\n99% with\nα\n= 0.01\nNote: A 95% confidence level means that if we take 100 different samples and make confidence intervals for each:\nThe true parameter will be inside the confidence interval 95 out of those 100 times.\nWe use the standard normal distribution to find the margin of error for the confidence interval.\nThe remaining probabilities (\nα\n) are divided in two so that half is in each tail area of the distribution.\nThe values on the z-value axis that separate the tails area from the middle are called critical z-values.\nBelow are graphs of the standard normal distribution showing the tail areas (\nα\n) for different confidence levels.\n4. Calculating the Margin of Error\nThe margin of error is the difference between the point estimate and the lower and upper bounds.\nThe margin of error (\nE\n) for a proportion is calculated with a critical z-value and the standard error:\nE\n=\nZ\nα\n/\n2\n⋅\np\n^\n(\n1\n−\np\n^\n)\nn\nThe critical z-value\nZ\nα\n/\n2\nis calculated from the standard normal distribution and the confidence level.\nThe standard error\np\n^\n(\n1\n−\np\n^\n)\nn\nis calculated from the point estimate (\np\n^\n) and sample size (\nn\n).\nIn our example with 6 US-born Nobel Prize winners out of a sample of 30 the standard error is:\np\n^\n(\n1\n−\np\n^\n)\nn\n=\n0.2\n(\n1\n−\n0.2\n)\n30\n=\n0.2\n⋅\n0.8\n30\n=\n0.16\n30\n=\n0.00533\n.\n.\n≈\n0.073\n―\nIf we choose 95% as the confidence level, the\nα\nis 0.05.\nSo we need to find the critical z-value\nZ\n0.05\n/\n2\n=\nZ\n0.025\nThe critical z-value can be found using a Z-table or with a programming language function:\nExampleGet your own Python Server\nWith Python use the Scipy Stats library norm.ppf() function find the Z-value for an\nα\n/2 = 0.025\nExample\nWith R use the built-in qnorm() function to find the Z-value for an\nα\n/2 = 0.025\nUsing either method we can find that the critical Z-value\nZ\nα\n/\n2\nis\n≈\n1.96\n―\nThe standard error\np\n^\n(\n1\n−\np\n^\n)\nn\nwas\n≈\n0.073\n―\nSo the margin of error (\nE\n) is:\nE\n=\nZ\nα\n/\n2\n⋅\np\n^\n(\n1\n−\np\n^\n)\nn\n≈\n1.96\n⋅\n0.073\n=\n0.143\n―\n5. Calculate the Confidence Interval\nThe lower and upper bounds of the confidence interval are found by subtracting and adding the margin of error (\nE\n) from the point estimate (\np\n^\n).\nIn our example the point estimate was 0.2 and the margin of error was 0.143, then:\nThe lower bound is:\np\n^\n−\nE\n=\n0.2\n−\n0.143\n=\n0.057\n―\nThe upper bound is:\np\n^\n+\nE\n=\n0.2\n+\n0.143\n=\n0.343\n―\nThe confidence interval is:\n[\n0.057\n,\n0.343\n]\nor\n[\n5.7\n%\n,\n34.4\n%\n]\nAnd we can summarize the confidence interval by stating:\nThe 95% confidence interval for the proportion of Nobel Prize winners born in the US is between 5.7% and 34.4%\nCalculating a Confidence Interval with Programming\nA confidence interval can be calculated with many programming languages.\nUsing software and programming to calculate statistics is more common for bigger sets of data, as calculating manually becomes difficult.\nExample\nWith Python, use the scipy and math libraries to calculate the confidence interval for an estimated proportion.\nHere, the sample size is 30 and the occurrences is 6.\nExample\nWith R, use the built-in math and statistics functions to calculate the confidence interval for an estimated proportion.\nHere, the sample size is 30 and the occurrences is 6.",
      "examples": [
        "6 out of 30 Nobel Prize winners in the sample were born in the US",
        "import scipy.stats as stats\nprint(stats.norm.ppf(1-0.025))",
        "qnorm(1-0.025)",
        "The 95% confidence interval for the proportion of Nobel Prize winners born in the US is between 5.7% and 34.4%",
        "import scipy.stats as stats\nimport math\n\n# Specify sample occurrences (x), sample size (n) and confidence level\nx = 6\nn = 30\nconfidence_level = 0.95\n\n# Calculate the point estimate, alpha, the critical z-value, the standard error, and the margin of error\npoint_estimate = x/n\nalpha = (1-confidence_level)\ncritical_z = stats.norm.ppf(1-alpha/2)\nstandard_error = math.sqrt((point_estimate*(1-point_estimate)/n))\nmargin_of_error = critical_z * standard_error\n\n# Calculate the lower and upper bound of the confidence interval\nlower_bound = point_estimate - margin_of_error\nupper_bound = point_estimate + margin_of_error\n\n# Print the results\nprint(\"Point Estimate: {:.3f}\".format(point_estimate))\nprint(\"Critical Z-value: {:.3f}\".format(critical_z))\nprint(\"Margin of Error: {:.3f}\".format(margin_of_error))\nprint(\"Confidence Interval: [{:.3f},{:.3f}]\".format(lower_bound,upper_bound))\nprint(\"The {:.1%} confidence interval for the population proportion is:\".format(confidence_level))\nprint(\"between {:.3f} and {:.3f}\".format(lower_bound,upper_bound))",
        "# Specify sample occurrences (x), sample size (n) and confidence level\nx = 6\nn = 30\nconfidence_level = 0.95\n\n# Calculate the point estimate, alpha, the critical z-value, the standard error, and the margin of error\npoint_estimate = x/n\nalpha = (1-confidence_level)\ncritical_z = qnorm(1-alpha/2)\nstandard_error = sqrt(point_estimate*(1-point_estimate)/n)\nmargin_of_error = critical_z * standard_error\n\n# Calculate the lower and upper bound of the confidence interval\nlower_bound = point_estimate - margin_of_error\nupper_bound = point_estimate + margin_of_error\n\n# Print the results\nsprintf(\"Point Estimate: %0.3f\", point_estimate)\nsprintf(\"Critical Z-value: %0.3f\", critical_z)\nsprintf(\"Margin of Error: %0.3f\", margin_of_error)\nsprintf(\"Confidence Interval: [%0.3f,%0.3f]\", lower_bound, upper_bound)\nsprintf(\"The %0.1f%% confidence interval for the population proportion is:\", confidence_level*100)\nsprintf(\"between %0.4f and %0.4f\", lower_bound, upper_bound)",
        "norm.ppf()",
        "qnorm()"
      ]
    },
    {
      "title": "Statistics - Estimating Population Means",
      "summary": "A population mean is an average of a numerical population variable.\nConfidence intervals are used to estimate population means.\nEstimating Population Mean\nA statistic from a sample is used to estimate a parameter of the population.\nThe most likely value for a parameter is the point estimate.\nAdditionally, we can calculate a lower bound and an upper bound for the estimated parameter.\nThe margin of error is the difference between the lower and upper bounds from the point estimate.\nTogether, the lower and upper bounds define a confidence interval.\nCalculating a Confidence Interval\nThe following steps are used to calculate a confidence interval:\nCheck the conditions\nFind the point estimate\nDecide the confidence level\nCalculate the margin of error\nCalculate the confidence interval\nFor example:\nPopulation: Nobel Prize winners\nVariable: Age when they received the Nobel Prize\nWe can take a sample and calculate the mean and the standard deviation of that sample.\nThe sample data is used to make an estimation of the average age of all the Nobel Prize winners.\nBy randomly selecting 30 Nobel Prize winners we could find that:\nThe mean age in the sample is 62.1\nThe standard deviation of age in the sample is 13.46\nFrom this data we can calculate a confidence interval with the steps below.\n1. Checking the Conditions\nThe conditions for calculating a confidence interval for a mean are:\nThe sample is randomly selected\nAnd either:\nThe population data is normally distributed\nSample size is large enough\nThe population data is normally distributed\nSample size is large enough\nA moderately large sample size, like 30, is typically large enough.\nIn the example, the sample size was 30 and it was randomly selected, so the conditions are fulfilled.\nNote: Checking if the data is normally distributed can be done with specialized statistical tests.\nREMOVE ADS\n2. Finding the Point Estimate\nThe point estimate is the sample mean (\nx\n¯\n).\nThe formula for calculating the sample mean is the sum of all the values\n∑\nx\ni\ndivided by the sample size (\nn\n):\nx\n¯\n=\n∑\nx\ni\nn\nIn our example, the mean age was 62.1 in the sample.\n3. Deciding the Confidence Level\nThe confidence level is expressed with a percentage or a decimal number.\nFor example, if the confidence level is 95% or 0.95:\nThe remaining probability (\nα\n) is then: 5%, or 1 - 0.95 = 0.05.\nCommonly used confidence levels are:\n90% with\nα\n= 0.1\n95% with\nα\n= 0.05\n99% with\nα\n= 0.01\nNote: A 95% confidence level means that if we take 100 different samples and make confidence intervals for each:\nThe true parameter will be inside the confidence interval 95 out of those 100 times.\nWe use the student's t-distribution to find the margin of error for the confidence interval.\nThe t-distribution is adjusted for the sample size with 'degrees of freedom' (df).\nThe degrees of freedom is the sample size (n) - 1, so in this example it is 30 - 1 = 29\nThe remaining probabilities (\nα\n) are divided in two so that half is in each tail area of the distribution.\nThe values on the t-value axis that separate the tails area from the middle are called critical t-values.\nBelow are graphs of the standard normal distribution showing the tail areas (\nα\n) for different confidence levels at 29 degrees of freedom (df).\n4. Calculating the Margin of Error\nThe margin of error is the difference between the point estimate and the lower and upper bounds.\nThe margin of error (\nE\n) for a proportion is calculated with a critical t-value and the standard error:\nE\n=\nt\nα\n/\n2\n(\nd\nf\n)\n⋅\ns\nn\nThe critical t-value\nt\nα\n/\n2\n(\nd\nf\n)\nis calculated from the standard normal distribution and the confidence level.\nThe standard error\ns\nn\nis calculated from the sample standard deviation (\ns\n) and the sample size (\nn\n).\nIn our example with a sample standard deviation (\ns\n) of 13.46 and sample size of 30 the standard error is:\ns\nn\n=\n13.46\n30\n≈\n13.46\n5.477\n=\n2.458\n―\nIf we choose 95% as the confidence level, the\nα\nis 0.05.\nSo we need to find the critical t-value\nt\n0.05\n/\n2\n(\n29\n)\n=\nt\n0.025\n(\n29\n)\nThe critical t-value can be found using a t-table or with a programming language function:\nExampleGet your own Python Server\nWith Python use the Scipy Stats library t.ppf() function find the t-value for an\nα\n/2 = 0.025 and 29 degrees of freedom.\nExample\nWith R use the built-in qt() function to find the t-value for an\nα\n/2 = 0.025 and 29 degrees of freedom.\nUsing either method we can find that the critical t-value\nt\nα\n/\n2\n(\nd\nf\n)\nis\n≈\n2.05\n―\nThe standard error\ns\nn\nwas\n≈\n2.458\n―\nSo the margin of error (\nE\n) is:\nE\n=\nt\nα\n/\n2\n(\nd\nf\n)\n⋅\ns\nn\n≈\n2.05\n⋅\n2.458\n=\n5.0389\n―\n5. Calculate the Confidence Interval\nThe lower and upper bounds of the confidence interval are found by subtracting and adding the margin of error (\nE\n) from the point estimate (\nx\n¯\n).\nIn our example the point estimate was 0.2 and the margin of error was 0.143, then:\nThe lower bound is:\nx\n¯\n−\nE\n=\n62.1\n−\n5.0389\n≈\n57.06\n―\nThe upper bound is:\nx\n¯\n+\nE\n=\n62.1\n+\n5.0389\n≈\n67.14\n―\nThe confidence interval is:\n[\n57.06\n,\n67.14\n]\nAnd we can summarize the confidence interval by stating:\nThe 95% confidence interval for the mean age of Nobel Prize winners is between 57.06 and 67.14 years\nCalculating a Confidence Interval with Programming\nA confidence interval can be calculated with many programming languages.\nUsing software and programming to calculate statistics is more common for bigger sets of data, as calculating manually becomes difficult.\nNote: The results from using the programming code will be more accurate because of rounding of values when calculating by hand.\nExample\nWith Python use the scipy and math libraries to calculate the confidence interval for an estimated proportion.\nHere, the sample size is 30, sample mean is 62.1 and sample standard deviation is 13.46.\nExample\nR can use built-in math and statistics functions to calculate the confidence interval for an estimated proportion.\nHere, the sample size is 30, sample mean is 62.1 and sample standard deviation is 13.46.\nNote: R also has a built in function for calculating a confidence interval for a population mean.\nExample\nR can use the built-in t.test() function to calculate the confidence interval for an estimated mean.\nHere, the sample is 30 randomly generated values with a mean of 60 and standard deviation is 12.5 using the rnorm() function to generate the sample.",
      "examples": [
        "The mean age in the sample is 62.1\nThe standard deviation of age in the sample is 13.46",
        "import scipy.stats as stats\nprint(stats.t.ppf(1-0.025, 29))",
        "qt(1-0.025, 29)",
        "The 95% confidence interval for the mean age of Nobel Prize winners is between 57.06 and 67.14 years",
        "import scipy.stats as stats\nimport math\n\n# Specify sample mean (x_bar), sample standard deviation (s), sample size (n) and confidence level\nx_bar = 62.1\ns = 13.46\nn = 30\nconfidence_level = 0.95\n\n# Calculate alpha, degrees of freedom (df), the critical t-value, and the margin of error\nalpha = (1-confidence_level)\ndf = n - 1\nstandard_error = s/math.sqrt(n)\ncritical_t = stats.t.ppf(1-alpha/2, df)\nmargin_of_error = critical_t * standard_error\n\n# Calculate the lower and upper bound of the confidence interval\nlower_bound = x_bar - margin_of_error\nupper_bound = x_bar + margin_of_error\n\n# Print the results\nprint(\"Critical t-value: {:.3f}\".format(critical_t))\nprint(\"Margin of Error: {:.3f}\".format(margin_of_error))\nprint(\"Confidence Interval: [{:.3f},{:.3f}]\".format(lower_bound,upper_bound))\nprint(\"The {:.1%} confidence interval for the population mean is:\".format(confidence_level))\nprint(\"between {:.3f} and {:.3f}\".format(lower_bound,upper_bound))",
        "# Specify sample mean (x_bar), sample standard deviation (s), sample size (n) and confidence level\nx_bar = 62.1\ns = 13.46\nn = 30\nconfidence_level = 0.95\n\n# Calculate alpha, degrees of freedom (df), the critical t-value, and the margin of error\nalpha = (1-confidence_level)\ndf = n - 1\nstandard_error = s/sqrt(n)\ncritical_t = qt(1-alpha/2, 29)\nmargin_of_error = critical_t * standard_error\n\n# Calculate the lower and upper bound of the confidence interval\nlower_bound = x_bar - margin_of_error\nupper_bound = x_bar + margin_of_error\n\n# Print the results\nsprintf(\"Critical t-value: %0.3f\", critical_t)\nsprintf(\"Margin of Error: %0.3f\", margin_of_error)\nsprintf(\"Confidence Interval: [%0.3f,%0.3f]\", lower_bound, upper_bound)\nsprintf(\"The %0.1f%% confidence interval for the population mean is:\", confidence_level*100)\nsprintf(\"between %0.4f and %0.4f\", lower_bound, upper_bound)",
        "# Specify sample size (n) and confidence level\nn = 30\nconfidence_level = 0.95\n\n# Set random seed and generate sample data with mean of 60 and standard deviation of 12.5\nset.seed(3)\nsample <- rnorm(n, 60, 12.5)\n\n# t.test function for sample data, confidence level, and selecting the $conf.int option\nt.test(sample, conf.level = confidence_level)$conf.int",
        "t.ppf()",
        "qt()",
        "t.test()",
        "rnorm()"
      ]
    },
    {
      "title": "Statistics - Hypothesis Testing",
      "summary": "Hypothesis testing is a formal way of checking if a hypothesis about a population is true or not.\nHypothesis Testing\nA hypothesis is a claim about a population parameter.\nA hypothesis test is a formal procedure to check if a hypothesis is true or not.\nExamples of claims that can be checked:\nThe average height of people in Denmark is more than 170 cm.\nThe share of left handed people in Australia is not 10%.\nThe average income of dentists is less the average income of lawyers.\nThe Null and Alternative Hypothesis\nHypothesis testing is based on making two different claims about a population parameter.\nThe null hypothesis (\nH\n0\n) and the alternative hypothesis (\nH\n1\n) are the claims.\nThe two claims needs to be mutually exclusive, meaning only one of them can be true.\nThe alternative hypothesis is typically what we are trying to prove.\nFor example, we want to check the following claim:\n\"The average height of people in Denmark is more than 170 cm.\"\nIn this case, the parameter is the average height of people in Denmark (\nμ\n).\nThe null and alternative hypothesis would be:\nNull hypothesis: The average height of people in Denmark is 170 cm.\nAlternative hypothesis: The average height of people in Denmark is more than 170 cm.\nThe claims are often expressed with symbols like this:\nH\n0\n:\nμ\n=\n170\nc\nm\nH\n1\n:\nμ\n>\n170\nc\nm\nIf the data supports the alternative hypothesis, we reject the null hypothesis and accept the alternative hypothesis.\nIf the data does not support the alternative hypothesis, we keep the null hypothesis.\nNote: The alternative hypothesis is also referred to as (\nH\nA\n).\nThe Significance Level\nThe significance level (\nα\n) is the uncertainty we accept when rejecting the null hypothesis in the hypothesis test.\nThe significance level is a percentage probability of accidentally making the wrong conclusion.\nTypical significance levels are:\nα\n=\n0.1\n(10%)\nα\n=\n0.05\n(5%)\nα\n=\n0.01\n(1%)\nA lower significance level means that the evidence in the data needs to be stronger to reject the null hypothesis.\nThere is no \"correct\" significance level - it only states the uncertainty of the conclusion.\nNote: A 5% significance level means that when we reject a null hypothesis:\nWe expect to reject a true null hypothesis 5 out of 100 times.\nREMOVE ADS\nThe Test Statistic\nThe test statistic is used to decide the outcome of the hypothesis test.\nThe test statistic is a standardized value calculated from the sample.\nStandardization means converting a statistic to a well known probability distribution.\nThe type of probability distribution depends on the type of test.\nCommon examples are:\nStandard Normal Distribution (Z): used for Testing Population Proportions\nStudent's T-Distribution (T): used for Testing Population Means\nNote: You will learn how to calculate the test statistic for each type of test in the following chapters.\nThe Critical Value and P-Value Approach\nThere are two main approaches used for hypothesis tests:\nThe critical value approach compares the test statistic with the critical value of the significance level.\nThe p-value approach compares the p-value of the test statistic and with the significance level.\nThe Critical Value Approach\nThe critical value approach checks if the test statistic is in the rejection region.\nThe rejection region is an area of probability in the tails of the distribution.\nThe size of the rejection region is decided by the significance level (\nα\n).\nThe value that separates the rejection region from the rest is called the critical value.\nHere is a graphical illustration:\nIf the test statistic is inside this rejection region, the null hypothesis is rejected.\nFor example, if the test statistic is 2.3 and the critical value is 2 for a significance level (\nα\n=\n0.05\n):\nWe reject the null hypothesis (\nH\n0\n) at 0.05 significance level (\nα\n)\nThe P-Value Approach\nThe p-value approach checks if the p-value of the test statistic is smaller than the significance level (\nα\n).\nThe p-value of the test statistic is the area of probability in the tails of the distribution from the value of the test statistic.\nHere is a graphical illustration:\nIf the p-value is smaller than the significance level, the null hypothesis is rejected.\nThe p-value directly tells us the lowest significance level where we can reject the null hypothesis.\nFor example, if the p-value is 0.03:\nWe reject the null hypothesis (\nH\n0\n) at a 0.05 significance level (\nα\n)\nWe keep the null hypothesis (\nH\n0\n) at a 0.01 significance level (\nα\n)\nNote: The two approaches are only different in how they present the conclusion.\nSteps for a Hypothesis Test\nThe following steps are used for a hypothesis test:\nCheck the conditions\nDefine the claims\nDecide the significance level\nCalculate the test statistic\nConclusion\nOne condition is that the sample is randomly selected from the population.\nThe other conditions depends on what type of parameter you are testing the hypothesis for.\nCommon parameters to test hypotheses are:\nProportions (for qualitative data)\nMean values (for numerical data)\nYou will learn the steps for both types in the following pages.",
      "examples": [
        "The average height of people in Denmark is more than 170 cm.\nThe share of left handed people in Australia is not 10%.\nThe average income of dentists is less the average income of lawyers.",
        "\"The average height of people in Denmark is more than 170 cm.\"",
        "Null hypothesis: The average height of people in Denmark is 170 cm.\nAlternative hypothesis: The average height of people in Denmark is more than 170 cm.",
        "H\n0\n:\nμ\n=\n170\nc\nm\nH\n1\n:\nμ\n>\n170\nc\nm",
        "We reject the null hypothesis (\nH\n0\n) at 0.05 significance level (\nα\n)",
        "We reject the null hypothesis (\nH\n0\n) at a 0.05 significance level (\nα\n)\nWe keep the null hypothesis (\nH\n0\n) at a 0.01 significance level (\nα\n)"
      ]
    },
    {
      "title": "Statistics - Hypothesis Testing a Proportion",
      "summary": "A population proportion is the share of a population that belongs to a particular category.\nHypothesis tests are used to check a claim about the size of that population proportion.\nHypothesis Testing a Proportion\nThe following steps are used for a hypothesis test:\nCheck the conditions\nDefine the claims\nDecide the significance level\nCalculate the test statistic\nConclusion\nFor example:\nPopulation: Nobel Prize winners\nCategory: Born in the United States of America\nAnd we want to check the claim:\n\"More than 20% of Nobel Prize winners were born in the US\"\nBy taking a sample of 40 randomly selected Nobel Prize winners we could find that:\n10 out of 40 Nobel Prize winners in the sample were born in the US\nThe sample proportion is then:\n10\n40\n=\n0.25\n, or 25%.\nFrom this sample data we check the claim with the steps below.\n1. Checking the Conditions\nThe conditions for calculating a confidence interval for a proportion are:\nThe sample is randomly selected\nThere is only two options:\nBeing in the category\nNot being in the category\nBeing in the category\nNot being in the category\nThe sample needs at least:\n5 members in the category\n5 members not in the category\n5 members in the category\n5 members not in the category\nIn our example, we randomly selected 10 people that were born in the US.\nThe rest were not born in the US, so there are 30 in the other category.\nThe conditions are fulfilled in this case.\nNote: It is possible to do a hypothesis test without having 5 of each category. But special adjustments need to be made.\n2. Defining the Claims\nWe need to define a null hypothesis (\nH\n0\n) and an alternative hypothesis (\nH\n1\n) based on the claim we are checking.\nThe claim was:\n\"More than 20% of Nobel Prize winners were born in the US\"\nIn this case, the parameter is the proportion of Nobel Prize winners born in the US (\np\n).\nThe null and alternative hypothesis are then:\nNull hypothesis: 20% of Nobel Prize winners were born in the US.\nAlternative hypothesis: More than 20% of Nobel Prize winners were born in the US.\nWhich can be expressed with symbols as:\nH\n0\n:\np\n=\n0.20\nH\n1\n:\np\n>\n0.20\nThis is a 'right tailed' test, because the alternative hypothesis claims that the proportion is more than in the null hypothesis.\nIf the data supports the alternative hypothesis, we reject the null hypothesis and accept the alternative hypothesis.\nREMOVE ADS\n3. Deciding the Significance Level\nThe significance level (\nα\n) is the uncertainty we accept when rejecting the null hypothesis in a hypothesis test.\nThe significance level is a percentage probability of accidentally making the wrong conclusion.\nTypical significance levels are:\nα\n=\n0.1\n(10%)\nα\n=\n0.05\n(5%)\nα\n=\n0.01\n(1%)\nA lower significance level means that the evidence in the data needs to be stronger to reject the null hypothesis.\nThere is no \"correct\" significance level - it only states the uncertainty of the conclusion.\nNote: A 5% significance level means that when we reject a null hypothesis:\nWe expect to reject a true null hypothesis 5 out of 100 times.\n4. Calculating the Test Statistic\nThe test statistic is used to decide the outcome of the hypothesis test.\nThe test statistic is a standardized value calculated from the sample.\nThe formula for the test statistic (TS) of a population proportion is:\np\n^\n−\np\np\n(\n1\n−\np\n)\n⋅\nn\np\n^\n−\np\nis the difference between the sample proportion (\np\n^\n) and the claimed population proportion (\np\n).\nn\nis the sample size.\nIn our example:\nThe claimed (\nH\n0\n) population proportion (\np\n) was\n0.20\nThe sample proportion (\np\n^\n) was 10 out of 40, or:\n10\n40\n=\n0.25\nThe sample size (\nn\n) was\n40\nSo the test statistic (TS) is then:\n0.25\n−\n0.20\n0.2\n(\n1\n−\n0.2\n)\n⋅\n40\n=\n0.05\n0.2\n(\n0.8\n)\n⋅\n40\n=\n0.05\n0.16\n⋅\n40\n≈\n0.05\n0.4\n⋅\n6.325\n=\n0.791\n―\nYou can also calculate the test statistic using programming language functions:\nExampleGet your own Python Server\nWith Python use the scipy and math libraries to calculate the test statistic for a proportion.\nExample\nWith R use the built-in prop.test() function to calculate the test statistic for a proportion.\n5. Concluding\nThere are two main approaches for making the conclusion of a hypothesis test:\nThe critical value approach compares the test statistic with the critical value of the significance level.\nThe P-value approach compares the P-value of the test statistic and with the significance level.\nNote: The two approaches are only different in how they present the conclusion.\nThe Critical Value Approach\nFor the critical value approach we need to find the critical value (CV) of the significance level (\nα\n).\nFor a population proportion test, the critical value (CV) is a Z-value from a standard normal distribution.\nThis critical Z-value (CV) defines the rejection region for the test.\nThe rejection region is an area of probability in the tails of the standard normal distribution.\nBecause the claim is that the population proportion is more than 20%, the rejection region is in the right tail:\nThe size of the rejection region is decided by the significance level (\nα\n).\nChoosing a significance level (\nα\n) of 0.05, or 5%, we can find the critical Z-value from a Z-table, or with a programming language function:\nNote: The functions find the Z-value for an area from the left side.\nTo find the Z-value for a right tail we need to use the function on the area to the left of the tail (1-0.05 = 0.95).\nExample\nWith Python use the Scipy Stats library norm.ppf() function find the Z-value for an\nα\n= 0.05 in the right tail.\nExample\nWith R use the built-in qnorm() function to find the Z-value for an\nα\n= 0.05 in the right tail.\nUsing either method we can find that the critical Z-value is\n≈\n1.6449\n―\nFor a right tailed test we need to check if the test statistic (TS) is bigger than the critical value (CV).\nIf the test statistic is bigger than the critical value, the test statistic is in the rejection region.\nWhen the test statistic is in the rejection region, we reject the null hypothesis (\nH\n0\n).\nHere, the test statistic (TS) was\n≈\n0.791\n―\nand the critical value was\n≈\n1.6449\n―\nHere is an illustration of this test in a graph:\nSince the test statistic was smaller than the critical value we do not reject the null hypothesis.\nThis means that the sample data does not support the alternative hypothesis.\nAnd we can summarize the conclusion stating:\nThe sample data does not support the claim that \"more than 20% of Nobel Prize winners were born in the US\" at a 5% significance level.\nThe P-Value Approach\nFor the P-value approach we need to find the P-value of the test statistic (TS).\nIf the P-value is smaller than the significance level (\nα\n), we reject the null hypothesis (\nH\n0\n).\nThe test statistic was found to be\n≈\n0.791\n―\nFor a population proportion test, the test statistic is a Z-Value from a standard normal distribution.\nBecause this is a right tailed test, we need to find the P-value of a Z-value bigger than 0.791.\nWe can find the P-value using a Z-table, or with a programming language function:\nNote: The functions find the P-value (area) to the left side of Z-value.\nTo find the P-value for a right tail we need to subtract the left area from the total area: 1 - the output of the function.\nExample\nWith Python use the Scipy Stats library norm.cdf() function find the P-value of a Z-value bigger than 0.791:\nExample\nWith R use the built-in pnorm() function find the P-value of a Z-value bigger than 0.791:\nUsing either method we can find that the P-value is\n≈\n0.2145\n―\nThis tells us that the significance level (\nα\n) would need to be bigger than 0.2145, or 21.45%, to reject the null hypothesis.\nHere is an illustration of this test in a graph:\nThis P-value is bigger than any of the common significance levels (10%, 5%, 1%).\nSo the null hypothesis is kept at all of these significance levels.\nAnd we can summarize the conclusion stating:\nThe sample data does not support the claim that \"more than 20% of Nobel Prize winners were born in the US\" at a 10%, 5%, or 1% significance level.\nNote: It may still be true that the real population proportion is more than 20%.\nBut there was not strong enough evidence to support it with this sample.\nCalculating a P-Value for a Hypothesis Test with Programming\nMany programming languages can calculate the P-value to decide outcome of a hypothesis test.\nUsing software and programming to calculate statistics is more common for bigger sets of data, as calculating manually becomes difficult.\nThe P-value calculated here will tell us the lowest possible significance level where the null-hypothesis can be rejected.\nExample\nWith Python use the scipy and math libraries to calculate the P-value for a right tailed hypothesis test for a proportion.\nHere, the sample size is 40, the occurrences are 10, and the test is for a proportion bigger than 0.20.\nExample\nWith R use the built-in prop.test() function find the P-value for a right tailed hypothesis test for a proportion.\nHere, the sample size is 40, the occurrences are 10, and the test is for a proportion bigger than 0.20.\nNote: The conf.level in the R code is the reverse of the significance level.\nHere, the significance level is 0.05, or 5%, so the conf.level is 1-0.05 = 0.95, or 95%.\nLeft-Tailed and Two-Tailed Tests\nThis was an example of a right tailed test, where the alternative hypothesis claimed that parameter is bigger than the null hypothesis claim.\nYou can check out an equivalent step-by-step guide for other types here:\nLeft-Tailed Test\nTwo-Tailed Test",
      "examples": [
        "\"More than 20% of Nobel Prize winners were born in the US\"",
        "10 out of 40 Nobel Prize winners in the sample were born in the US",
        "Null hypothesis: 20% of Nobel Prize winners were born in the US.\nAlternative hypothesis: More than 20% of Nobel Prize winners were born in the US.",
        "H\n0\n:\np\n=\n0.20\nH\n1\n:\np\n>\n0.20",
        "The claimed (\nH\n0\n) population proportion (\np\n) was\n0.20\nThe sample proportion (\np\n^\n) was 10 out of 40, or:\n10\n40\n=\n0.25\nThe sample size (\nn\n) was\n40",
        "import scipy.stats as stats\nimport math\n\n# Specify the number of occurrences (x), the sample size (n), and the proportion claimed in the null-hypothesis (p)\nx = 10\nn = 40\np = 0.2\n\n# Calculate the sample proportion\np_hat = x/n\n\n# Calculate and print the test statistic\nprint((p_hat-p)/(math.sqrt((p*(1-p))/(n))))",
        "# Specify the sample occurrences (x), the sample size (n), and the null-hypothesis claim (p)\nx <- 10\nn <- 40\np <- 0.20\n\n# Calculate the sample proportion\np_hat = x/n\n\n# Calculate and print the test statistic\n(p_hat-p)/(sqrt((p*(1-p))/(n)))",
        "import scipy.stats as stats\nprint(stats.norm.ppf(1-0.05))",
        "qnorm(1-0.05)",
        "The sample data does not support the claim that \"more than 20% of Nobel Prize winners were born in the US\" at a 5% significance level.",
        "import scipy.stats as stats\nprint(1-stats.norm.cdf(0.791))",
        "1-pnorm(0.791)",
        "The sample data does not support the claim that \"more than 20% of Nobel Prize winners were born in the US\" at a 10%, 5%, or 1% significance level.",
        "import scipy.stats as stats\nimport math\n\n# Specify the number of occurrences (x), the sample size (n), and the proportion claimed in the null-hypothesis (p)\nx = 10\nn = 40\np = 0.2\n\n# Calculate the sample proportion\np_hat = x/n\n\n# Calculate the test statistic\ntest_stat = (p_hat-p)/(math.sqrt((p*(1-p))/(n)))\n\n# Output the p-value of the test statistic (right tailed test)\nprint(1-stats.norm.cdf(test_stat))",
        "# Specify the sample occurrences (x), the sample size (n), and the null-hypothesis claim (p)\nx <- 10\nn <- 40\np <- 0.20\n\n# P-value from right-tail proportion test at 0.05 significance level\nprop.test(x, n, p, alternative = c(\"greater\"), conf.level = 0.95, correct = FALSE)$p.value",
        "prop.test()",
        "norm.ppf()",
        "qnorm()",
        "norm.cdf()",
        "pnorm()",
        "conf.level"
      ]
    },
    {
      "title": "Statistics - Hypothesis Testing a Mean",
      "summary": "A population mean is an average of value a population.\nHypothesis tests are used to check a claim about the size of that population mean.\nHypothesis Testing a Mean\nThe following steps are used for a hypothesis test:\nCheck the conditions\nDefine the claims\nDecide the significance level\nCalculate the test statistic\nConclusion\nFor example:\nPopulation: Nobel Prize winners\nCategory: Age when they received the prize.\nAnd we want to check the claim:\n\"The average age of Nobel Prize winners when they received the prize is more than 55\"\nBy taking a sample of 30 randomly selected Nobel Prize winners we could find that:\nThe mean age in the sample (\nx\n¯\n) is 62.1\nThe standard deviation of age in the sample (\ns\n) is 13.46\nFrom this sample data we check the claim with the steps below.\n1. Checking the Conditions\nThe conditions for calculating a confidence interval for a proportion are:\nThe sample is randomly selected\nAnd either:\nThe population data is normally distributed\nSample size is large enough\nThe population data is normally distributed\nSample size is large enough\nA moderately large sample size, like 30, is typically large enough.\nIn the example, the sample size was 30 and it was randomly selected, so the conditions are fulfilled.\nNote: Checking if the data is normally distributed can be done with specialized statistical tests.\n2. Defining the Claims\nWe need to define a null hypothesis (\nH\n0\n) and an alternative hypothesis (\nH\n1\n) based on the claim we are checking.\nThe claim was:\n\"The average age of Nobel Prize winners when they received the prize is more than 55\"\nIn this case, the parameter is the mean age of Nobel Prize winners when they received the prize (\nμ\n).\nThe null and alternative hypothesis are then:\nNull hypothesis: The average age was 55.\nAlternative hypothesis: The average age was more than 55.\nWhich can be expressed with symbols as:\nH\n0\n:\nμ\n=\n55\nH\n1\n:\nμ\n>\n55\nThis is a 'right tailed' test, because the alternative hypothesis claims that the proportion is more than in the null hypothesis.\nIf the data supports the alternative hypothesis, we reject the null hypothesis and accept the alternative hypothesis.\nREMOVE ADS\n3. Deciding the Significance Level\nThe significance level (\nα\n) is the uncertainty we accept when rejecting the null hypothesis in a hypothesis test.\nThe significance level is a percentage probability of accidentally making the wrong conclusion.\nTypical significance levels are:\nα\n=\n0.1\n(10%)\nα\n=\n0.05\n(5%)\nα\n=\n0.01\n(1%)\nA lower significance level means that the evidence in the data needs to be stronger to reject the null hypothesis.\nThere is no \"correct\" significance level - it only states the uncertainty of the conclusion.\nNote: A 5% significance level means that when we reject a null hypothesis:\nWe expect to reject a true null hypothesis 5 out of 100 times.\n4. Calculating the Test Statistic\nThe test statistic is used to decide the outcome of the hypothesis test.\nThe test statistic is a standardized value calculated from the sample.\nThe formula for the test statistic (TS) of a population mean is:\nx\n¯\n−\nμ\ns\n⋅\nn\nx\n¯\n−\nμ\nis the difference between the sample mean (\nx\n¯\n) and the claimed population mean (\nμ\n).\ns\nis the sample standard deviation.\nn\nis the sample size.\nIn our example:\nThe claimed (\nH\n0\n) population mean (\nμ\n) was\n55\nThe sample mean (\nx\n¯\n) was\n62.1\nThe sample standard deviation (\ns\n) was\n13.46\nThe sample size (\nn\n) was\n30\nSo the test statistic (TS) is then:\n62.1\n−\n55\n13.46\n⋅\n30\n=\n7.1\n13.46\n⋅\n30\n≈\n0.528\n⋅\n5.477\n=\n2.889\n―\nYou can also calculate the test statistic using programming language functions:\nExampleGet your own Python Server\nWith Python use the scipy and math libraries to calculate the test statistic.\nExample\nWith R use built-in math and statistics functions to calculate the test statistic.\n5. Concluding\nThere are two main approaches for making the conclusion of a hypothesis test:\nThe critical value approach compares the test statistic with the critical value of the significance level.\nThe P-value approach compares the P-value of the test statistic and with the significance level.\nNote: The two approaches are only different in how they present the conclusion.\nThe Critical Value Approach\nFor the critical value approach we need to find the critical value (CV) of the significance level (\nα\n).\nFor a population mean test, the critical value (CV) is a T-value from a student's t-distribution.\nThis critical T-value (CV) defines the rejection region for the test.\nThe rejection region is an area of probability in the tails of the standard normal distribution.\nBecause the claim is that the population mean is more than 55, the rejection region is in the right tail:\nThe size of the rejection region is decided by the significance level (\nα\n).\nThe student's t-distribution is adjusted for the uncertainty from smaller samples.\nThis adjustment is called degrees of freedom (df), which is the sample size\n(\nn\n)\n−\n1\nIn this case the degrees of freedom (df) is:\n30\n−\n1\n=\n29\n―\nChoosing a significance level (\nα\n) of 0.01, or 1%, we can find the critical T-value from a T-table, or with a programming language function:\nExample\nWith Python use the Scipy Stats library t.ppf() function find the T-Value for an\nα\n= 0.01 at 29 degrees of freedom (df).\nExample\nWith R use the built-in qt() function to find the t-value for an\nα\n= 0.01 at 29 degrees of freedom (df).\nUsing either method we can find that the critical T-Value is\n≈\n2.462\n―\nFor a right tailed test we need to check if the test statistic (TS) is bigger than the critical value (CV).\nIf the test statistic is bigger than the critical value, the test statistic is in the rejection region.\nWhen the test statistic is in the rejection region, we reject the null hypothesis (\nH\n0\n).\nHere, the test statistic (TS) was\n≈\n2.889\n―\nand the critical value was\n≈\n2.462\n―\nHere is an illustration of this test in a graph:\nSince the test statistic was bigger than the critical value we reject the null hypothesis.\nThis means that the sample data supports the alternative hypothesis.\nAnd we can summarize the conclusion stating:\nThe sample data supports the claim that \"The average age of Nobel Prize winners when they received the prize is more than 55\" at a 1% significance level.\nThe P-Value Approach\nFor the P-value approach we need to find the P-value of the test statistic (TS).\nIf the P-value is smaller than the significance level (\nα\n), we reject the null hypothesis (\nH\n0\n).\nThe test statistic was found to be\n≈\n2.889\n―\nFor a population proportion test, the test statistic is a T-Value from a student's t-distribution.\nBecause this is a right tailed test, we need to find the P-value of a t-value bigger than 2.889.\nThe student's t-distribution is adjusted according to degrees of freedom (df), which is the sample size\n(\n30\n)\n−\n1\n=\n29\n―\nWe can find the P-value using a T-table, or with a programming language function:\nExample\nWith Python use the Scipy Stats library t.cdf() function find the P-value of a T-value bigger than 2.889 at 29 degrees of freedom (df):\nExample\nWith R use the built-in pt() function find the P-value of a T-Value bigger than 2.889 at 29 degrees of freedom (df):\nUsing either method we can find that the P-value is\n≈\n0.0036\n―\nThis tells us that the significance level (\nα\n) would need to be bigger than 0.0036, or 0.36%, to reject the null hypothesis.\nHere is an illustration of this test in a graph:\nThis P-value is smaller than any of the common significance levels (10%, 5%, 1%).\nSo the null hypothesis is rejected at all of these significance levels.\nAnd we can summarize the conclusion stating:\nThe sample data supports the claim that \"The average age of Nobel Prize winners when they received the prize is more than 55\" at a 10%, 5%, or 1% significance level.\nNote: An outcome of an hypothesis test that rejects the null hypothesis with a p-value of 0.36% means:\nFor this p-value, we only expect to reject a true null hypothesis 36 out of 10000 times.\nCalculating a P-Value for a Hypothesis Test with Programming\nMany programming languages can calculate the P-value to decide outcome of a hypothesis test.\nUsing software and programming to calculate statistics is more common for bigger sets of data, as calculating manually becomes difficult.\nThe P-value calculated here will tell us the lowest possible significance level where the null-hypothesis can be rejected.\nExample\nWith Python use the scipy and math libraries to calculate the P-value for a right tailed hypothesis test for a mean.\nHere, the sample size is 30, the sample mean is 62.1, the sample standard deviation is 13.46, and the test is for a mean bigger than 55.\nExample\nWith R use built-in math and statistics functions find the P-value for a right tailed hypothesis test for a mean.\nHere, the sample size is 30, the sample mean is 62.1, the sample standard deviation is 13.46, and the test is for a mean bigger than 55.\nLeft-Tailed and Two-Tailed Tests\nThis was an example of a right tailed test, where the alternative hypothesis claimed that parameter is bigger than the null hypothesis claim.\nYou can check out an equivalent step-by-step guide for other types here:\nLeft-Tailed Test\nTwo-Tailed Test",
      "examples": [
        "\"The average age of Nobel Prize winners when they received the prize is more than 55\"",
        "The mean age in the sample (\nx\n¯\n) is 62.1\nThe standard deviation of age in the sample (\ns\n) is 13.46",
        "Null hypothesis: The average age was 55.\nAlternative hypothesis: The average age was more than 55.",
        "H\n0\n:\nμ\n=\n55\nH\n1\n:\nμ\n>\n55",
        "The claimed (\nH\n0\n) population mean (\nμ\n) was\n55\nThe sample mean (\nx\n¯\n) was\n62.1\nThe sample standard deviation (\ns\n) was\n13.46\nThe sample size (\nn\n) was\n30",
        "import scipy.stats as stats\nimport math\n\n# Specify the sample mean (x_bar), the sample standard deviation (s), the mean claimed in the null-hypothesis (mu_null), and the sample size (n)\nx_bar = 62.1\ns = 13.46\nmu_null = 55\nn = 30\n\n# Calculate and print the test statistic\nprint((x_bar - mu_null)/(s/math.sqrt(n)))",
        "# Specify the sample mean (x_bar), the sample standard deviation (s), the mean claimed in the null-hypothesis (mu_null), and the sample size (n)\nx_bar <- 62.1\ns <- 13.46\nmu_null <- 55\nn <- 30\n\n# Output the test statistic\n(x_bar - mu_null)/(s/sqrt(n))",
        "import scipy.stats as stats\nprint(stats.t.ppf(1-0.01, 29))",
        "qt(1-0.01, 29)",
        "The sample data supports the claim that \"The average age of Nobel Prize winners when they received the prize is more than 55\" at a 1% significance level.",
        "import scipy.stats as stats\nprint(1-stats.t.cdf(2.889, 29))",
        "1-pt(2.889, 29)",
        "The sample data supports the claim that \"The average age of Nobel Prize winners when they received the prize is more than 55\" at a 10%, 5%, or 1% significance level.",
        "import scipy.stats as stats\nimport math\n\n# Specify the sample mean (x_bar), the sample standard deviation (s), the mean claimed in the null-hypothesis (mu_null), and the sample size (n)\nx_bar = 62.1\ns = 13.46\nmu_null = 55\nn = 30\n\n# Calculate the test statistic\ntest_stat = (x_bar - mu_null)/(s/math.sqrt(n))\n\n# Output the p-value of the test statistic (right tailed test)\nprint(1-stats.t.cdf(test_stat, n-1))",
        "# Specify the sample mean (x_bar), the sample standard deviation (s), the mean claimed in the null-hypothesis (mu_null), and the sample size (n)\nx_bar <- 62.1\ns <- 13.46\nmu_null <- 55\nn <- 30\n\n# Calculate the test statistic\ntest_stat = (x_bar - mu_null)/(s/sqrt(n))\n\n# P-value the p-value of the test statistic (right tailed test)\n1-pt(test_stat, n-1)",
        "t.ppf()",
        "qt()",
        "t.cdf()",
        "pt()"
      ]
    },
    {
      "title": "Statistics - Z-table",
      "summary": "The Z-distribution is a standardized normal distribution where the mean is 0 and the standard deviation is 1.\nThe Z-distribution can be used to find which percent of a population is within a particular number of standard deviations.\nZ-Distribution and Table for Negative Z-values\nThe numbers in the table cells correspond to the area under the graph.\nThe area under graph is the probability of getting a value that is smaller than z.\nThe probabilities are decimal values and can be thought of as percentages. For example: 0.1515 is 15.15%.\nREMOVE ADS\nZ-Distribution and Table of Positive Z-Values\nThe numbers in the table cells correspond to the area under the graph.\nThe area under graph is the probability of getting a value that is smaller than z.\nThe probabilities are decimal values and can be thought of as percentages. For example: 0.7088 is 70.88%.\nExample of How to Use the Z-Table\nThe Z-value is found combining the numbers in the first column and the first row.\nFor example, the probability for getting a z-value smaller than 1.85 is found by finding the number in the table where the row is 1.8 and the column is 0.05.\nSo the probability is: 0.9678 is 96.78%.",
      "examples": []
    },
    {
      "title": "Statistics - T-table",
      "summary": "The Student's t-distribution is similar to a normal distribution. But it is adjusted for uncertainty with smaller sample sizes.\nSmaller sample sizes give fewer degrees of freedom (df), which makes the tails larger.\nT-Distribution and Table of Critical T-Values\nThe numbers in the table cells correspond to critical t-values for different tail-areas (α).\nThe tail areas are probabilities. For example, 0.05 is 5% percent probability.\nEach row corresponds to different degrees of freedom (df).",
      "examples": []
    },
    {
      "title": "Statistics - Hypothesis Testing a Proportion (Left Tailed)",
      "summary": "A population proportion is the share of a population that belongs to a particular category.\nHypothesis tests are used to check a claim about the size of that population proportion.\nHypothesis Testing a Proportion\nThe following steps are used for a hypothesis test:\nCheck the conditions\nDefine the claims\nDecide the significance level\nCalculate the test statistic\nConclusion\nFor example:\nPopulation: Nobel Prize winners\nCategory: Born in the United States of America\nAnd we want to check the claim:\n\"Less than 45% of Nobel Prize winners were born in the US\"\nBy taking a sample of 40 randomly selected Nobel Prize winners we could find that:\n10 out of 40 Nobel Prize winners in the sample were born in the US\nThe sample proportion is then:\n10\n40\n=\n0.25\n, or 25%.\nFrom this sample data we check the claim with the steps below.\n1. Checking the Conditions\nThe conditions for calculating a confidence interval for a proportion are:\nThe sample is randomly selected\nThere is only two options:\nBeing in the category\nNot being in the category\nThe sample needs at least:\n5 members in the category\n5 members not in the category\nIn our example, we randomly selected 10 people that were born in the US.\nThe rest were not born in the US, so there are 30 in the other category.\nThe conditions are fulfilled in this case.\nNote: It is possible to do a hypothesis test without having 5 of each category. But special adjustments need to be made.\n2. Defining the Claims\nWe need to define a null hypothesis (\nH\n0\n) and an alternative hypothesis (\nH\n1\n) based on the claim we are checking.\nThe claim was:\n\"Less than 45% of Nobel Prize winners were born in the US\"\nIn this case, the parameter is the proportion of Nobel Prize winners born in the US (\np\n).\nThe null and alternative hypothesis are then:\nNull hypothesis: 45% of Nobel Prize winners were born in the US.\nAlternative hypothesis: Less than 45% of Nobel Prize winners were born in the US.\nWhich can be expressed with symbols as:\nH\n0\n:\np\n=\n0.45\nH\n1\n:\np\n<\n0.45\nThis is a 'left tailed' test, because the alternative hypothesis claims that the proportion is less than in the null hypothesis.\nIf the data supports the alternative hypothesis, we reject the null hypothesis and accept the alternative hypothesis.\n3. Deciding the Significance Level\nThe significance level (\nα\n) is the uncertainty we accept when rejecting the null hypothesis in a hypothesis test.\nThe significance level is a percentage probability of accidentally making the wrong conclusion.\nTypical significance levels are:\nα\n=\n0.1\n(10%)\nα\n=\n0.05\n(5%)\nα\n=\n0.01\n(1%)\nA lower significance level means that the evidence in the data needs to be stronger to reject the null hypothesis.\nThere is no \"correct\" significance level - it only states the uncertainty of the conclusion.\nNote: A 5% significance level means that when we reject a null hypothesis:\nWe expect to reject a true null hypothesis 5 out of 100 times.\n4. Calculating the Test Statistic\nThe test statistic is used to decide the outcome of the hypothesis test.\nThe test statistic is a standardized value calculated from the sample.\nThe formula for the test statistic (TS) of a population proportion is:\np\n^\n−\np\np\n(\n1\n−\np\n)\n⋅\nn\np\n^\n−\np\nis the difference between the sample proportion (\np\n^\n) and the claimed population proportion (\np\n).\nn\nis the sample size.\nIn our example:\nThe claimed (\nH\n0\n) population proportion (\np\n) was\n0.45\nThe sample proportion (\np\n^\n) was 10 out of 40, or:\n10\n40\n=\n0.25\nThe sample size (\nn\n) was\n40\nSo the test statistic (TS) is then:\n0.25\n−\n0.45\n0.45\n(\n1\n−\n0.45\n)\n⋅\n40\n=\n−\n0.2\n0.45\n(\n0.55\n)\n⋅\n40\n=\n−\n0.2\n0.2475\n⋅\n40\n≈\n−\n0.2\n0.498\n⋅\n6.325\n=\n−\n2.543\n―\nYou can also calculate the test statistic using programming language functions:\nExampleGet your own Python Server\nWith Python use the scipy and math libraries to calculate the test statistic for a proportion.\nExample\nWith R use the built-in math functions to calculate the test statistic for a proportion.\n5. Concluding\nThere are two main approaches for making the conclusion of a hypothesis test:\nThe critical value approach compares the test statistic with the critical value of the significance level.\nThe P-value approach compares the P-value of the test statistic and with the significance level.\nNote: The two approaches are only different in how they present the conclusion.\nThe Critical Value Approach\nFor the critical value approach we need to find the critical value (CV) of the significance level (\nα\n).\nFor a population proportion test, the critical value (CV) is a Z-value from a standard normal distribution.\nThis critical Z-value (CV) defines the rejection region for the test.\nThe rejection region is an area of probability in the tails of the standard normal distribution.\nBecause the claim is that the population proportion is less than 45%, the rejection region is in the left tail:\nThe size of the rejection region is decided by the significance level (\nα\n).\nChoosing a significance level (\nα\n) of 0.01, or 1%, we can find the critical Z-value from a Z-table, or with a programming language function:\nExample\nWith Python use the Scipy Stats library norm.ppf() function find the Z-value for an\nα\n= 0.01 in the left tail.\nExample\nWith R use the built-in qnorm() function to find the Z-value for an\nα\n= 0.01 in the left tail.\nUsing either method we can find that the critical Z-value is\n≈\n−\n2.3264\n―\nFor a left tailed test we need to check if the test statistic (TS) is smaller than the critical value (CV).\nIf the test statistic is smaller than the critical value, the test statistic is in the rejection region.\nWhen the test statistic is in the rejection region, we reject the null hypothesis (\nH\n0\n).\nHere, the test statistic (TS) was\n≈\n−\n2.543\n―\nand the critical value was\n≈\n−\n2.3264\n―\nHere is an illustration of this test in a graph:\nSince the test statistic was smaller than the critical value we reject the null hypothesis.\nThis means that the sample data supports the alternative hypothesis.\nAnd we can summarize the conclusion stating:\nThe sample data supports the claim that \"less than 45% of Nobel Prize winners were born in the US\" at a 1% significance level.\nThe P-Value Approach\nFor the P-value approach we need to find the P-value of the test statistic (TS).\nIf the P-value is\nsmaller\nthan the significance level (\nα\n), we reject the null hypothesis (\nH\n0\n).\nThe test statistic was found to be\n≈\n−\n2.543\n―\nFor a population proportion test, the test statistic is a Z-Value from a standard normal distribution.\nBecause this is a left tailed test, we need to find the P-value of a Z-value smaller than -2.543.\nWe can find the P-value using a Z-table, or with a programming language function:\nExample\nWith Python use the Scipy Stats library norm.cdf() function find the P-value of a Z-value smaller than -2.543:\nExample\nWith R use the built-in pnorm() function find the P-value of a Z-value smaller than -2.543:\nUsing either method we can find that the P-value is\n≈\n0.0055\n―\nThis tells us that the significance level (\nα\n) would need to be bigger than 0.0055, or 0.55%, to reject the null hypothesis.\nHere is an illustration of this test in a graph:\nThis P-value is smaller than any of the common significance levels (10%, 5%, 1%).\nSo the null hypothesis is rejected at all of these significance levels.\nAnd we can summarize the conclusion stating:\nThe sample data supports the claim that \"less than 45% of Nobel Prize winners were born in the US\" at a 10%, 5%, and 1% significance level.\nCalculating a P-Value for a Hypothesis Test with Programming\nMany programming languages can calculate the P-value to decide outcome of a hypothesis test.\nUsing software and programming to calculate statistics is more common for bigger sets of data, as calculating manually becomes difficult.\nThe P-value calculated here will tell us the lowest possible significance level where the null-hypothesis can be rejected.\nExample\nWith Python use the scipy and math libraries to calculate the P-value for a left tailed hypothesis test for a proportion.\nHere, the sample size is 40, the occurrences are 10, and the test is for a proportion smaller than 0.45.\nExample\nWith R use the built-in prop.test() function find the P-value for a left tailed hypothesis test for a proportion.\nHere, the sample size is 40, the occurrences are 10, and the test is for a proportion bigger than 0.45.\nNote: The conf.level in the R code is the reverse of the significance level.\nHere, the significance level is 0.01, or 1%, so the conf.level is 1-0.01 = 0.99, or 99%.\nLeft-Tailed and Two-Tailed Tests\nThis was an example of a left tailed test, where the alternative hypothesis claimed that parameter is smaller than the null hypothesis claim.\nYou can check out an equivalent step-by-step guide for other types here:\nRight-Tailed Test\nTwo-Tailed Test",
      "examples": [
        "\"Less than 45% of Nobel Prize winners were born in the US\"",
        "10 out of 40 Nobel Prize winners in the sample were born in the US",
        "Null hypothesis: 45% of Nobel Prize winners were born in the US.\nAlternative hypothesis: Less than 45% of Nobel Prize winners were born in the US.",
        "H\n0\n:\np\n=\n0.45\nH\n1\n:\np\n<\n0.45",
        "The claimed (\nH\n0\n) population proportion (\np\n) was\n0.45\nThe sample proportion (\np\n^\n) was 10 out of 40, or:\n10\n40\n=\n0.25\nThe sample size (\nn\n) was\n40",
        "import scipy.stats as stats\nimport math\n\n# Specify the number of occurrences (x), the sample size (n), and the proportion claimed in the null-hypothesis (p)\nx = 10\nn = 40\np = 0.45\n\n# Calculate the sample proportion\np_hat = x/n\n\n# Calculate and print the test statistic\nprint((p_hat-p)/(math.sqrt((p*(1-p))/(n))))",
        "# Specify the sample occurrences (x), the sample size (n), and the null-hypothesis claim (p)\nx <- 10\nn <- 40\np <- 0.45\n\n# Calculate the sample proportion\np_hat = x/n\n\n# Calculate and output the test statistic\n(p_hat-p)/(sqrt((p*(1-p))/(n)))",
        "import scipy.stats as stats\nprint(stats.norm.ppf(0.01))",
        "qnorm(0.01)",
        "The sample data supports the claim that \"less than 45% of Nobel Prize winners were born in the US\" at a 1% significance level.",
        "import scipy.stats as stats\nprint(stats.norm.cdf(-2.543))",
        "pnorm(-2.543)",
        "The sample data supports the claim that \"less than 45% of Nobel Prize winners were born in the US\" at a 10%, 5%, and 1% significance level.",
        "import scipy.stats as stats\nimport math\n\n# Specify the number of occurrences (x), the sample size (n), and the proportion claimed in the null-hypothesis (p)\nx = 10\nn = 40\np = 0.45\n\n# Calculate the sample proportion\np_hat = x/n\n\n# Calculate the test statistic\ntest_stat = (p_hat-p)/(math.sqrt((p*(1-p))/(n)))\n\n# Output the p-value of the test statistic (left tailed test)\nprint(stats.norm.cdf(test_stat))",
        "# Specify the sample occurrences (x), the sample size (n), and the null-hypothesis claim (p)\nx <- 10\nn <- 40\np <- 0.45\n\n# P-value from left-tail proportion test at 0.01 significance level\nprop.test(x, n, p, alternative = c(\"less\"), conf.level = 0.99, correct = FALSE)$p.value",
        "norm.ppf()",
        "qnorm()",
        "norm.cdf()",
        "pnorm()",
        "prop.test()",
        "conf.level"
      ]
    },
    {
      "title": "Statistics - Hypothesis Testing a Proportion (Two Tailed)",
      "summary": "A population proportion is the share of a population that belongs to a particular category.\nHypothesis tests are used to check a claim about the size of that population proportion.\nHypothesis Testing a Proportion\nThe following steps are used for a hypothesis test:\nCheck the conditions\nDefine the claims\nDecide the significance level\nCalculate the test statistic\nConclusion\nFor example:\nPopulation: Nobel Prize winners\nCategory: Women\nAnd we want to check the claim:\n\"The share of Nobel Prize winners that are women is not 50%\"\nBy taking a sample of 100 randomly selected Nobel Prize winners we could find that:\n10 out of 100 Nobel Prize winners in the sample were women\nThe sample proportion is then:\n10\n100\n=\n0.1\n, or 10%.\nFrom this sample data we check the claim with the steps below.\n1. Checking the Conditions\nThe conditions for calculating a confidence interval for a proportion are:\nThe sample is randomly selected\nThere is only two options:\nBeing in the category\nNot being in the category\nBeing in the category\nNot being in the category\nThe sample needs at least:\n5 members in the category\n5 members not in the category\n5 members in the category\n5 members not in the category\nIn our example, we randomly selected 10 people that were women.\nThe rest were not women, so there are 90 in the other category.\nThe conditions are fulfilled in this case.\nNote: It is possible to do a hypothesis test without having 5 of each category. But special adjustments need to be made.\n2. Defining the Claims\nWe need to define a null hypothesis (\nH\n0\n) and an alternative hypothesis (\nH\n1\n) based on the claim we are checking.\nThe claim was:\n\"The share of Nobel Prize winners that are women is not 50%\"\nIn this case, the parameter is the proportion of Nobel Prize winners that are women (\np\n).\nThe null and alternative hypothesis are then:\nNull hypothesis: 50% of Nobel Prize winners were women.\nAlternative hypothesis: The share of Nobel Prize winners that are women is not 50%\nWhich can be expressed with symbols as:\nH\n0\n:\np\n=\n0.50\nH\n1\n:\np\n≠\n0.50\nThis is a 'two-tailed' test, because the alternative hypothesis claims that the proportion is different (larger or smaller) than in the null hypothesis.\nIf the data supports the alternative hypothesis, we reject the null hypothesis and accept the alternative hypothesis.\nREMOVE ADS\n3. Deciding the Significance Level\nThe significance level (\nα\n) is the uncertainty we accept when rejecting the null hypothesis in a hypothesis test.\nThe significance level is a percentage probability of accidentally making the wrong conclusion.\nTypical significance levels are:\nα\n=\n0.1\n(10%)\nα\n=\n0.05\n(5%)\nα\n=\n0.01\n(1%)\nA lower significance level means that the evidence in the data needs to be stronger to reject the null hypothesis.\nThere is no \"correct\" significance level - it only states the uncertainty of the conclusion.\nNote: A 5% significance level means that when we reject a null hypothesis:\nWe expect to reject a true null hypothesis 5 out of 100 times.\n4. Calculating the Test Statistic\nThe test statistic is used to decide the outcome of the hypothesis test.\nThe test statistic is a standardized value calculated from the sample.\nThe formula for the test statistic (TS) of a population proportion is:\np\n^\n−\np\np\n(\n1\n−\np\n)\n⋅\nn\np\n^\n−\np\nis the difference between the sample proportion (\np\n^\n) and the claimed population proportion (\np\n).\nn\nis the sample size.\nIn our example:\nThe claimed (\nH\n0\n) population proportion (\np\n) was\n0.50\nThe sample proportion (\np\n^\n) was 10 out of 100, or:\n10\n100\n=\n0.10\nThe sample size (\nn\n) was\n100\nSo the test statistic (TS) is then:\n0.1\n−\n0.5\n0.5\n(\n1\n−\n0.5\n)\n⋅\n100\n=\n−\n0.4\n0.5\n(\n0.5\n)\n⋅\n100\n=\n−\n0.4\n0.25\n⋅\n100\n=\n−\n0.4\n0.5\n⋅\n10\n=\n−\n8\n―\nYou can also calculate the test statistic using programming language functions:\nExampleGet your own Python Server\nWith Python use the scipy and math libraries to calculate the test statistic for a proportion.\nExample\nWith R use the built-in math functions to calculate the test statistic for a proportion.\n5. Concluding\nThere are two main approaches for making the conclusion of a hypothesis test:\nThe critical value approach compares the test statistic with the critical value of the significance level.\nThe P-value approach compares the P-value of the test statistic and with the significance level.\nNote: The two approaches are only different in how they present the conclusion.\nThe Critical Value Approach\nFor the critical value approach we need to find the critical value (CV) of the significance level (\nα\n).\nFor a population proportion test, the critical value (CV) is a Z-value from a standard normal distribution.\nThis critical Z-value (CV) defines the rejection region for the test.\nThe rejection region is an area of probability in the tails of the standard normal distribution.\nBecause the claim is that the population proportion is different from 50%, the rejection region is split into both the left and right tail:\nThe size of the rejection region is decided by the significance level (\nα\n).\nChoosing a significance level (\nα\n) of 0.01, or 1%, we can find the critical Z-value from a Z-table, or with a programming language function:\nNote: Because this is a two-tailed test the tail area (\nα\n) needs to be split in half (divided by 2).\nExample\nWith Python use the Scipy Stats library norm.ppf() function find the Z-value for an\nα\n/2 = 0.005 in the left tail.\nExample\nWith R use the built-in qnorm() function to find the Z-value for an\nα\n= 0.005 in the left tail.\nUsing either method we can find that the critical Z-value in the left tail is\n≈\n−\n2.5758\n―\nSince a normal distribution i symmetric, we know that the critical Z-value in the right tail will be the same number, only positive:\n2.5758\n―\nFor a two-tailed test we need to check if the test statistic (TS) is smaller than the negative critical value (-CV), or bigger than the positive critical value (CV).\nIf the test statistic is smaller than the negative critical value, the test statistic is in the rejection region.\nIf the test statistic is bigger than the positive critical value, the test statistic is in the rejection region.\nWhen the test statistic is in the rejection region, we reject the null hypothesis (\nH\n0\n).\nHere, the test statistic (TS) was\n≈\n−\n8\n―\nand the critical value was\n≈\n−\n2.5758\n―\nHere is an illustration of this test in a graph:\nSince the test statistic was smaller than the negative critical value we reject the null hypothesis.\nThis means that the sample data supports the alternative hypothesis.\nAnd we can summarize the conclusion stating:\nThe sample data supports the claim that \"The share of Nobel Prize winners that are women is not 50%\" at a 1% significance level.\nThe P-Value Approach\nFor the P-value approach we need to find the P-value of the test statistic (TS).\nIf the P-value is smaller than the significance level (\nα\n), we reject the null hypothesis (\nH\n0\n).\nThe test statistic was found to be\n≈\n−\n8\n―\nFor a population proportion test, the test statistic is a Z-Value from a standard normal distribution.\nBecause this is a two-tailed test, we need to find the P-value of a Z-value smaller than -8 and multiply it by 2.\nWe can find the P-value using a Z-table, or with a programming language function:\nExample\nWith Python use the Scipy Stats library norm.cdf() function find the P-value of a Z-value smaller than -8 for a two tailed test:\nExample\nWith R use the built-in pnorm() function find the P-value of a Z-value smaller than -8 for a two tailed test:\nUsing either method we can find that the P-value is\n≈\n1.25\n⋅\n10\n−\n15\n―\nor\n0.00000000000000125\nThis tells us that the significance level (\nα\n) would need to be bigger than 0.000000000000125%, to reject the null hypothesis.\nHere is an illustration of this test in a graph:\nThis P-value is smaller than any of the common significance levels (10%, 5%, 1%).\nSo the null hypothesis is rejected at all of these significance levels.\nAnd we can summarize the conclusion stating:\nThe sample data supports the claim that \"The share of Nobel Prize winners that are women is not 50%\" at a 10%, 5%, and 1% significance level.\nCalculating a P-Value for a Hypothesis Test with Programming\nMany programming languages can calculate the P-value to decide outcome of a hypothesis test.\nUsing software and programming to calculate statistics is more common for bigger sets of data, as calculating manually becomes difficult.\nThe P-value calculated here will tell us the lowest possible significance level where the null-hypothesis can be rejected.\nExample\nWith Python use the scipy and math libraries to calculate the P-value for a two-tailed tailed hypothesis test for a proportion.\nHere, the sample size is 100, the occurrences are 10, and the test is for a proportion different from than 0.50.\nExample\nWith R use the built-in prop.test() function find the P-value for a left tailed hypothesis test for a proportion.\nHere, the sample size is 100, the occurrences are 10, and the test is for a proportion different from 0.50.\nNote: The conf.level in the R code is the reverse of the significance level.\nHere, the significance level is 0.01, or 1%, so the conf.level is 1-0.01 = 0.99, or 99%.\nLeft-Tailed and Two-Tailed Tests\nThis was an example of a two tailed test, where the alternative hypothesis claimed that parameter is different from the null hypothesis claim.\nYou can check out an equivalent step-by-step guide for other types here:\nRight-Tailed Test\nLeft-Tailed Test",
      "examples": [
        "\"The share of Nobel Prize winners that are women is not 50%\"",
        "10 out of 100 Nobel Prize winners in the sample were women",
        "Null hypothesis: 50% of Nobel Prize winners were women.\nAlternative hypothesis: The share of Nobel Prize winners that are women is not 50%",
        "H\n0\n:\np\n=\n0.50\nH\n1\n:\np\n≠\n0.50",
        "The claimed (\nH\n0\n) population proportion (\np\n) was\n0.50\nThe sample proportion (\np\n^\n) was 10 out of 100, or:\n10\n100\n=\n0.10\nThe sample size (\nn\n) was\n100",
        "import scipy.stats as stats\nimport math\n\n# Specify the number of occurrences (x), the sample size (n), and the proportion claimed in the null-hypothesis (p)\nx = 10\nn = 100\np = 0.5\n\n# Calculate the sample proportion\np_hat = x/n\n\n# Calculate and print the test statistic\nprint((p_hat-p)/(math.sqrt((p*(1-p))/(n))))",
        "# Specify the sample occurrences (x), the sample size (n), and the null-hypothesis claim (p)\nx <- 10\nn <- 100\np <- 0.5\n\n# Calculate the sample proportion\np_hat = x/n\n\n# Calculate and output the test statistic\n(p_hat-p)/(sqrt((p*(1-p))/(n)))",
        "import scipy.stats as stats\nprint(stats.norm.ppf(0.005))",
        "qnorm(0.005)",
        "The sample data supports the claim that \"The share of Nobel Prize winners that are women is not 50%\" at a 1% significance level.",
        "import scipy.stats as stats\nprint(2*stats.norm.cdf(-8))",
        "2*pnorm(-8)",
        "The sample data supports the claim that \"The share of Nobel Prize winners that are women is not 50%\" at a 10%, 5%, and 1% significance level.",
        "import scipy.stats as stats\nimport math\n\n# Specify the number of occurrences (x), the sample size (n), and the proportion claimed in the null-hypothesis (p)\nx = 10\nn = 100\np = 0.5\n\n# Calculate the sample proportion\np_hat = x/n\n\n# Calculate the test statistic\ntest_stat = (p_hat-p)/(math.sqrt((p*(1-p))/(n)))\n\n# Output the p-value of the test statistic (two-tailed test)\nprint(2*stats.norm.cdf(test_stat))",
        "# Specify the sample occurrences (x), the sample size (n), and the null-hypothesis claim (p)\nx <- 10\nn <- 100\np <- 0.5\n\n# P-value from left-tail proportion test at 0.01 significance level\nprop.test(x, n, p, alternative = c(\"two.sided\"), conf.level = 0.99, correct = FALSE)$p.value",
        "norm.ppf()",
        "qnorm()",
        "norm.cdf()",
        "pnorm()",
        "prop.test()",
        "conf.level"
      ]
    },
    {
      "title": "Statistics - Hypothesis Testing a Mean (Left Tailed)",
      "summary": "A population mean is an average of value a population.\nHypothesis tests are used to check a claim about the size of that population mean.\nHypothesis Testing a Mean\nThe following steps are used for a hypothesis test:\nCheck the conditions\nDefine the claims\nDecide the significance level\nCalculate the test statistic\nConclusion\nFor example:\nPopulation: Nobel Prize winners\nCategory: Age when they received the prize.\nAnd we want to check the claim:\n\"The average age of Nobel Prize winners when they received the prize is less than 60\"\nBy taking a sample of 30 randomly selected Nobel Prize winners we could find that:\nThe mean age in the sample (\nx\n¯\n) is 62.1\nThe standard deviation of age in the sample (\ns\n) is 13.46\nFrom this sample data we check the claim with the steps below.\n1. Checking the Conditions\nThe conditions for calculating a confidence interval for a proportion are:\nThe sample is randomly selected\nAnd either:\nThe population data is normally distributed\nSample size is large enough\nThe population data is normally distributed\nSample size is large enough\nA moderately large sample size, like 30, is typically large enough.\nIn the example, the sample size was 30 and it was randomly selected, so the conditions are fulfilled.\nNote: Checking if the data is normally distributed can be done with specialized statistical tests.\n2. Defining the Claims\nWe need to define a null hypothesis (\nH\n0\n) and an alternative hypothesis (\nH\n1\n) based on the claim we are checking.\nThe claim was:\n\"The average age of Nobel Prize winners when they received the prize is less than 60\"\nIn this case, the parameter is the mean age of Nobel Prize winners when they received the prize (\nμ\n).\nThe null and alternative hypothesis are then:\nNull hypothesis: The average age was 60.\nAlternative hypothesis: The average age was less than 60.\nWhich can be expressed with symbols as:\nH\n0\n:\nμ\n=\n60\nH\n1\n:\nμ\n<\n60\nThis is a 'left tailed' test, because the alternative hypothesis claims that the proportion is less than in the null hypothesis.\nIf the data supports the alternative hypothesis, we reject the null hypothesis and accept the alternative hypothesis.\nREMOVE ADS\n3. Deciding the Significance Level\nThe significance level (\nα\n) is the uncertainty we accept when rejecting the null hypothesis in a hypothesis test.\nThe significance level is a percentage probability of accidentally making the wrong conclusion.\nTypical significance levels are:\nα\n=\n0.1\n(10%)\nα\n=\n0.05\n(5%)\nα\n=\n0.01\n(1%)\nA lower significance level means that the evidence in the data needs to be stronger to reject the null hypothesis.\nThere is no \"correct\" significance level - it only states the uncertainty of the conclusion.\nNote: A 5% significance level means that when we reject a null hypothesis:\nWe expect to reject a true null hypothesis 5 out of 100 times.\n4. Calculating the Test Statistic\nThe test statistic is used to decide the outcome of the hypothesis test.\nThe test statistic is a standardized value calculated from the sample.\nThe formula for the test statistic (TS) of a population mean is:\nx\n¯\n−\nμ\ns\n⋅\nn\nx\n¯\n−\nμ\nis the difference between the sample mean (\nx\n¯\n) and the claimed population mean (\nμ\n).\ns\nis the sample standard deviation.\nn\nis the sample size.\nIn our example:\nThe claimed (\nH\n0\n) population mean (\nμ\n) was\n60\nThe sample mean (\nx\n¯\n) was\n62.1\nThe sample standard deviation (\ns\n) was\n13.46\nThe sample size (\nn\n) was\n30\nSo the test statistic (TS) is then:\n62.1\n−\n60\n13.46\n⋅\n30\n=\n2.1\n13.46\n⋅\n30\n≈\n0.156\n⋅\n5.477\n=\n0.855\n―\nYou can also calculate the test statistic using programming language functions:\nExampleGet your own Python Server\nWith Python use the scipy and math libraries to calculate the test statistic.\nExample\nWith R use built-in math and statistics functions to calculate the test statistic.\n5. Concluding\nThere are two main approaches for making the conclusion of a hypothesis test:\nThe critical value approach compares the test statistic with the critical value of the significance level.\nThe P-value approach compares the P-value of the test statistic and with the significance level.\nNote: The two approaches are only different in how they present the conclusion.\nThe Critical Value Approach\nFor the critical value approach we need to find the critical value (CV) of the significance level (\nα\n).\nFor a population mean test, the critical value (CV) is a T-value from a student's t-distribution.\nThis critical T-value (CV) defines the rejection region for the test.\nThe rejection region is an area of probability in the tails of the standard normal distribution.\nBecause the claim is that the population mean is less than 60, the rejection region is in the left tail:\nThe size of the rejection region is decided by the significance level (\nα\n).\nThe student's t-distribution is adjusted for the uncertainty from smaller samples.\nThis adjustment is called degrees of freedom (df), which is the sample size\n(\nn\n)\n−\n1\nIn this case the degrees of freedom (df) is:\n30\n−\n1\n=\n29\n―\nChoosing a significance level (\nα\n) of 0.05, or 5%, we can find the critical T-value from a T-table, or with a programming language function:\nExample\nWith Python use the Scipy Stats library t.ppf() function find the T-Value for an\nα\n= 0.05 at 29 degrees of freedom (df).\nExample\nWith R use the built-in qt() function to find the t-value for an\nα\n= 0.05 at 29 degrees of freedom (df).\nUsing either method we can find that the critical T-Value is\n≈\n−\n1.699\n―\nFor a left tailed test we need to check if the test statistic (TS) is smaller than the critical value (CV).\nIf the test statistic is smaller the critical value, the test statistic is in the rejection region.\nWhen the test statistic is in the rejection region, we reject the null hypothesis (\nH\n0\n).\nHere, the test statistic (TS) was\n≈\n0.855\n―\nand the critical value was\n≈\n−\n1.699\n―\nHere is an illustration of this test in a graph:\nSince the test statistic was bigger than the critical value we keep the null hypothesis.\nThis means that the sample data does not support the alternative hypothesis.\nAnd we can summarize the conclusion stating:\nThe sample data does not support the claim that \"The average age of Nobel Prize winners when they received the prize is less than 60\" at a 5% significance level.\nThe P-Value Approach\nFor the P-value approach we need to find the P-value of the test statistic (TS).\nIf the P-value is smaller than the significance level (\nα\n), we reject the null hypothesis (\nH\n0\n).\nThe test statistic was found to be\n≈\n0.855\n―\nFor a population proportion test, the test statistic is a T-Value from a student's t-distribution.\nBecause this is a left tailed test, we need to find the P-value of a t-value smaller than 0.855.\nThe student's t-distribution is adjusted according to degrees of freedom (df), which is the sample size\n(\n30\n)\n−\n1\n=\n29\n―\nWe can find the P-value using a T-table, or with a programming language function:\nExample\nWith Python use the Scipy Stats library t.cdf() function find the P-value of a T-value smaller than 0.855 at 29 degrees of freedom (df):\nExample\nWith R use the built-in pt() function find the P-value of a T-Value smaller than 0.855 at 29 degrees of freedom (df):\nUsing either method we can find that the P-value is\n≈\n0.800\n―\nThis tells us that the significance level (\nα\n) would need to be smaller 0.80, or 80%, to reject the null hypothesis.\nHere is an illustration of this test in a graph:\nThis P-value is far bigger than any of the common significance levels (10%, 5%, 1%).\nSo the null hypothesis is kept at all of these significance levels.\nAnd we can summarize the conclusion stating:\nThe sample data does not support the claim that \"The average age of Nobel Prize winners when they received the prize is less than 60\" at a 10%, 5%, or 1% significance level.\nCalculating a P-Value for a Hypothesis Test with Programming\nMany programming languages can calculate the P-value to decide outcome of a hypothesis test.\nUsing software and programming to calculate statistics is more common for bigger sets of data, as calculating manually becomes difficult.\nThe P-value calculated here will tell us the lowest possible significance level where the null-hypothesis can be rejected.\nExample\nWith Python use the scipy and math libraries to calculate the P-value for a left tailed hypothesis test for a mean.\nHere, the sample size is 30, the sample mean is 62.1, the sample standard deviation is 13.46, and the test is for a mean smaller 60.\nExample\nWith R use built-in math and statistics functions find the P-value for a left tailed hypothesis test for a mean.\nHere, the sample size is 30, the sample mean is 62.1, the sample standard deviation is 13.46, and the test is for a mean smaller 60.\nLeft-Tailed and Two-Tailed Tests\nThis was an example of a left tailed test, where the alternative hypothesis claimed that parameter is smaller than the null hypothesis claim.\nYou can check out an equivalent step-by-step guide for other types here:\nRight-Tailed Test\nTwo-Tailed Test",
      "examples": [
        "\"The average age of Nobel Prize winners when they received the prize is less than 60\"",
        "The mean age in the sample (\nx\n¯\n) is 62.1\nThe standard deviation of age in the sample (\ns\n) is 13.46",
        "Null hypothesis: The average age was 60.\nAlternative hypothesis: The average age was less than 60.",
        "H\n0\n:\nμ\n=\n60\nH\n1\n:\nμ\n<\n60",
        "The claimed (\nH\n0\n) population mean (\nμ\n) was\n60\nThe sample mean (\nx\n¯\n) was\n62.1\nThe sample standard deviation (\ns\n) was\n13.46\nThe sample size (\nn\n) was\n30",
        "import scipy.stats as stats\nimport math\n\n# Specify the sample mean (x_bar), the sample standard deviation (s), the mean claimed in the null-hypothesis (mu_null), and the sample size (n)\nx_bar = 62.1\ns = 13.46\nmu_null = 60\nn = 30\n\n# Calculate and print the test statistic\nprint((x_bar - mu_null)/(s/math.sqrt(n)))",
        "# Specify the sample mean (x_bar), the sample standard deviation (s), the mean claimed in the null-hypothesis (mu_null), and the sample size (n)\nx_bar <- 62.1\ns <- 13.46\nmu_null <- 60\nn <- 30\n\n# Output the test statistic\n(x_bar - mu_null)/(s/sqrt(n))",
        "import scipy.stats as stats\nprint(stats.t.ppf(0.05, 29))",
        "qt(0.05, 29)",
        "The sample data does not support the claim that \"The average age of Nobel Prize winners when they received the prize is less than 60\" at a 5% significance level.",
        "import scipy.stats as stats\nprint(stats.t.cdf(0.855, 29))",
        "pt(0.855, 29)",
        "The sample data does not support the claim that \"The average age of Nobel Prize winners when they received the prize is less than 60\" at a 10%, 5%, or 1% significance level.",
        "import scipy.stats as stats\nimport math\n\n# Specify the sample mean (x_bar), the sample standard deviation (s), the mean claimed in the null-hypothesis (mu_null), and the sample size (n)\nx_bar = 62.1\ns = 13.46\nmu_null = 60\nn = 30\n\n# Calculate the test statistic\ntest_stat = (x_bar - mu_null)/(s/math.sqrt(n))\n\n# Output the p-value of the test statistic (left tailed test)\nprint(stats.t.cdf(test_stat, n-1))",
        "# Specify the sample mean (x_bar), the sample standard deviation (s), the mean claimed in the null-hypothesis (mu_null), and the sample size (n)\nx_bar <- 62.1\ns <- 13.46\nmu_null <- 60\nn <- 30\n\n# Calculate the test statistic\ntest_stat = (x_bar - mu_null)/(s/sqrt(n))\n\n# P-value the p-value of the test statistic (left tailed test)\npt(test_stat, n-1)",
        "t.ppf()",
        "qt()",
        "t.cdf()",
        "pt()"
      ]
    },
    {
      "title": "Statistics - Hypothesis Testing a Mean (Two Tailed)",
      "summary": "A population mean is an average of value a population.\nHypothesis tests are used to check a claim about the size of that population mean.\nHypothesis Testing a Mean\nThe following steps are used for a hypothesis test:\nCheck the conditions\nDefine the claims\nDecide the significance level\nCalculate the test statistic\nConclusion\nFor example:\nPopulation: Nobel Prize winners\nCategory: Age when they received the prize.\nAnd we want to check the claim:\n\"The average age of Nobel Prize winners when they received the prize is not 60\"\nBy taking a sample of 30 randomly selected Nobel Prize winners we could find that:\nThe mean age in the sample (\nx\n¯\n) is 62.1\nThe standard deviation of age in the sample (\ns\n) is 13.46\nFrom this sample data we check the claim with the steps below.\n1. Checking the Conditions\nThe conditions for calculating a confidence interval for a proportion are:\nThe sample is randomly selected\nAnd either:\nThe population data is normally distributed\nSample size is large enough\nThe population data is normally distributed\nSample size is large enough\nA moderately large sample size, like 30, is typically large enough.\nIn the example, the sample size was 30 and it was randomly selected, so the conditions are fulfilled.\nNote: Checking if the data is normally distributed can be done with specialized statistical tests.\n2. Defining the Claims\nWe need to define a null hypothesis (\nH\n0\n) and an alternative hypothesis (\nH\n1\n) based on the claim we are checking.\nThe claim was:\n\"The average age of Nobel Prize winners when they received the prize is not 60\"\nIn this case, the parameter is the mean age of Nobel Prize winners when they received the prize (\nμ\n).\nThe null and alternative hypothesis are then:\nNull hypothesis: The average age was 60.\nAlternative hypothesis: The average age is not 60.\nWhich can be expressed with symbols as:\nH\n0\n:\nμ\n=\n60\nH\n1\n:\nμ\n≠\n60\nThis is a 'two-tailed' test, because the alternative hypothesis claims that the proportion is different from the null hypothesis.\nIf the data supports the alternative hypothesis, we reject the null hypothesis and accept the alternative hypothesis.\nREMOVE ADS\n3. Deciding the Significance Level\nThe significance level (\nα\n) is the uncertainty we accept when rejecting the null hypothesis in a hypothesis test.\nThe significance level is a percentage probability of accidentally making the wrong conclusion.\nTypical significance levels are:\nα\n=\n0.1\n(10%)\nα\n=\n0.05\n(5%)\nα\n=\n0.01\n(1%)\nA lower significance level means that the evidence in the data needs to be stronger to reject the null hypothesis.\nThere is no \"correct\" significance level - it only states the uncertainty of the conclusion.\nNote: A 5% significance level means that when we reject a null hypothesis:\nWe expect to reject a true null hypothesis 5 out of 100 times.\n4. Calculating the Test Statistic\nThe test statistic is used to decide the outcome of the hypothesis test.\nThe test statistic is a standardized value calculated from the sample.\nThe formula for the test statistic (TS) of a population mean is:\nx\n¯\n−\nμ\ns\n⋅\nn\nx\n¯\n−\nμ\nis the difference between the sample mean (\nx\n¯\n) and the claimed population mean (\nμ\n).\ns\nis the sample standard deviation.\nn\nis the sample size.\nIn our example:\nThe claimed (\nH\n0\n) population mean (\nμ\n) was\n60\nThe sample mean (\nx\n¯\n) was\n62.1\nThe sample standard deviation (\ns\n) was\n13.46\nThe sample size (\nn\n) was\n30\nSo the test statistic (TS) is then:\n62.1\n−\n60\n13.46\n⋅\n30\n=\n2.1\n13.46\n⋅\n30\n≈\n0.156\n⋅\n5.477\n=\n0.855\n―\nYou can also calculate the test statistic using programming language functions:\nExampleGet your own Python Server\nWith Python use the scipy and math libraries to calculate the test statistic.\nExample\nWith R use built-in math and statistics functions to calculate the test statistic.\n5. Concluding\nThere are two main approaches for making the conclusion of a hypothesis test:\nThe critical value approach compares the test statistic with the critical value of the significance level.\nThe P-value approach compares the P-value of the test statistic and with the significance level.\nNote: The two approaches are only different in how they present the conclusion.\nThe Critical Value Approach\nFor the critical value approach we need to find the critical value (CV) of the significance level (\nα\n).\nFor a population mean test, the critical value (CV) is a T-value from a student's t-distribution.\nThis critical T-value (CV) defines the rejection region for the test.\nThe rejection region is an area of probability in the tails of the standard normal distribution.\nBecause the claim is that the population proportion is different from 60, the rejection region is split into both the left and right tail:\nThe size of the rejection region is decided by the significance level (\nα\n).\nThe student's t-distribution is adjusted for the uncertainty from smaller samples.\nThis adjustment is called degrees of freedom (df), which is the sample size\n(\nn\n)\n−\n1\nIn this case the degrees of freedom (df) is:\n30\n−\n1\n=\n29\n―\nChoosing a significance level (\nα\n) of 0.05, or 5%, we can find the critical T-value from a T-table, or with a programming language function:\nNote: Because this is a two-tailed test the tail area (\nα\n) needs to be split in half (divided by 2).\nExample\nWith Python use the Scipy Stats library t.ppf() function find the T-Value for an\nα\n/2 = 0.025 at 29 degrees of freedom (df).\nExample\nWith R use the built-in qt() function to find the t-value for an\nα\n/ = 0.025 at 29 degrees of freedom (df).\nUsing either method we can find that the critical T-Value is\n≈\n−\n2.045\n―\nFor a two-tailed test we need to check if the test statistic (TS) is smaller than the negative critical value (-CV), or bigger than the positive critical value (CV).\nIf the test statistic is smaller than the negative critical value, the test statistic is in the rejection region.\nIf the test statistic is bigger than the positive critical value, the test statistic is in the rejection region.\nWhen the test statistic is in the rejection region, we reject the null hypothesis (\nH\n0\n).\nHere, the test statistic (TS) was\n≈\n0.855\n―\nand the critical value was\n≈\n−\n2.045\n―\nHere is an illustration of this test in a graph:\nSince the test statistic is between the critical values we keep the null hypothesis.\nThis means that the sample data does not support the alternative hypothesis.\nAnd we can summarize the conclusion stating:\nThe sample data does not support the claim that \"The average age of Nobel Prize winners when they received the prize is not 60\" at a 5% significance level.\nThe P-Value Approach\nFor the P-value approach we need to find the P-value of the test statistic (TS).\nIf the P-value is smaller than the significance level (\nα\n), we reject the null hypothesis (\nH\n0\n).\nThe test statistic was found to be\n≈\n0.855\n―\nFor a population proportion test, the test statistic is a T-Value from a student's t-distribution.\nBecause this is a two-tailed test, we need to find the P-value of a T-value bigger than 0.855 and multiply it by 2.\nThe student's t-distribution is adjusted according to degrees of freedom (df), which is the sample size\n(\n30\n)\n−\n1\n=\n29\n―\nWe can find the P-value using a T-table, or with a programming language function:\nExample\nWith Python use the Scipy Stats library t.cdf() function find the P-value of a T-value bigger than 0.855 for a two tailed test at 29 degrees of freedom (df):\nExample\nWith R use the built-in pt() function find the P-value of a T-Value bigger than 0.855 for a two tailed test at 29 degrees of freedom (df):\nUsing either method we can find that the P-value is\n≈\n0.3996\n―\nThis tells us that the significance level (\nα\n) would need to be smaller 0.3996, or 39.96%, to reject the null hypothesis.\nHere is an illustration of this test in a graph:\nThis P-value is bigger than any of the common significance levels (10%, 5%, 1%).\nSo the null hypothesis is kept at all of these significance levels.\nAnd we can summarize the conclusion stating:\nThe sample data does not support the claim that \"The average age of Nobel Prize winners when they received the prize is not 60\" at a 10%, 5%, or 1% significance level.\nCalculating a P-Value for a Hypothesis Test with Programming\nMany programming languages can calculate the P-value to decide outcome of a hypothesis test.\nUsing software and programming to calculate statistics is more common for bigger sets of data, as calculating manually becomes difficult.\nThe P-value calculated here will tell us the lowest possible significance level where the null-hypothesis can be rejected.\nExample\nWith Python use the scipy and math libraries to calculate the P-value for a two tailed hypothesis test for a mean.\nHere, the sample size is 30, the sample mean is 62.1, the sample standard deviation is 13.46, and the test is for a mean different from 60.\nExample\nWith R use built-in math and statistics functions find the P-value for a two tailed hypothesis test for a mean.\nHere, the sample size is 30, the sample mean is 62.1, the sample standard deviation is 13.46, and the test is for a mean different from 60.\nLeft-Tailed and Two-Tailed Tests\nThis was an example of a left tailed test, where the alternative hypothesis claimed that parameter is smaller than the null hypothesis claim.\nYou can check out an equivalent step-by-step guide for other types here:\nRight-Tailed Test\nTwo-Tailed Test",
      "examples": [
        "\"The average age of Nobel Prize winners when they received the prize is not 60\"",
        "The mean age in the sample (\nx\n¯\n) is 62.1\nThe standard deviation of age in the sample (\ns\n) is 13.46",
        "Null hypothesis: The average age was 60.\nAlternative hypothesis: The average age is not 60.",
        "H\n0\n:\nμ\n=\n60\nH\n1\n:\nμ\n≠\n60",
        "The claimed (\nH\n0\n) population mean (\nμ\n) was\n60\nThe sample mean (\nx\n¯\n) was\n62.1\nThe sample standard deviation (\ns\n) was\n13.46\nThe sample size (\nn\n) was\n30",
        "import scipy.stats as stats\nimport math\n\n# Specify the sample mean (x_bar), the sample standard deviation (s), the mean claimed in the null-hypothesis (mu_null), and the sample size (n)\nx_bar = 62.1\ns = 13.46\nmu_null = 60\nn = 30\n\n# Calculate and print the test statistic\nprint((x_bar - mu_null)/(s/math.sqrt(n)))",
        "# Specify the sample mean (x_bar), the sample standard deviation (s), the mean claimed in the null-hypothesis (mu_null), and the sample size (n)\nx_bar <- 62.1\ns <- 13.46\nmu_null <- 60\nn <- 30\n\n# Output the test statistic\n(x_bar - mu_null)/(s/sqrt(n))",
        "import scipy.stats as stats\nprint(stats.t.ppf(0.025, 29))",
        "qt(0.025, 29)",
        "The sample data does not support the claim that \"The average age of Nobel Prize winners when they received the prize is not 60\" at a 5% significance level.",
        "import scipy.stats as stats\nprint(2*(1-stats.t.cdf(0.855, 29)))",
        "2*(1-pt(0.855, 29))",
        "The sample data does not support the claim that \"The average age of Nobel Prize winners when they received the prize is not 60\" at a 10%, 5%, or 1% significance level.",
        "import scipy.stats as stats\nimport math\n\n# Specify the sample mean (x_bar), the sample standard deviation (s), the mean claimed in the null-hypothesis (mu_null), and the sample size (n)\nx_bar = 62.1\ns = 13.46\nmu_null = 60\nn = 30\n\n# Calculate the test statistic\ntest_stat = (x_bar - mu_null)/(s/math.sqrt(n))\n\n# Output the p-value of the test statistic (two tailed test)\nprint(2*(1-stats.t.cdf(test_stat, n-1)))",
        "# Specify the sample mean (x_bar), the sample standard deviation (s), the mean claimed in the null-hypothesis (mu_null), and the sample size (n)\nx_bar <- 62.1\ns <- 13.46\nmu_null <- 60\nn <- 30\n\n# Calculate the test statistic\ntest_stat = (x_bar - mu_null)/(s/sqrt(n))\n\n# P-value the p-value of the test statistic (two tailed test)\n2*(1-pt(test_stat, n-1))",
        "t.ppf()",
        "qt()",
        "t.cdf()",
        "pt()"
      ]
    },
    {
      "title": "W3Schools Statistics Certificate",
      "summary": "W3Schools offers an Online Certification Program.\nThe perfect solution for busy professionals who need to balance work, family, and career building.\nMore than 50 000 certificates already issued!\nGet Your Certificate »\nW3Schools offers an Online Certification Program.\nThe perfect solution for busy professionals who need to balance work, family, and career building.\nMore than 50 000 certificates already issued!\nGet Your Certificate »\nWho Should Consider Getting Certified?\nAny student or professional within the digital industry.\nCertifications are valuable assets to gain trust and demonstrate knowledge to your clients, current or future employers on a ever increasing competitive market.\nW3Schools is Trusted by Top Companies\nW3Schools has over two decades of experience with teaching coding online.\nOur certificates are recognized and valued by companies looking to employ skilled developers.\nSave Time and Money\nShow the world your coding skills by getting a certification.\nThe prices is a small fraction compared to the price of traditional education.\nDocument and validate your competence by getting certified!\nExam overview\nFee: 95 USD\nAchievable certification levels:\nIntermediate (40%)\nAdvanced (75%)\nProfessional (90%)\nNumber of questions:\nAdaptive, 60 on average\nRequirement to pass:\nMinimum 40% - Intermediate level\nTime limit: 60 minutes\nNumber of attempts to pass: 3\nExam deadline: None\nCertification Expiration: None\nFormat: Online, multiple choice\nRegister now »\nAdvance Faster in Your Career\nGetting a certificate proves your commitment to upgrading your skills.\nThe certificate can be added as credentials to your CV, Resume, LinkedIn profile, and so on.\nIt gives you the credibility needed for more responsibilities, larger projects, and a higher salary.\nKnowledge is power, especially in the current job market.\nDocumentation of your skills enables you to advance your career or helps you to start a new one.\nHow Does It Work?\nStudy for free at W3Schools.com\nStudy at your own speed\nTest your skills with W3Schools online quizzes\nApply for your certificate by paying an exam fee\nTake your exam online, at any time, and from any location\nGet Your Certificate and Share It With The World\nExample certificate:\nEach certificate gets a unique link that can be shared with others.\nValidate your certification with the link or QR code.\nCheck how it looks like in this Example.\nShare your certificate on Linked in the Certifications section in just one click!\nDocument Your Skills\nGetting a certificate proves your commitment to upgrade your skills, gives you the credibility needed for more responsibilities, larger projects, and a higher salary.\nGet Your Certificate »\nLooking to add multiple users?\nAre you an educator, manager or business owner looking for courses or certifications?\nWe are working with schools, companies and organizations from all over the world.\nGet courses and/or certifications for your team here.",
      "examples": []
    }
  ],
  "glossary": [
    "backend",
    "basic concepts",
    "descriptive statistics",
    "exercises",
    "free",
    "inferential statistics",
    "javascript",
    "mean",
    "note",
    "programs",
    "report error",
    "stat average",
    "stat estimation",
    "stat histograms",
    "stat home",
    "stat inference",
    "stat mean",
    "stat median",
    "stat mode",
    "stat range",
    "stat reference",
    "stat t-table",
    "stat variation",
    "stat z-table",
    "w3.css",
    "w3schools spaces"
  ],
  "objectives": [
    "View your completed tutorials, exercises, and quizzes",
    "Keep an eye on your progress and daily streaks",
    "Set goals and create learning paths",
    "Create your own personal website"
  ]
}